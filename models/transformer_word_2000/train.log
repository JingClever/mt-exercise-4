2025-05-28 17:19:33,784 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                           cfg.name : transformer_word_config
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                     cfg.data.train : data_word_level/train
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                       cfg.data.dev : data_word_level/dev
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                      cfg.data.test : data_word_level/test
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                 cfg.data.src.level : word
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 2000
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 2000
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/transformer_word_2000
2025-05-28 17:19:33,784 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-28 17:19:33,785 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-28 17:19:33,786 - INFO - joeynmt.data - Building tokenizer...
2025-05-28 17:19:33,786 - INFO - joeynmt.tokenizers - it tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-28 17:19:33,786 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-28 17:19:33,786 - INFO - joeynmt.data - Loading train set...
2025-05-28 17:19:33,874 - INFO - joeynmt.data - Building vocabulary...
2025-05-28 17:19:34,588 - INFO - joeynmt.data - Loading dev set...
2025-05-28 17:19:34,591 - INFO - joeynmt.data - Loading test set...
2025-05-28 17:19:34,593 - INFO - joeynmt.data - Data loaded.
2025-05-28 17:19:34,593 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 17:19:34,593 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 17:19:34,593 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 17:19:34,593 - INFO - joeynmt.data - First training example:
	[SRC] Al Gore : arrestiamo il riscaldamento globale
	[TRG] Al Gore : Averting the climate crisis
2025-05-28 17:19:34,593 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) di (7) che (8) e (9) è
2025-05-28 17:19:34,593 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2025-05-28 17:19:34,593 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2025-05-28 17:19:34,593 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2025-05-28 17:19:34,594 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 17:19:34,629 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 17:19:34,630 - INFO - joeynmt.model - Total params: 3925248
2025-05-28 17:19:34,630 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2025-05-28 17:19:34,630 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-28 17:19:34,631 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-28 17:19:34,631 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-28 17:19:34,631 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-28 17:19:34,631 - INFO - joeynmt.training - EPOCH 1
2025-05-28 17:19:48,371 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.045883, Batch Acc: 0.157904, Tokens per Sec:     5140, Lr: 0.000300
2025-05-28 17:20:02,817 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.952877, Batch Acc: 0.202684, Tokens per Sec:     4761, Lr: 0.000300
2025-05-28 17:20:19,212 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.708436, Batch Acc: 0.227459, Tokens per Sec:     4084, Lr: 0.000300
2025-05-28 17:20:34,914 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.619364, Batch Acc: 0.254334, Tokens per Sec:     4531, Lr: 0.000300
2025-05-28 17:20:52,910 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.635102, Batch Acc: 0.273492, Tokens per Sec:     3843, Lr: 0.000300
2025-05-28 17:20:52,913 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:20:52,914 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:21:22,208 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.61, ppl:  13.65, acc:   0.28, generation: 29.2866[sec], evaluation: 0.0000[sec]
2025-05-28 17:21:22,210 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:21:22,356 - INFO - joeynmt.training - Example #0
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'I', "'ve", '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', ',', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:21:22,356 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:21:22,356 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:21:22,356 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , I 've <unk> <unk> <unk> <unk> , <unk> <unk> <unk> , and <unk> <unk> <unk> , and <unk> , and <unk> <unk> .
2025-05-28 17:21:22,356 - INFO - joeynmt.training - Example #1
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:21:22,356 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '.', '</s>']
2025-05-28 17:21:22,356 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:21:22,356 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - Example #2
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', 'is', '<unk>', ',', '<unk>', ',', 'the', '<unk>', ',', '<unk>', ',', '<unk>', '.', '</s>']
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Hypothesis: The <unk> is <unk> , <unk> , the <unk> , <unk> , <unk> .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - Example #3
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> <unk> .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - Example #4
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:21:22,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', 'It', "'s", 'a', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:21:22,357 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> <unk> . It 's a <unk> <unk> .
2025-05-28 17:21:36,662 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.466806, Batch Acc: 0.295170, Tokens per Sec:     4697, Lr: 0.000300
2025-05-28 17:21:52,666 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.581205, Batch Acc: 0.307453, Tokens per Sec:     4325, Lr: 0.000300
2025-05-28 17:22:08,048 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.342331, Batch Acc: 0.320888, Tokens per Sec:     4460, Lr: 0.000300
2025-05-28 17:22:21,271 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.360043, Batch Acc: 0.332877, Tokens per Sec:     5192, Lr: 0.000300
2025-05-28 17:22:35,149 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.437353, Batch Acc: 0.342909, Tokens per Sec:     4957, Lr: 0.000300
2025-05-28 17:22:35,150 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:22:35,150 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:22:57,880 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.35, ppl:  10.49, acc:   0.34, generation: 22.7245[sec], evaluation: 0.0000[sec]
2025-05-28 17:22:57,882 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:22:58,050 - INFO - joeynmt.training - Example #0
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', ',', 'I', "'ve", 'got', 'to', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', '<unk>', 'years', 'years', ',', 'in', 'the', '<unk>', '<unk>', ',', '<unk>', ',', 'in', 'the', '<unk>', ',', 'in', 'the', '<unk>', '.', '</s>']
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> , I 've got to <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> , <unk> <unk> <unk> years years , in the <unk> <unk> , <unk> , in the <unk> , in the <unk> .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - Example #1
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', '<unk>', 'is', '<unk>', ',', 'because', 'it', "'s", 'not', '<unk>', '.', '</s>']
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> <unk> is <unk> , because it 's not <unk> .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - Example #2
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:22:58,050 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', 'is', '<unk>', ',', 'in', 'a', '<unk>', ',', 'in', 'the', '<unk>', ',', 'the', '<unk>', 'of', 'the', '<unk>', '.', '</s>']
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - 	Hypothesis: The <unk> is <unk> , in a <unk> , in the <unk> , the <unk> of the <unk> .
2025-05-28 17:22:58,050 - INFO - joeynmt.training - Example #3
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Hypothesis: You can see <unk> and <unk> <unk> .
2025-05-28 17:22:58,051 - INFO - joeynmt.training - Example #4
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:22:58,051 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:22:58,051 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> <unk> <unk> .
2025-05-28 17:23:12,601 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.294226, Batch Acc: 0.351476, Tokens per Sec:     4848, Lr: 0.000300
2025-05-28 17:23:26,536 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.329403, Batch Acc: 0.362631, Tokens per Sec:     4821, Lr: 0.000300
2025-05-28 17:23:42,725 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.112797, Batch Acc: 0.368728, Tokens per Sec:     4181, Lr: 0.000300
2025-05-28 17:23:57,641 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.197489, Batch Acc: 0.381442, Tokens per Sec:     4581, Lr: 0.000300
2025-05-28 17:24:12,119 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.020156, Batch Acc: 0.390205, Tokens per Sec:     4703, Lr: 0.000300
2025-05-28 17:24:12,120 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:24:12,120 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:24:38,400 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.16, ppl:   8.69, acc:   0.38, generation: 26.2725[sec], evaluation: 0.0000[sec]
2025-05-28 17:24:38,402 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:24:38,578 - INFO - joeynmt.training - Example #0
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'had', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', 'million', 'million', 'years', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', 'the', '<unk>', 'of', 'the', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:24:38,578 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:24:38,578 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:24:38,578 - INFO - joeynmt.training - 	Hypothesis: The year I had <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> million million years , <unk> , <unk> <unk> , <unk> , <unk> , the <unk> of the <unk> <unk> .
2025-05-28 17:24:38,578 - INFO - joeynmt.training - Example #1
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:24:38,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', ',', 'because', 'the', 'problem', 'of', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', '.', '</s>']
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> , because the problem of the <unk> of the <unk> of the <unk> of the <unk> .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - Example #2
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', ',', 'the', '<unk>', '<unk>', ',', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', '.', '</s>']
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> , the <unk> <unk> , the <unk> of the <unk> of the <unk> of the <unk> .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - Example #3
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> <unk> .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - Example #4
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:24:38,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', 'a', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:24:38,579 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is a <unk> <unk> <unk> <unk> <unk> .
2025-05-28 17:24:55,425 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.034300, Batch Acc: 0.395984, Tokens per Sec:     4040, Lr: 0.000300
2025-05-28 17:25:11,756 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.059070, Batch Acc: 0.407477, Tokens per Sec:     4302, Lr: 0.000300
2025-05-28 17:25:27,470 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     1.985860, Batch Acc: 0.410961, Tokens per Sec:     4393, Lr: 0.000300
2025-05-28 17:25:43,156 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     1.914548, Batch Acc: 0.423310, Tokens per Sec:     4455, Lr: 0.000300
2025-05-28 17:26:02,778 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.046792, Batch Acc: 0.425428, Tokens per Sec:     3460, Lr: 0.000300
2025-05-28 17:26:02,779 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:26:02,779 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:26:29,863 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.57, acc:   0.42, generation: 27.0784[sec], evaluation: 0.0000[sec]
2025-05-28 17:26:29,865 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:26:30,033 - INFO - joeynmt.training - Example #0
2025-05-28 17:26:30,033 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:26:30,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:26:30,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'been', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', 'million', 'years', 'of', 'three', 'million', 'years', 'of', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', 'percent', 'of', 'the', '<unk>', ',', '<unk>', 'is', '<unk>', 'percent', 'of', '<unk>', ',', '<unk>', 'percent', 'of', '<unk>', '.', '</s>']
2025-05-28 17:26:30,033 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:26:30,033 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:26:30,033 - INFO - joeynmt.training - 	Hypothesis: The year I 've been <unk> to <unk> that the <unk> <unk> <unk> , which is <unk> million years of three million years of <unk> <unk> , which is <unk> percent of the <unk> , <unk> is <unk> percent of <unk> , <unk> percent of <unk> .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - Example #1
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '<unk>', 'this', '<unk>', 'problem', ',', 'because', 'it', "'s", 'not', 'because', 'it', "'s", 'not', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Hypothesis: And I <unk> this <unk> problem , because it 's not because it 's not <unk> <unk> .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - Example #2
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'way', ',', 'the', '<unk>', 'of', 'the', '<unk>', 'system', '.', '</s>']
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a sense , the way , the <unk> of the <unk> system .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - Example #3
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', ',', '</s>']
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> ,
2025-05-28 17:26:30,034 - INFO - joeynmt.training - Example #4
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:26:30,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', 'years', '.', '</s>']
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:26:30,034 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last years .
2025-05-28 17:26:45,076 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.919758, Batch Acc: 0.432903, Tokens per Sec:     4388, Lr: 0.000300
2025-05-28 17:26:59,892 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.881071, Batch Acc: 0.444580, Tokens per Sec:     4647, Lr: 0.000300
2025-05-28 17:27:14,449 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.970050, Batch Acc: 0.444699, Tokens per Sec:     4709, Lr: 0.000300
2025-05-28 17:27:32,570 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.697535, Batch Acc: 0.448791, Tokens per Sec:     3741, Lr: 0.000300
2025-05-28 17:27:47,958 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.134533, Batch Acc: 0.448547, Tokens per Sec:     4494, Lr: 0.000300
2025-05-28 17:27:47,959 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:27:47,959 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:28:12,100 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.81, acc:   0.44, generation: 24.1353[sec], evaluation: 0.0000[sec]
2025-05-28 17:28:12,101 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:28:12,267 - INFO - joeynmt.training - Example #0
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', "'ve", 'got', 'these', '<unk>', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'three', 'million', 'years', 'has', 'been', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', '<unk>', ',', 'it', "'s", '<unk>', '<unk>', ',', 'it', "'s", '<unk>', 'of', '<unk>', ',', 'it', "'s", '<unk>', '<unk>', ',', '</s>']
2025-05-28 17:28:12,267 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:28:12,267 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:28:12,267 - INFO - joeynmt.training - 	Hypothesis: <unk> year I 've got these <unk> <unk> to <unk> that the <unk> <unk> <unk> , which for three million years has been <unk> <unk> <unk> , which is <unk> <unk> , it 's <unk> <unk> , it 's <unk> of <unk> , it 's <unk> <unk> ,
2025-05-28 17:28:12,267 - INFO - joeynmt.training - Example #1
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:28:12,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', '<unk>', 'is', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', 'of', 'ice', '.', '</s>']
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> <unk> is because it 's not <unk> the ice of ice .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - Example #2
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', '<unk>', 'system', '.', '</s>']
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , the heart <unk> of the <unk> system .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - Example #3
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', '<unk>', '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Hypothesis: You <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - Example #4
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:28:12,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', 'years', '.', '</s>']
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:28:12,268 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last years .
2025-05-28 17:28:27,183 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.053093, Batch Acc: 0.458906, Tokens per Sec:     4589, Lr: 0.000300
2025-05-28 17:28:41,853 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     1.881651, Batch Acc: 0.456537, Tokens per Sec:     4743, Lr: 0.000300
2025-05-28 17:28:57,516 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     1.774077, Batch Acc: 0.466027, Tokens per Sec:     4389, Lr: 0.000300
2025-05-28 17:29:13,162 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     1.869531, Batch Acc: 0.471810, Tokens per Sec:     4390, Lr: 0.000300
2025-05-28 17:29:28,842 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     1.769354, Batch Acc: 0.472500, Tokens per Sec:     4420, Lr: 0.000300
2025-05-28 17:29:28,843 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:29:28,843 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:29:47,853 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.28, acc:   0.46, generation: 19.0042[sec], evaluation: 0.0000[sec]
2025-05-28 17:29:47,855 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:29:48,048 - INFO - joeynmt.helpers - delete models/transformer_word_2000/500.ckpt
2025-05-28 17:29:48,051 - INFO - joeynmt.training - Example #0
2025-05-28 17:29:48,051 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:29:48,051 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:29:48,051 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'got', 'these', '<unk>', 'for', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'almost', 'three', 'million', 'years', 'years', 'years', '.', '</s>']
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Hypothesis: The year I 've got these <unk> for <unk> <unk> <unk> , which is for almost three million years years years .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - Example #1
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the <unk> of the problem because it 's not <unk> the <unk> of the ice .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - Example #2
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'system', '.', '</s>']
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , the heart of the <unk> system .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - Example #3
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> and <unk> <unk> .
2025-05-28 17:29:48,052 - INFO - joeynmt.training - Example #4
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:29:48,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:29:48,053 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:29:48,053 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:29:48,053 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years .
2025-05-28 17:30:03,649 - INFO - joeynmt.training - Epoch   1: total training loss 6827.18
2025-05-28 17:30:03,650 - INFO - joeynmt.training - EPOCH 2
2025-05-28 17:30:05,043 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     1.565489, Batch Acc: 0.498791, Tokens per Sec:     3567, Lr: 0.000300
2025-05-28 17:30:21,243 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.529409, Batch Acc: 0.490137, Tokens per Sec:     4315, Lr: 0.000300
2025-05-28 17:30:36,517 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.486623, Batch Acc: 0.488773, Tokens per Sec:     4561, Lr: 0.000300
2025-05-28 17:30:51,473 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.788374, Batch Acc: 0.492631, Tokens per Sec:     4691, Lr: 0.000300
2025-05-28 17:31:06,473 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.606655, Batch Acc: 0.496970, Tokens per Sec:     4610, Lr: 0.000300
2025-05-28 17:31:06,474 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:31:06,474 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:31:28,453 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.93, acc:   0.48, generation: 21.9721[sec], evaluation: 0.0000[sec]
2025-05-28 17:31:28,454 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:31:28,665 - INFO - joeynmt.helpers - delete models/transformer_word_2000/1000.ckpt
2025-05-28 17:31:28,668 - INFO - joeynmt.training - Example #0
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', "'ve", 'got', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', '<unk>']
2025-05-28 17:31:28,668 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:31:28,668 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:31:28,668 - INFO - joeynmt.training - 	Hypothesis: The last year I 've got these <unk> for <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had the <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> <unk> , <unk> <unk> <unk> , which <unk> <unk> <unk>
2025-05-28 17:31:28,668 - INFO - joeynmt.training - Example #1
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:31:28,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> of the problem because it 's not <unk> <unk> <unk> <unk> .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - Example #2
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'heart', 'of', 'the', 'global', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a sense , the heart heart of the global <unk> of climate system .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - Example #3
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> and <unk> <unk> <unk> .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - Example #4
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:31:28,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:31:28,669 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:31:43,501 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.720141, Batch Acc: 0.493314, Tokens per Sec:     4602, Lr: 0.000300
2025-05-28 17:31:58,163 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.623826, Batch Acc: 0.495641, Tokens per Sec:     4773, Lr: 0.000300
2025-05-28 17:32:13,176 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.648309, Batch Acc: 0.500466, Tokens per Sec:     4576, Lr: 0.000300
2025-05-28 17:32:27,735 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.765412, Batch Acc: 0.501682, Tokens per Sec:     4697, Lr: 0.000300
2025-05-28 17:32:42,229 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.630189, Batch Acc: 0.501486, Tokens per Sec:     4783, Lr: 0.000300
2025-05-28 17:32:42,231 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:32:42,231 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:33:02,715 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.48, generation: 20.4795[sec], evaluation: 0.0000[sec]
2025-05-28 17:33:02,716 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:33:02,948 - INFO - joeynmt.helpers - delete models/transformer_word_2000/1500.ckpt
2025-05-28 17:33:02,950 - INFO - joeynmt.training - Example #0
2025-05-28 17:33:02,950 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:33:02,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:33:02,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'which', 'was', 'the', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'is', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', ',', 'it', "'s", '40', 'percent', '.', '</s>']
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Hypothesis: The year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years , which was the <unk> <unk> <unk> , <unk> is <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , it 's 40 percent .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - Example #1
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'so', 'this', '<unk>', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Hypothesis: And so this <unk> <unk> <unk> of the problem because it 's not <unk> the ice <unk> <unk> .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - Example #2
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'system', '.', '</s>']
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart of the <unk> system .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - Example #3
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> <unk> .
2025-05-28 17:33:02,951 - INFO - joeynmt.training - Example #4
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:33:02,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:33:02,952 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:33:02,952 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:33:02,952 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:33:16,720 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.651629, Batch Acc: 0.501447, Tokens per Sec:     4935, Lr: 0.000300
2025-05-28 17:33:30,985 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.420744, Batch Acc: 0.504259, Tokens per Sec:     4799, Lr: 0.000300
2025-05-28 17:33:45,302 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.581076, Batch Acc: 0.506029, Tokens per Sec:     4901, Lr: 0.000300
2025-05-28 17:33:59,394 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.634724, Batch Acc: 0.507569, Tokens per Sec:     4965, Lr: 0.000300
2025-05-28 17:34:13,838 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.687060, Batch Acc: 0.506915, Tokens per Sec:     4591, Lr: 0.000300
2025-05-28 17:34:13,838 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:34:13,838 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:34:35,427 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.57, acc:   0.49, generation: 21.5833[sec], evaluation: 0.0000[sec]
2025-05-28 17:34:35,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:34:35,599 - INFO - joeynmt.helpers - delete models/transformer_word_2000/2000.ckpt
2025-05-28 17:34:35,601 - INFO - joeynmt.training - Example #0
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', '<unk>', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', '<unk>', 'of', '40', 'percent', '.', '</s>']
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Hypothesis: The year I showed these <unk> for <unk> that <unk> <unk> <unk> , which is about three million years had the <unk> , which is about three million years had the <unk> of 40 percent .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - Example #1
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'we', 'have', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '.', '</s>']
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Hypothesis: So we have this <unk> <unk> of the problem because it 's not <unk> the ice .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - Example #2
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the heart of climate system .
2025-05-28 17:34:35,601 - INFO - joeynmt.training - Example #3
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:34:35,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> and <unk> <unk> .
2025-05-28 17:34:35,602 - INFO - joeynmt.training - Example #4
2025-05-28 17:34:35,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:34:35,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:34:35,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:34:35,602 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:34:49,664 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.525961, Batch Acc: 0.510820, Tokens per Sec:     4921, Lr: 0.000300
2025-05-28 17:35:03,807 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.763990, Batch Acc: 0.515726, Tokens per Sec:     4901, Lr: 0.000300
2025-05-28 17:35:18,089 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.838738, Batch Acc: 0.513326, Tokens per Sec:     4732, Lr: 0.000300
2025-05-28 17:35:32,386 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.545316, Batch Acc: 0.514417, Tokens per Sec:     4912, Lr: 0.000300
2025-05-28 17:35:46,362 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.558733, Batch Acc: 0.518448, Tokens per Sec:     4978, Lr: 0.000300
2025-05-28 17:35:46,363 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:35:46,364 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:36:06,350 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.36, acc:   0.50, generation: 19.9816[sec], evaluation: 0.0000[sec]
2025-05-28 17:36:06,351 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:36:06,531 - INFO - joeynmt.helpers - delete models/transformer_word_2000/2500.ckpt
2025-05-28 17:36:06,532 - INFO - joeynmt.training - Example #0
2025-05-28 17:36:06,532 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:36:06,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:36:06,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'had', 'the', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'percent', '.', '</s>']
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had had the <unk> <unk> <unk> , <unk> percent .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - Example #1
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'we', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Hypothesis: So , we <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - Example #2
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , the heart of climate system .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - Example #3
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:36:06,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:36:06,533 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:36:06,534 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> and <unk> <unk> .
2025-05-28 17:36:06,534 - INFO - joeynmt.training - Example #4
2025-05-28 17:36:06,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:36:06,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:36:06,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:36:06,534 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:36:06,534 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:36:06,534 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:36:20,725 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.593460, Batch Acc: 0.522544, Tokens per Sec:     4822, Lr: 0.000300
2025-05-28 17:36:35,054 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.579473, Batch Acc: 0.512495, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 17:36:49,337 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.686210, Batch Acc: 0.513274, Tokens per Sec:     4771, Lr: 0.000300
2025-05-28 17:37:03,143 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.517373, Batch Acc: 0.519972, Tokens per Sec:     5098, Lr: 0.000300
2025-05-28 17:37:17,667 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.388005, Batch Acc: 0.522423, Tokens per Sec:     4780, Lr: 0.000300
2025-05-28 17:37:17,668 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:37:17,668 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:37:39,295 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.29, acc:   0.50, generation: 21.6225[sec], evaluation: 0.0000[sec]
2025-05-28 17:37:39,296 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:37:39,456 - INFO - joeynmt.helpers - delete models/transformer_word_2000/3000.ckpt
2025-05-28 17:37:39,459 - INFO - joeynmt.training - Example #0
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'had', 'the', 'size', 'of', 'the', '<unk>', '<unk>', 'in', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', 'and', 'it', "'s", '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Hypothesis: The year I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years had had the size of the <unk> <unk> in the United States , <unk> <unk> , and it 's <unk> 40 percent .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - Example #1
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'I', "'m", 'going', 'to', '<unk>', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - 	Hypothesis: So , I 'm going to <unk> the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:37:39,459 - INFO - joeynmt.training - Example #2
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:37:39,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'some', 'sense', ',', 'the', 'heart', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in some sense , the heart <unk> <unk> .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - Example #3
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Hypothesis: You <unk> <unk> and <unk> <unk> .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - Example #4
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:37:39,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:37:39,460 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:37:53,882 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.681246, Batch Acc: 0.518448, Tokens per Sec:     4714, Lr: 0.000300
2025-05-28 17:38:08,027 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.709497, Batch Acc: 0.518252, Tokens per Sec:     4782, Lr: 0.000300
2025-05-28 17:38:22,175 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.656985, Batch Acc: 0.524392, Tokens per Sec:     4909, Lr: 0.000300
2025-05-28 17:38:36,675 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.511519, Batch Acc: 0.525658, Tokens per Sec:     4800, Lr: 0.000300
2025-05-28 17:38:50,661 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.498275, Batch Acc: 0.520055, Tokens per Sec:     4761, Lr: 0.000300
2025-05-28 17:38:50,661 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:38:50,661 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:39:13,153 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.15, acc:   0.50, generation: 22.4868[sec], evaluation: 0.0000[sec]
2025-05-28 17:39:13,155 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:39:13,308 - INFO - joeynmt.helpers - delete models/transformer_word_2000/3500.ckpt
2025-05-28 17:39:13,312 - INFO - joeynmt.training - Example #0
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'who', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', 'which', 'is']
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> to <unk> that the <unk> <unk> <unk> , who for almost three million years had the size of the United States <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , which is
2025-05-28 17:39:13,312 - INFO - joeynmt.training - Example #1
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'we', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Hypothesis: So , we <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - Example #2
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:39:13,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the heart of the climate system .
2025-05-28 17:39:13,312 - INFO - joeynmt.training - Example #3
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> <unk> .
2025-05-28 17:39:13,313 - INFO - joeynmt.training - Example #4
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:39:13,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:39:13,313 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:39:27,388 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.396896, Batch Acc: 0.521982, Tokens per Sec:     4888, Lr: 0.000300
2025-05-28 17:39:37,147 - INFO - joeynmt.training - Epoch   2: total training loss 5043.98
2025-05-28 17:39:37,148 - INFO - joeynmt.training - EPOCH 3
2025-05-28 17:39:41,055 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     1.513131, Batch Acc: 0.538478, Tokens per Sec:     4875, Lr: 0.000300
2025-05-28 17:39:54,975 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.465876, Batch Acc: 0.542476, Tokens per Sec:     4898, Lr: 0.000300
2025-05-28 17:40:08,913 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.456885, Batch Acc: 0.541837, Tokens per Sec:     5007, Lr: 0.000300
2025-05-28 17:40:23,105 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.591271, Batch Acc: 0.537722, Tokens per Sec:     4972, Lr: 0.000300
2025-05-28 17:40:23,106 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:40:23,106 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:40:44,381 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.12, acc:   0.51, generation: 21.2697[sec], evaluation: 0.0000[sec]
2025-05-28 17:40:44,383 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:40:44,555 - INFO - joeynmt.helpers - delete models/transformer_word_2000/4000.ckpt
2025-05-28 17:40:44,558 - INFO - joeynmt.training - Example #0
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', '<unk>', 'percent', 'of', 'the', 'United', 'States', '.', '</s>']
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Hypothesis: <unk> year , I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years had the size of the United States <unk> , <unk> , <unk> percent of the United States .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - Example #1
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - Example #2
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart of climate system .
2025-05-28 17:40:44,558 - INFO - joeynmt.training - Example #3
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:40:44,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:40:44,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> and <unk> <unk> .
2025-05-28 17:40:44,559 - INFO - joeynmt.training - Example #4
2025-05-28 17:40:44,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:40:44,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:40:44,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:40:44,559 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:40:58,258 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.620921, Batch Acc: 0.535374, Tokens per Sec:     4869, Lr: 0.000300
2025-05-28 17:41:12,251 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.593463, Batch Acc: 0.543336, Tokens per Sec:     4891, Lr: 0.000300
2025-05-28 17:41:26,337 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.425118, Batch Acc: 0.542576, Tokens per Sec:     4978, Lr: 0.000300
2025-05-28 17:41:40,560 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.649225, Batch Acc: 0.541317, Tokens per Sec:     4842, Lr: 0.000300
2025-05-28 17:41:54,538 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.431494, Batch Acc: 0.538850, Tokens per Sec:     4973, Lr: 0.000300
2025-05-28 17:41:54,539 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:41:54,539 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:42:15,015 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.51, generation: 20.4716[sec], evaluation: 0.0000[sec]
2025-05-28 17:42:15,016 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:42:15,252 - INFO - joeynmt.helpers - delete models/transformer_word_2000/4500.ckpt
2025-05-28 17:42:15,255 - INFO - joeynmt.training - Example #0
2025-05-28 17:42:15,255 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:42:15,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:42:15,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'is', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:42:15,255 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk> <unk> , which for almost three million years , has had the size of the United States <unk> , <unk> is <unk> 40 percent .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - Example #1
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Hypothesis: We <unk> this <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - Example #2
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', '<unk>', 'system', '.', '</s>']
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a certain sense , the heart <unk> of climate <unk> system .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - Example #3
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - Example #4
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:42:15,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:42:15,256 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 17:42:29,405 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.463089, Batch Acc: 0.540049, Tokens per Sec:     4822, Lr: 0.000300
2025-05-28 17:42:44,644 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.622978, Batch Acc: 0.542157, Tokens per Sec:     4565, Lr: 0.000300
2025-05-28 17:42:58,849 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.549852, Batch Acc: 0.539583, Tokens per Sec:     4865, Lr: 0.000300
2025-05-28 17:43:13,882 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.603439, Batch Acc: 0.540893, Tokens per Sec:     4581, Lr: 0.000300
2025-05-28 17:43:29,133 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.466060, Batch Acc: 0.541525, Tokens per Sec:     4521, Lr: 0.000300
2025-05-28 17:43:29,134 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:43:29,134 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:43:52,899 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.02, acc:   0.52, generation: 23.7590[sec], evaluation: 0.0000[sec]
2025-05-28 17:43:52,901 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:43:53,044 - INFO - joeynmt.helpers - delete models/transformer_word_2000/5000.ckpt
2025-05-28 17:43:53,047 - INFO - joeynmt.training - Example #0
2025-05-28 17:43:53,047 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:43:53,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:43:53,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'percent', '.', '</s>']
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had had the size of the United States , <unk> <unk> , <unk> <unk> percent .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - Example #1
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this <unk> of the <unk> of the ice .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - Example #2
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - Example #3
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 17:43:53,048 - INFO - joeynmt.training - Example #4
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:43:53,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:43:53,049 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:43:53,049 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:43:53,049 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:44:07,427 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.583251, Batch Acc: 0.544209, Tokens per Sec:     4867, Lr: 0.000300
2025-05-28 17:44:22,654 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.213020, Batch Acc: 0.541330, Tokens per Sec:     4501, Lr: 0.000300
2025-05-28 17:44:38,702 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.501931, Batch Acc: 0.550058, Tokens per Sec:     4391, Lr: 0.000300
2025-05-28 17:44:54,072 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.329991, Batch Acc: 0.538090, Tokens per Sec:     4545, Lr: 0.000300
2025-05-28 17:45:10,152 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.453935, Batch Acc: 0.544322, Tokens per Sec:     4245, Lr: 0.000300
2025-05-28 17:45:10,153 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:45:10,153 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:45:34,696 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.51, generation: 24.5381[sec], evaluation: 0.0000[sec]
2025-05-28 17:45:34,698 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:45:34,870 - INFO - joeynmt.helpers - delete models/transformer_word_2000/5500.ckpt
2025-05-28 17:45:34,873 - INFO - joeynmt.training - Example #0
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'almost', 'three', 'million', 'years', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:45:34,873 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:45:34,873 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:45:34,873 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which is almost three million years of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> 40 percent .
2025-05-28 17:45:34,873 - INFO - joeynmt.training - Example #1
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:45:34,873 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - Example #2
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - Example #3
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> and <unk> in the summer .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - Example #4
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:45:34,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:45:34,874 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 17:45:50,935 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.408512, Batch Acc: 0.542868, Tokens per Sec:     4220, Lr: 0.000300
2025-05-28 17:46:06,060 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.547282, Batch Acc: 0.543043, Tokens per Sec:     4497, Lr: 0.000300
2025-05-28 17:46:21,311 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.644391, Batch Acc: 0.537624, Tokens per Sec:     4489, Lr: 0.000300
2025-05-28 17:46:37,019 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.499104, Batch Acc: 0.542546, Tokens per Sec:     4365, Lr: 0.000300
2025-05-28 17:46:53,336 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.376018, Batch Acc: 0.544463, Tokens per Sec:     4175, Lr: 0.000300
2025-05-28 17:46:53,337 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:46:53,337 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:47:13,664 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.94, acc:   0.52, generation: 20.3215[sec], evaluation: 0.0000[sec]
2025-05-28 17:47:13,666 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:47:13,845 - INFO - joeynmt.helpers - delete models/transformer_word_2000/6000.ckpt
2025-05-28 17:47:13,849 - INFO - joeynmt.training - Example #0
2025-05-28 17:47:13,849 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:47:13,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:47:13,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', 'percent', '.', '</s>']
2025-05-28 17:47:13,849 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:47:13,849 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:47:13,849 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had the size of the United States <unk> , <unk> <unk> percent .
2025-05-28 17:47:13,849 - INFO - joeynmt.training - Example #1
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'I', "'m", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', '<unk>', 'the', 'ice', '.', '</s>']
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Hypothesis: So , I 'm going to <unk> the <unk> of the problem because it 's not <unk> <unk> the ice .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - Example #2
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a certain sense , the heart of climate system .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - Example #3
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> <unk> and <unk> <unk> .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - Example #4
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:47:13,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:47:13,850 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 17:47:28,067 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.546365, Batch Acc: 0.542463, Tokens per Sec:     4798, Lr: 0.000300
2025-05-28 17:47:41,981 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.497076, Batch Acc: 0.552667, Tokens per Sec:     4910, Lr: 0.000300
2025-05-28 17:47:56,412 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.625524, Batch Acc: 0.545676, Tokens per Sec:     4693, Lr: 0.000300
2025-05-28 17:48:10,733 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.309165, Batch Acc: 0.543993, Tokens per Sec:     4786, Lr: 0.000300
2025-05-28 17:48:24,845 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.333261, Batch Acc: 0.548695, Tokens per Sec:     4783, Lr: 0.000300
2025-05-28 17:48:24,846 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:48:24,846 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:48:46,542 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.52, generation: 21.6861[sec], evaluation: 0.0000[sec]
2025-05-28 17:48:46,544 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:48:46,701 - INFO - joeynmt.helpers - delete models/transformer_word_2000/6500.ckpt
2025-05-28 17:48:46,705 - INFO - joeynmt.training - Example #0
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'is', '<unk>', 'percent', '.', '</s>']
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years had the size of the United States <unk> , <unk> is <unk> percent .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - Example #1
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - Example #2
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 17:48:46,706 - INFO - joeynmt.training - Example #3
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:48:46,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Hypothesis: You <unk> <unk> and <unk> <unk> .
2025-05-28 17:48:46,707 - INFO - joeynmt.training - Example #4
2025-05-28 17:48:46,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:48:46,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:48:46,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:48:46,707 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 17:49:00,798 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.474207, Batch Acc: 0.544735, Tokens per Sec:     4884, Lr: 0.000300
2025-05-28 17:49:14,718 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.600032, Batch Acc: 0.546688, Tokens per Sec:     5019, Lr: 0.000300
2025-05-28 17:49:23,167 - INFO - joeynmt.training - Epoch   3: total training loss 4643.09
2025-05-28 17:49:23,168 - INFO - joeynmt.training - EPOCH 4
2025-05-28 17:49:28,768 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     1.415893, Batch Acc: 0.558791, Tokens per Sec:     5016, Lr: 0.000300
2025-05-28 17:49:43,162 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.436978, Batch Acc: 0.558180, Tokens per Sec:     4741, Lr: 0.000300
2025-05-28 17:49:57,218 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.373886, Batch Acc: 0.565464, Tokens per Sec:     4912, Lr: 0.000300
2025-05-28 17:49:57,219 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:49:57,219 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:50:16,764 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.83, acc:   0.53, generation: 19.5396[sec], evaluation: 0.0000[sec]
2025-05-28 17:50:16,766 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:50:16,890 - INFO - joeynmt.helpers - delete models/transformer_word_2000/7000.ckpt
2025-05-28 17:50:16,893 - INFO - joeynmt.training - Example #0
2025-05-28 17:50:16,893 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:50:16,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:50:16,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', 'it', "'s", '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years had the size of the United States <unk> , <unk> , it 's <unk> 40 percent .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - Example #1
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'m", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Hypothesis: And I 'm going to <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - Example #2
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - Example #3
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> in the summer .
2025-05-28 17:50:16,894 - INFO - joeynmt.training - Example #4
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:50:16,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:50:16,895 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:50:16,895 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:50:16,895 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 17:50:31,551 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.350105, Batch Acc: 0.559881, Tokens per Sec:     4703, Lr: 0.000300
2025-05-28 17:50:46,569 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.348613, Batch Acc: 0.566468, Tokens per Sec:     4641, Lr: 0.000300
2025-05-28 17:51:01,915 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.480578, Batch Acc: 0.562481, Tokens per Sec:     4366, Lr: 0.000300
2025-05-28 17:51:17,748 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.438837, Batch Acc: 0.560965, Tokens per Sec:     4325, Lr: 0.000300
2025-05-28 17:51:33,438 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.432769, Batch Acc: 0.557614, Tokens per Sec:     4443, Lr: 0.000300
2025-05-28 17:51:33,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:51:33,438 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:52:30,145 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.52, generation: 56.7005[sec], evaluation: 0.0000[sec]
2025-05-28 17:52:30,149 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:52:30,264 - INFO - joeynmt.helpers - delete models/transformer_word_2000/7500.ckpt
2025-05-28 17:52:30,269 - INFO - joeynmt.training - Example #0
2025-05-28 17:52:30,269 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:52:30,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:52:30,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', '<unk>', 'percent', '.', '</s>']
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had the size of the United States , <unk> , <unk> <unk> percent .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - Example #1
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'I', "'m", 'going', 'to', 'take', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Hypothesis: So , I 'm going to take the <unk> of the ice .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - Example #2
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the heart of climate system .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - Example #3
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 17:52:30,270 - INFO - joeynmt.training - Example #4
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:52:30,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:52:30,271 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:52:30,271 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:52:30,271 - INFO - joeynmt.training - 	Hypothesis: So , next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:52:46,172 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.277844, Batch Acc: 0.562027, Tokens per Sec:     4403, Lr: 0.000300
2025-05-28 17:53:00,918 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.552028, Batch Acc: 0.557188, Tokens per Sec:     4685, Lr: 0.000300
2025-05-28 17:53:16,495 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.260519, Batch Acc: 0.557729, Tokens per Sec:     4431, Lr: 0.000300
2025-05-28 17:53:33,255 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.431816, Batch Acc: 0.557601, Tokens per Sec:     4061, Lr: 0.000300
2025-05-28 17:53:48,698 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.344709, Batch Acc: 0.558021, Tokens per Sec:     4542, Lr: 0.000300
2025-05-28 17:53:48,699 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:53:48,699 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:54:14,098 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.82, acc:   0.52, generation: 25.3924[sec], evaluation: 0.0000[sec]
2025-05-28 17:54:14,239 - INFO - joeynmt.helpers - delete models/transformer_word_2000/8000.ckpt
2025-05-28 17:54:14,243 - INFO - joeynmt.training - Example #0
2025-05-28 17:54:14,243 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:54:14,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:54:14,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'was', '<unk>', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2025-05-28 17:54:14,243 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:54:14,243 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:54:14,243 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years , it was <unk> <unk> , <unk> of 40 percent .
2025-05-28 17:54:14,243 - INFO - joeynmt.training - Example #1
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - Example #2
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a certain sense , the heart of climate system .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - Example #3
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - Example #4
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:54:14,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:54:14,244 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:54:30,434 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     1.460283, Batch Acc: 0.561249, Tokens per Sec:     4201, Lr: 0.000300
2025-05-28 17:54:46,939 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     1.530809, Batch Acc: 0.562979, Tokens per Sec:     4313, Lr: 0.000300
2025-05-28 17:55:02,259 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.490914, Batch Acc: 0.560495, Tokens per Sec:     4463, Lr: 0.000300
2025-05-28 17:55:17,784 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.284650, Batch Acc: 0.559151, Tokens per Sec:     4484, Lr: 0.000300
2025-05-28 17:55:32,042 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.379352, Batch Acc: 0.559611, Tokens per Sec:     4893, Lr: 0.000300
2025-05-28 17:55:32,043 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:55:32,043 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:55:53,596 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.74, acc:   0.53, generation: 21.5471[sec], evaluation: 0.0000[sec]
2025-05-28 17:55:53,598 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:55:53,733 - INFO - joeynmt.helpers - delete models/transformer_word_2000/8500.ckpt
2025-05-28 17:55:53,739 - INFO - joeynmt.training - Example #0
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', ',', '<unk>', 'percent', 'of', 'the', 'United', 'States', '.', '</s>']
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk> , which for almost three million years , it has had the size of the United States , <unk> , <unk> , <unk> percent of the United States .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - Example #1
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - Example #2
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:55:53,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:55:53,739 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - Example #3
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> and <unk> <unk> .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - Example #4
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:55:53,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:55:53,740 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:56:08,014 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.361730, Batch Acc: 0.559772, Tokens per Sec:     4917, Lr: 0.000300
2025-05-28 17:56:22,399 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.360753, Batch Acc: 0.564918, Tokens per Sec:     4842, Lr: 0.000300
2025-05-28 17:56:37,032 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.511387, Batch Acc: 0.558983, Tokens per Sec:     4662, Lr: 0.000300
2025-05-28 17:56:51,959 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.343370, Batch Acc: 0.559310, Tokens per Sec:     4703, Lr: 0.000300
2025-05-28 17:57:06,617 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.470507, Batch Acc: 0.560400, Tokens per Sec:     4700, Lr: 0.000300
2025-05-28 17:57:06,618 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:57:06,618 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:57:26,640 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.72, acc:   0.53, generation: 20.0174[sec], evaluation: 0.0000[sec]
2025-05-28 17:57:26,641 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:57:26,758 - INFO - joeynmt.helpers - delete models/transformer_word_2000/9000.ckpt
2025-05-28 17:57:26,764 - INFO - joeynmt.training - Example #0
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', 'percent', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk> , which for almost three million years had had the size of the United States , <unk> <unk> , <unk> percent of the United States , <unk> , <unk> 40 percent .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - Example #1
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - Example #2
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> of climate system .
2025-05-28 17:57:26,764 - INFO - joeynmt.training - Example #3
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:57:26,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:57:26,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> and <unk> <unk> <unk> .
2025-05-28 17:57:26,765 - INFO - joeynmt.training - Example #4
2025-05-28 17:57:26,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:57:26,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:57:26,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:57:26,765 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 17:57:41,606 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.323388, Batch Acc: 0.556943, Tokens per Sec:     4565, Lr: 0.000300
2025-05-28 17:57:56,393 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.343914, Batch Acc: 0.562929, Tokens per Sec:     4606, Lr: 0.000300
2025-05-28 17:58:10,744 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.508130, Batch Acc: 0.559706, Tokens per Sec:     4707, Lr: 0.000300
2025-05-28 17:58:24,932 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.382804, Batch Acc: 0.560026, Tokens per Sec:     4872, Lr: 0.000300
2025-05-28 17:58:39,625 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.376715, Batch Acc: 0.558587, Tokens per Sec:     4803, Lr: 0.000300
2025-05-28 17:58:39,626 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 17:58:39,626 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:58:59,476 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.53, generation: 19.8449[sec], evaluation: 0.0000[sec]
2025-05-28 17:58:59,477 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 17:58:59,633 - INFO - joeynmt.helpers - delete models/transformer_word_2000/9500.ckpt
2025-05-28 17:58:59,638 - INFO - joeynmt.training - Example #0
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', 'that', 'for', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', 'it', "'s", '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> that for <unk> <unk> <unk> , which for almost three million years had the size of the United States <unk> , <unk> , it 's <unk> 40 percent .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - Example #1
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - Example #2
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a certain sense , the heart of the climate system .
2025-05-28 17:58:59,638 - INFO - joeynmt.training - Example #3
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 17:58:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', 'in', 'summer', '.', '</s>']
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> in summer .
2025-05-28 17:58:59,639 - INFO - joeynmt.training - Example #4
2025-05-28 17:58:59,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 17:58:59,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 17:58:59,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 17:58:59,639 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 17:59:14,121 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.452186, Batch Acc: 0.559794, Tokens per Sec:     4833, Lr: 0.000300
2025-05-28 17:59:28,456 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.411049, Batch Acc: 0.563485, Tokens per Sec:     4749, Lr: 0.000300
2025-05-28 17:59:43,024 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.471469, Batch Acc: 0.558799, Tokens per Sec:     4670, Lr: 0.000300
2025-05-28 17:59:48,465 - INFO - joeynmt.training - Epoch   4: total training loss 4405.53
2025-05-28 17:59:48,465 - INFO - joeynmt.training - EPOCH 5
2025-05-28 17:59:57,453 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     1.450467, Batch Acc: 0.582887, Tokens per Sec:     4693, Lr: 0.000300
2025-05-28 18:00:12,195 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     1.341708, Batch Acc: 0.581228, Tokens per Sec:     4610, Lr: 0.000300
2025-05-28 18:00:12,196 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:00:12,196 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:00:32,948 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.68, acc:   0.53, generation: 20.7461[sec], evaluation: 0.0000[sec]
2025-05-28 18:00:32,949 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:00:33,098 - INFO - joeynmt.helpers - delete models/transformer_word_2000/10500.ckpt
2025-05-28 18:00:33,099 - INFO - joeynmt.training - Example #0
2025-05-28 18:00:33,099 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:00:33,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:00:33,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years , had the size of three million years , had the size of the United States , <unk> 40 percent .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - Example #1
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'shows', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> the <unk> of the problem because it shows the ice <unk> .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - Example #2
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a certain sense , the heart <unk> of climate system .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - Example #3
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> .
2025-05-28 18:00:33,100 - INFO - joeynmt.training - Example #4
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:00:33,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:00:33,101 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:00:33,101 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:00:33,101 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 18:00:48,101 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.369662, Batch Acc: 0.581491, Tokens per Sec:     4559, Lr: 0.000300
2025-05-28 18:01:03,250 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.303548, Batch Acc: 0.578987, Tokens per Sec:     4611, Lr: 0.000300
2025-05-28 18:01:17,826 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.249341, Batch Acc: 0.576298, Tokens per Sec:     4658, Lr: 0.000300
2025-05-28 18:01:32,165 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.325410, Batch Acc: 0.579000, Tokens per Sec:     4752, Lr: 0.000300
2025-05-28 18:01:46,572 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.361703, Batch Acc: 0.576103, Tokens per Sec:     4777, Lr: 0.000300
2025-05-28 18:01:46,573 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:01:46,573 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:02:08,151 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.69, acc:   0.53, generation: 21.5741[sec], evaluation: 0.0000[sec]
2025-05-28 18:02:08,393 - INFO - joeynmt.helpers - delete models/transformer_word_2000/10000.ckpt
2025-05-28 18:02:08,396 - INFO - joeynmt.training - Example #0
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:02:08,396 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:02:08,396 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:02:08,396 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> .
2025-05-28 18:02:08,396 - INFO - joeynmt.training - Example #1
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:02:08,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'shows', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Hypothesis: And I <unk> this <unk> the <unk> of the problem because it shows the ice <unk> .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - Example #2
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - Example #3
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - Example #4
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:02:08,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'one', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:02:08,397 - INFO - joeynmt.training - 	Hypothesis: The next one is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:02:22,585 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.382268, Batch Acc: 0.572215, Tokens per Sec:     4654, Lr: 0.000300
2025-05-28 18:02:37,143 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.455649, Batch Acc: 0.576605, Tokens per Sec:     4785, Lr: 0.000300
2025-05-28 18:02:52,003 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     1.367505, Batch Acc: 0.576566, Tokens per Sec:     4769, Lr: 0.000300
2025-05-28 18:03:06,441 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     1.405741, Batch Acc: 0.570870, Tokens per Sec:     4835, Lr: 0.000300
2025-05-28 18:03:20,840 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     1.349105, Batch Acc: 0.577076, Tokens per Sec:     4902, Lr: 0.000300
2025-05-28 18:03:20,840 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:03:20,840 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:03:43,791 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.69, acc:   0.53, generation: 22.9458[sec], evaluation: 0.0000[sec]
2025-05-28 18:03:43,963 - INFO - joeynmt.helpers - delete models/transformer_word_2000/11000.ckpt
2025-05-28 18:03:43,966 - INFO - joeynmt.training - Example #0
2025-05-28 18:03:43,966 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:03:43,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:03:43,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'was', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years , it was the size of the United States , <unk> <unk> , <unk> <unk> <unk> <unk> .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - Example #1
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it 's not <unk> the ice <unk> .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - Example #2
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', 'system', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , the heart of the system of climate system .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - Example #3
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:03:43,967 - INFO - joeynmt.training - Example #4
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:03:43,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:03:43,967 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:03:43,968 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:03:43,968 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:03:58,526 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     1.428191, Batch Acc: 0.569805, Tokens per Sec:     4729, Lr: 0.000300
2025-05-28 18:04:13,761 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     1.536224, Batch Acc: 0.575070, Tokens per Sec:     4663, Lr: 0.000300
2025-05-28 18:04:28,451 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     1.277282, Batch Acc: 0.572445, Tokens per Sec:     4621, Lr: 0.000300
2025-05-28 18:04:43,087 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     1.282255, Batch Acc: 0.572948, Tokens per Sec:     4780, Lr: 0.000300
2025-05-28 18:04:58,071 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     1.363757, Batch Acc: 0.569184, Tokens per Sec:     4609, Lr: 0.000300
2025-05-28 18:04:58,072 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:04:58,072 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:05:18,635 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.68, acc:   0.53, generation: 20.5582[sec], evaluation: 0.0000[sec]
2025-05-28 18:05:18,636 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:05:18,818 - INFO - joeynmt.helpers - delete models/transformer_word_2000/11500.ckpt
2025-05-28 18:05:18,821 - INFO - joeynmt.training - Example #0
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'it', "'s", '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , it 's <unk> 40 percent .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - Example #1
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - 	Hypothesis: And I <unk> this <unk> of the problem because it 's not <unk> the ice of the ice .
2025-05-28 18:05:18,821 - INFO - joeynmt.training - Example #2
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:05:18,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'global', 'global', '<unk>', 'system', '.', '</s>']
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a way , the heart of the global global <unk> system .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - Example #3
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', 'in', 'summer', '.', '</s>']
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> in summer .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - Example #4
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:05:18,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:05:18,822 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:05:32,953 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     1.593089, Batch Acc: 0.572632, Tokens per Sec:     4711, Lr: 0.000300
2025-05-28 18:05:47,565 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     1.309605, Batch Acc: 0.578136, Tokens per Sec:     4766, Lr: 0.000300
2025-05-28 18:06:01,933 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     1.421376, Batch Acc: 0.571114, Tokens per Sec:     4810, Lr: 0.000300
2025-05-28 18:06:16,618 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.389008, Batch Acc: 0.569394, Tokens per Sec:     4652, Lr: 0.000300
2025-05-28 18:06:31,019 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.451739, Batch Acc: 0.568347, Tokens per Sec:     4710, Lr: 0.000300
2025-05-28 18:06:31,020 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:06:31,020 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:06:51,812 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.53, generation: 20.7870[sec], evaluation: 0.0000[sec]
2025-05-28 18:06:51,813 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:06:52,026 - INFO - joeynmt.helpers - delete models/transformer_word_2000/12000.ckpt
2025-05-28 18:06:52,028 - INFO - joeynmt.training - Example #0
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'old', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk> , which for almost three million years old had the size of the United States <unk> , <unk> <unk> <unk> .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - Example #1
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', 'show', 'the', 'ice', '.', '</s>']
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this <unk> of the problem because it 's not show the ice .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - Example #2
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of global warming .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - Example #3
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:06:52,029 - INFO - joeynmt.training - Example #4
2025-05-28 18:06:52,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:06:52,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:06:52,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:06:52,030 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:06:52,030 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:06:52,030 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:07:06,739 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.448240, Batch Acc: 0.570162, Tokens per Sec:     4670, Lr: 0.000300
2025-05-28 18:07:21,216 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.297536, Batch Acc: 0.572309, Tokens per Sec:     4843, Lr: 0.000300
2025-05-28 18:07:35,494 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     1.370428, Batch Acc: 0.566636, Tokens per Sec:     4683, Lr: 0.000300
2025-05-28 18:07:50,077 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     1.440261, Batch Acc: 0.566262, Tokens per Sec:     4791, Lr: 0.000300
2025-05-28 18:08:04,899 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     1.288789, Batch Acc: 0.570340, Tokens per Sec:     4700, Lr: 0.000300
2025-05-28 18:08:04,901 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:08:04,901 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:08:27,831 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.53, generation: 22.9256[sec], evaluation: 0.0000[sec]
2025-05-28 18:08:27,833 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:08:28,050 - INFO - joeynmt.helpers - delete models/transformer_word_2000/13500.ckpt
2025-05-28 18:08:28,053 - INFO - joeynmt.training - Example #0
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'old', 'old', 'old', '<unk>', ',', '<unk>', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk> , which for almost three million years old old old <unk> , <unk> of the United States , <unk> , <unk> of 40 percent .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - Example #1
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'ice', '.', '</s>']
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> of the problem because it doesn 't show the <unk> of ice .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - Example #2
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', '<unk>', 'heart', 'of', 'climate', 'change', '.', '</s>']
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the <unk> heart of climate change .
2025-05-28 18:08:28,053 - INFO - joeynmt.training - Example #3
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:08:28,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:08:28,054 - INFO - joeynmt.training - Example #4
2025-05-28 18:08:28,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:08:28,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:08:28,054 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:08:28,054 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:08:42,588 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     1.345093, Batch Acc: 0.568941, Tokens per Sec:     4620, Lr: 0.000300
2025-05-28 18:08:57,503 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.328959, Batch Acc: 0.567737, Tokens per Sec:     4650, Lr: 0.000300
2025-05-28 18:09:11,955 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.549614, Batch Acc: 0.569453, Tokens per Sec:     4740, Lr: 0.000300
2025-05-28 18:09:26,424 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     1.412497, Batch Acc: 0.565823, Tokens per Sec:     4691, Lr: 0.000300
2025-05-28 18:09:29,727 - INFO - joeynmt.training - Epoch   5: total training loss 4258.88
2025-05-28 18:09:29,727 - INFO - joeynmt.training - EPOCH 6
2025-05-28 18:09:41,048 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     1.254100, Batch Acc: 0.591278, Tokens per Sec:     4642, Lr: 0.000300
2025-05-28 18:09:41,049 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:09:41,049 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:10:01,145 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.54, generation: 20.0903[sec], evaluation: 0.0000[sec]
2025-05-28 18:10:01,319 - INFO - joeynmt.helpers - delete models/transformer_word_2000/13000.ckpt
2025-05-28 18:10:01,320 - INFO - joeynmt.training - Example #0
2025-05-28 18:10:01,320 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk> , which for almost three million years had the size of the United States , <unk> <unk> , <unk> <unk> <unk> .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - Example #1
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - Example #2
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a certain sense , the heart of the climate system .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - Example #3
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> in the summer .
2025-05-28 18:10:01,321 - INFO - joeynmt.training - Example #4
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:10:01,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:10:01,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:10:01,322 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:10:01,322 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:10:01,322 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:10:15,422 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     1.257891, Batch Acc: 0.591615, Tokens per Sec:     4871, Lr: 0.000300
2025-05-28 18:10:29,846 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.488864, Batch Acc: 0.588094, Tokens per Sec:     4848, Lr: 0.000300
2025-05-28 18:10:44,308 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.307856, Batch Acc: 0.591156, Tokens per Sec:     4933, Lr: 0.000300
2025-05-28 18:10:58,623 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     1.423785, Batch Acc: 0.587061, Tokens per Sec:     4658, Lr: 0.000300
2025-05-28 18:11:13,048 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     1.302179, Batch Acc: 0.583999, Tokens per Sec:     4667, Lr: 0.000300
2025-05-28 18:11:13,049 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:11:13,049 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:11:35,462 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.53, generation: 22.4078[sec], evaluation: 0.0000[sec]
2025-05-28 18:11:35,660 - INFO - joeynmt.helpers - delete models/transformer_word_2000/12500.ckpt
2025-05-28 18:11:35,664 - INFO - joeynmt.training - Example #0
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - Example #1
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - Example #2
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of the global system .
2025-05-28 18:11:35,664 - INFO - joeynmt.training - Example #3
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:11:35,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> <unk> .
2025-05-28 18:11:35,665 - INFO - joeynmt.training - Example #4
2025-05-28 18:11:35,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:11:35,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:11:35,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:11:35,665 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years .
2025-05-28 18:11:50,451 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     1.427543, Batch Acc: 0.585923, Tokens per Sec:     4636, Lr: 0.000300
2025-05-28 18:12:05,131 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     1.397137, Batch Acc: 0.583913, Tokens per Sec:     4792, Lr: 0.000300
2025-05-28 18:12:19,573 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     1.209645, Batch Acc: 0.584422, Tokens per Sec:     4697, Lr: 0.000300
2025-05-28 18:12:34,767 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     1.323339, Batch Acc: 0.587426, Tokens per Sec:     4618, Lr: 0.000300
2025-05-28 18:12:49,612 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     1.174726, Batch Acc: 0.584723, Tokens per Sec:     4645, Lr: 0.000300
2025-05-28 18:12:49,614 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:12:49,614 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:13:12,717 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.61, acc:   0.54, generation: 23.0977[sec], evaluation: 0.0000[sec]
2025-05-28 18:13:12,992 - INFO - joeynmt.helpers - delete models/transformer_word_2000/14000.ckpt
2025-05-28 18:13:12,994 - INFO - joeynmt.training - Example #0
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States , <unk> , <unk> <unk> , <unk> 40 percent .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - Example #1
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'I', "'m", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Hypothesis: So , I 'm going to <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - Example #2
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart of the climate system .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - Example #3
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:13:12,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:13:12,994 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:13:12,995 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:13:12,995 - INFO - joeynmt.training - Example #4
2025-05-28 18:13:12,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:13:12,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:13:12,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:13:12,995 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:13:12,995 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:13:12,995 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:13:27,675 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     1.203413, Batch Acc: 0.580422, Tokens per Sec:     4552, Lr: 0.000300
2025-05-28 18:13:42,279 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     1.332172, Batch Acc: 0.581768, Tokens per Sec:     4811, Lr: 0.000300
2025-05-28 18:13:56,747 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     1.313368, Batch Acc: 0.582361, Tokens per Sec:     4707, Lr: 0.000300
2025-05-28 18:14:11,327 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     1.347506, Batch Acc: 0.581727, Tokens per Sec:     4766, Lr: 0.000300
2025-05-28 18:14:25,887 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     1.326030, Batch Acc: 0.576961, Tokens per Sec:     4692, Lr: 0.000300
2025-05-28 18:14:25,888 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:14:25,888 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:14:47,511 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.62, acc:   0.53, generation: 21.6174[sec], evaluation: 0.0000[sec]
2025-05-28 18:14:47,741 - INFO - joeynmt.helpers - delete models/transformer_word_2000/14500.ckpt
2025-05-28 18:14:47,746 - INFO - joeynmt.training - Example #0
2025-05-28 18:14:47,746 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:14:47,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:14:47,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> <unk> , which for almost three million years has had the size of three million years has had the size of the United States <unk> , <unk> 40 percent .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - Example #1
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - Example #2
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the heart <unk> of global warming .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - Example #3
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> <unk> .
2025-05-28 18:14:47,747 - INFO - joeynmt.training - Example #4
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:14:47,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:14:47,748 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:14:47,748 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:14:47,748 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:15:02,382 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     1.397195, Batch Acc: 0.585370, Tokens per Sec:     4668, Lr: 0.000300
2025-05-28 18:15:17,570 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     1.357789, Batch Acc: 0.577199, Tokens per Sec:     4700, Lr: 0.000300
2025-05-28 18:15:32,075 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     1.395910, Batch Acc: 0.578451, Tokens per Sec:     4551, Lr: 0.000300
2025-05-28 18:15:46,749 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     1.298553, Batch Acc: 0.581355, Tokens per Sec:     4647, Lr: 0.000300
2025-05-28 18:16:01,172 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     1.240694, Batch Acc: 0.577756, Tokens per Sec:     4698, Lr: 0.000300
2025-05-28 18:16:01,174 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:16:01,174 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:16:21,689 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.53, generation: 20.5108[sec], evaluation: 0.0000[sec]
2025-05-28 18:16:21,692 - INFO - joeynmt.training - Example #0
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', "'s", '<unk>', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', '<unk>', 'percent', '.', '</s>']
2025-05-28 18:16:21,692 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:16:21,692 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:16:21,692 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years , it 's <unk> the size of the United States , <unk> , <unk> <unk> percent .
2025-05-28 18:16:21,692 - INFO - joeynmt.training - Example #1
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:16:21,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', '<unk>', ',', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this <unk> <unk> , because it doesn 't show the ice <unk> .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - Example #2
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'global', 'global', 'system', '.', '</s>']
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a sense , the heart <unk> of global global global system .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - Example #3
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk> .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - Example #4
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:16:21,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:16:21,693 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:16:36,222 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     1.298684, Batch Acc: 0.578514, Tokens per Sec:     4719, Lr: 0.000300
2025-05-28 18:16:50,845 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     1.289742, Batch Acc: 0.580977, Tokens per Sec:     4649, Lr: 0.000300
2025-05-28 18:17:04,776 - INFO - joeynmt.training - Epoch   6, Step:    17800, Batch Loss:     1.533059, Batch Acc: 0.583478, Tokens per Sec:     4963, Lr: 0.000300
2025-05-28 18:17:19,289 - INFO - joeynmt.training - Epoch   6, Step:    17900, Batch Loss:     1.318877, Batch Acc: 0.582140, Tokens per Sec:     4753, Lr: 0.000300
2025-05-28 18:17:33,840 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     1.305396, Batch Acc: 0.584462, Tokens per Sec:     4822, Lr: 0.000300
2025-05-28 18:17:33,842 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:17:33,842 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:17:53,748 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.55, acc:   0.54, generation: 19.9011[sec], evaluation: 0.0000[sec]
2025-05-28 18:17:53,749 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:17:54,018 - INFO - joeynmt.helpers - delete models/transformer_word_2000/17000.ckpt
2025-05-28 18:17:54,020 - INFO - joeynmt.training - Example #0
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'was', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', '<unk>', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years , it was <unk> <unk> , which for almost three million years , <unk> <unk> , <unk> 40 percent .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - Example #1
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '.', '</s>']
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Hypothesis: And I 'm going to <unk> this <unk> of the problem because it 's not <unk> the ice .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - Example #2
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the heart <unk> of climate change .
2025-05-28 18:17:54,020 - INFO - joeynmt.training - Example #3
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:17:54,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:17:54,021 - INFO - joeynmt.training - Example #4
2025-05-28 18:17:54,021 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:17:54,021 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:17:54,021 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:17:54,021 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:18:08,742 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     1.399613, Batch Acc: 0.580455, Tokens per Sec:     4507, Lr: 0.000300
2025-05-28 18:18:22,968 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     1.357970, Batch Acc: 0.576663, Tokens per Sec:     4972, Lr: 0.000300
2025-05-28 18:18:37,449 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     1.394365, Batch Acc: 0.589099, Tokens per Sec:     4879, Lr: 0.000300
2025-05-28 18:18:52,059 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     1.420249, Batch Acc: 0.579731, Tokens per Sec:     4625, Lr: 0.000300
2025-05-28 18:19:06,558 - INFO - joeynmt.training - Epoch   6, Step:    18500, Batch Loss:     1.443818, Batch Acc: 0.578587, Tokens per Sec:     4647, Lr: 0.000300
2025-05-28 18:19:06,559 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:19:06,559 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:19:26,900 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.54, generation: 20.3358[sec], evaluation: 0.0000[sec]
2025-05-28 18:19:27,136 - INFO - joeynmt.helpers - delete models/transformer_word_2000/16500.ckpt
2025-05-28 18:19:27,139 - INFO - joeynmt.training - Example #0
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', 'it', "'s", '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk> , which for almost three million years , had the size of the United States , it 's <unk> 40 percent .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - Example #1
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:19:27,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - 	Hypothesis: And this <unk> the problem because it doesn 't show the <unk> of the ice .
2025-05-28 18:19:27,139 - INFO - joeynmt.training - Example #2
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a way , the heart <unk> of climate system .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - Example #3
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - Example #4
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:19:27,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:19:27,140 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:19:28,470 - INFO - joeynmt.training - Epoch   6: total training loss 4138.82
2025-05-28 18:19:28,470 - INFO - joeynmt.training - EPOCH 7
2025-05-28 18:19:42,009 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     1.282208, Batch Acc: 0.602402, Tokens per Sec:     4588, Lr: 0.000300
2025-05-28 18:19:56,570 - INFO - joeynmt.training - Epoch   7, Step:    18700, Batch Loss:     1.285010, Batch Acc: 0.600502, Tokens per Sec:     4786, Lr: 0.000300
2025-05-28 18:20:11,361 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     1.266392, Batch Acc: 0.602325, Tokens per Sec:     4641, Lr: 0.000300
2025-05-28 18:20:25,752 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     1.138922, Batch Acc: 0.596458, Tokens per Sec:     4682, Lr: 0.000300
2025-05-28 18:20:40,191 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.480217, Batch Acc: 0.600264, Tokens per Sec:     4670, Lr: 0.000300
2025-05-28 18:20:40,192 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:20:40,192 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:20:59,802 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.53, generation: 19.6044[sec], evaluation: 0.0000[sec]
2025-05-28 18:21:00,044 - INFO - joeynmt.helpers - delete models/transformer_word_2000/16000.ckpt
2025-05-28 18:21:00,046 - INFO - joeynmt.training - Example #0
2025-05-28 18:21:00,046 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:21:00,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:21:00,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:21:00,046 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:21:00,046 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:21:00,046 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years , has had the size of the United States <unk> , <unk> 40 percent .
2025-05-28 18:21:00,046 - INFO - joeynmt.training - Example #1
2025-05-28 18:21:00,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - Example #2
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a certain sense , the heart <unk> of climate change .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - Example #3
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - Example #4
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:21:00,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:21:00,047 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:21:14,520 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     1.370508, Batch Acc: 0.594965, Tokens per Sec:     4721, Lr: 0.000300
2025-05-28 18:21:29,447 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     1.378839, Batch Acc: 0.593656, Tokens per Sec:     4723, Lr: 0.000300
2025-05-28 18:21:43,977 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     1.213933, Batch Acc: 0.593575, Tokens per Sec:     4776, Lr: 0.000300
2025-05-28 18:21:58,797 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.218860, Batch Acc: 0.592615, Tokens per Sec:     4613, Lr: 0.000300
2025-05-28 18:22:13,574 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     1.394782, Batch Acc: 0.597636, Tokens per Sec:     4535, Lr: 0.000300
2025-05-28 18:22:13,576 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:22:13,576 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:22:33,736 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.53, generation: 20.1554[sec], evaluation: 0.0000[sec]
2025-05-28 18:22:33,959 - INFO - joeynmt.helpers - delete models/transformer_word_2000/15500.ckpt
2025-05-28 18:22:33,962 - INFO - joeynmt.training - Example #0
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', '<unk>', 'the', '<unk>', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has <unk> the <unk> of the United States <unk> , <unk> , <unk> <unk> <unk> .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - Example #1
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', "'re", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Hypothesis: We 're going to <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - Example #2
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart of climate system .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - Example #3
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:22:33,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:22:33,963 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:22:33,964 - INFO - joeynmt.training - Example #4
2025-05-28 18:22:33,964 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:22:33,964 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:22:33,964 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:22:33,964 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:22:33,964 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:22:33,964 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:22:48,917 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     1.136385, Batch Acc: 0.600289, Tokens per Sec:     4511, Lr: 0.000300
2025-05-28 18:23:03,845 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     1.307678, Batch Acc: 0.594988, Tokens per Sec:     4625, Lr: 0.000300
2025-05-28 18:23:18,643 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     1.391945, Batch Acc: 0.589627, Tokens per Sec:     4726, Lr: 0.000300
2025-05-28 18:23:33,200 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     1.352719, Batch Acc: 0.589472, Tokens per Sec:     4710, Lr: 0.000300
2025-05-28 18:23:48,163 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     1.297037, Batch Acc: 0.587146, Tokens per Sec:     4575, Lr: 0.000300
2025-05-28 18:23:48,163 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:23:48,163 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:24:09,347 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 21.1790[sec], evaluation: 0.0000[sec]
2025-05-28 18:24:09,349 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:24:09,583 - INFO - joeynmt.helpers - delete models/transformer_word_2000/15000.ckpt
2025-05-28 18:24:09,586 - INFO - joeynmt.training - Example #0
2025-05-28 18:24:09,586 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:24:09,586 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:24:09,586 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '40', 'percent', '<unk>', '.', '</s>']
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , 40 percent <unk> .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - Example #1
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - Example #2
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a certain sense , the heart <unk> of climate change .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - Example #3
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Hypothesis: You <unk> and <unk> <unk> .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - Example #4
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:24:09,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:24:09,587 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:24:09,588 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:24:24,209 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     1.360345, Batch Acc: 0.590329, Tokens per Sec:     4517, Lr: 0.000300
2025-05-28 18:24:38,991 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     1.311275, Batch Acc: 0.590453, Tokens per Sec:     4608, Lr: 0.000300
2025-05-28 18:24:54,288 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     1.340255, Batch Acc: 0.589572, Tokens per Sec:     4605, Lr: 0.000300
2025-05-28 18:25:10,522 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     1.222710, Batch Acc: 0.588802, Tokens per Sec:     4262, Lr: 0.000300
2025-05-28 18:25:27,558 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     1.420400, Batch Acc: 0.590432, Tokens per Sec:     4080, Lr: 0.000300
2025-05-28 18:25:27,559 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:25:27,559 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:25:50,973 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 23.4068[sec], evaluation: 0.0000[sec]
2025-05-28 18:25:50,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:25:51,262 - INFO - joeynmt.helpers - delete models/transformer_word_2000/19000.ckpt
2025-05-28 18:25:51,264 - INFO - joeynmt.training - Example #0
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> <unk> .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - Example #1
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', "'re", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Hypothesis: We 're going to <unk> this <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - Example #2
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is , in a way , the heart <unk> of climate change .
2025-05-28 18:25:51,265 - INFO - joeynmt.training - Example #3
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:25:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:25:51,266 - INFO - joeynmt.training - Example #4
2025-05-28 18:25:51,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:25:51,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:25:51,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:25:51,266 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:26:07,746 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     1.374179, Batch Acc: 0.587729, Tokens per Sec:     4176, Lr: 0.000300
2025-05-28 18:26:22,848 - INFO - joeynmt.training - Epoch   7, Step:    20700, Batch Loss:     1.386751, Batch Acc: 0.584255, Tokens per Sec:     4556, Lr: 0.000300
2025-05-28 18:26:38,133 - INFO - joeynmt.training - Epoch   7, Step:    20800, Batch Loss:     1.409475, Batch Acc: 0.589175, Tokens per Sec:     4627, Lr: 0.000300
2025-05-28 18:26:52,916 - INFO - joeynmt.training - Epoch   7, Step:    20900, Batch Loss:     1.366253, Batch Acc: 0.591325, Tokens per Sec:     4610, Lr: 0.000300
2025-05-28 18:27:07,423 - INFO - joeynmt.training - Epoch   7, Step:    21000, Batch Loss:     1.282045, Batch Acc: 0.587945, Tokens per Sec:     4756, Lr: 0.000300
2025-05-28 18:27:07,424 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:27:07,424 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:27:28,860 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 21.4319[sec], evaluation: 0.0000[sec]
2025-05-28 18:27:29,077 - INFO - joeynmt.helpers - delete models/transformer_word_2000/18500.ckpt
2025-05-28 18:27:29,081 - INFO - joeynmt.training - Example #0
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that the <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - Example #1
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'we', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Hypothesis: So , we <unk> this <unk> the <unk> of the problem because it 's not <unk> the <unk> of the ice .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - Example #2
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'global', 'warming', '.', '</s>']
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart of the global warming .
2025-05-28 18:27:29,081 - INFO - joeynmt.training - Example #3
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:27:29,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:27:29,082 - INFO - joeynmt.training - Example #4
2025-05-28 18:27:29,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:27:29,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:27:29,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:27:29,082 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years .
2025-05-28 18:27:44,388 - INFO - joeynmt.training - Epoch   7, Step:    21100, Batch Loss:     1.401917, Batch Acc: 0.582035, Tokens per Sec:     4427, Lr: 0.000300
2025-05-28 18:27:59,371 - INFO - joeynmt.training - Epoch   7, Step:    21200, Batch Loss:     1.580209, Batch Acc: 0.588014, Tokens per Sec:     4656, Lr: 0.000300
2025-05-28 18:28:14,464 - INFO - joeynmt.training - Epoch   7, Step:    21300, Batch Loss:     1.313768, Batch Acc: 0.587819, Tokens per Sec:     4545, Lr: 0.000300
2025-05-28 18:28:29,518 - INFO - joeynmt.training - Epoch   7, Step:    21400, Batch Loss:     1.476196, Batch Acc: 0.584149, Tokens per Sec:     4489, Lr: 0.000300
2025-05-28 18:28:44,846 - INFO - joeynmt.training - Epoch   7, Step:    21500, Batch Loss:     1.307808, Batch Acc: 0.588567, Tokens per Sec:     4530, Lr: 0.000300
2025-05-28 18:28:44,847 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:28:44,847 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:29:05,576 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.49, acc:   0.54, generation: 20.7243[sec], evaluation: 0.0000[sec]
2025-05-28 18:29:05,578 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:29:05,810 - INFO - joeynmt.helpers - delete models/transformer_word_2000/19500.ckpt
2025-05-28 18:29:05,814 - INFO - joeynmt.training - Example #0
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'been', 'the', 'size', 'of', 'the', 'United', 'States', ',', 'it', "'s", '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years , has been the size of the United States , it 's <unk> <unk> of 40 percent .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - Example #1
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:29:05,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> of the problem because it doesn 't show the <unk> of the ice .
2025-05-28 18:29:05,814 - INFO - joeynmt.training - Example #2
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - Example #3
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - Example #4
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:29:05,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:29:05,815 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the last 25 years .
2025-05-28 18:29:19,612 - INFO - joeynmt.training - Epoch   7: total training loss 4035.48
2025-05-28 18:29:19,613 - INFO - joeynmt.training - EPOCH 8
2025-05-28 18:29:20,282 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     1.274642, Batch Acc: 0.614281, Tokens per Sec:     4373, Lr: 0.000300
2025-05-28 18:29:34,475 - INFO - joeynmt.training - Epoch   8, Step:    21700, Batch Loss:     1.164263, Batch Acc: 0.604729, Tokens per Sec:     4854, Lr: 0.000300
2025-05-28 18:29:49,117 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     1.244655, Batch Acc: 0.606042, Tokens per Sec:     4610, Lr: 0.000300
2025-05-28 18:30:03,530 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     1.316619, Batch Acc: 0.606504, Tokens per Sec:     4971, Lr: 0.000300
2025-05-28 18:30:18,577 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     1.235486, Batch Acc: 0.609237, Tokens per Sec:     4575, Lr: 0.000300
2025-05-28 18:30:18,578 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:30:18,578 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:30:39,549 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 20.9659[sec], evaluation: 0.0000[sec]
2025-05-28 18:30:39,797 - INFO - joeynmt.helpers - delete models/transformer_word_2000/18000.ckpt
2025-05-28 18:30:39,801 - INFO - joeynmt.training - Example #0
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'was', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', '40', 'percent', '<unk>', '.', '</s>']
2025-05-28 18:30:39,801 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:30:39,801 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:30:39,801 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk> , which for almost three million years , it was the size of the United States <unk> , <unk> , <unk> <unk> , <unk> , <unk> , 40 percent <unk> .
2025-05-28 18:30:39,801 - INFO - joeynmt.training - Example #1
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:30:39,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', "'re", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', '<unk>', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Hypothesis: And we 're going to <unk> the <unk> of the <unk> because it doesn 't show the ice <unk> .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - Example #2
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , the heart <unk> of climate system .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - Example #3
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', 'they', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and they <unk> in the summer .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - Example #4
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:30:39,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:30:39,802 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:30:54,386 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     1.260004, Batch Acc: 0.606923, Tokens per Sec:     4658, Lr: 0.000300
2025-05-28 18:31:09,430 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.286401, Batch Acc: 0.602859, Tokens per Sec:     4684, Lr: 0.000300
2025-05-28 18:31:24,915 - INFO - joeynmt.training - Epoch   8, Step:    22300, Batch Loss:     1.283082, Batch Acc: 0.605949, Tokens per Sec:     4501, Lr: 0.000300
2025-05-28 18:31:40,035 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     1.256526, Batch Acc: 0.603303, Tokens per Sec:     4606, Lr: 0.000300
2025-05-28 18:31:54,835 - INFO - joeynmt.training - Epoch   8, Step:    22500, Batch Loss:     1.204248, Batch Acc: 0.599929, Tokens per Sec:     4777, Lr: 0.000300
2025-05-28 18:31:54,836 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:31:54,836 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:32:16,806 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.54, generation: 21.9653[sec], evaluation: 0.0000[sec]
2025-05-28 18:32:16,809 - INFO - joeynmt.training - Example #0
2025-05-28 18:32:16,809 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:32:16,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '</s>']
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States , <unk> the United States , <unk> <unk> , <unk> <unk> ,
2025-05-28 18:32:16,810 - INFO - joeynmt.training - Example #1
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', "'re", 'going', 'to', '<unk>', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Hypothesis: We 're going to <unk> the problem because it 's not <unk> the ice <unk> .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - Example #2
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , the heart of climate system .
2025-05-28 18:32:16,810 - INFO - joeynmt.training - Example #3
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:32:16,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:32:16,811 - INFO - joeynmt.training - Example #4
2025-05-28 18:32:16,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:32:16,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:32:16,811 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:32:16,811 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:32:31,494 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     1.483379, Batch Acc: 0.602400, Tokens per Sec:     4766, Lr: 0.000300
2025-05-28 18:32:46,068 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     1.193684, Batch Acc: 0.601881, Tokens per Sec:     4663, Lr: 0.000300
2025-05-28 18:33:01,022 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     1.240124, Batch Acc: 0.604293, Tokens per Sec:     4648, Lr: 0.000300
2025-05-28 18:33:16,100 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     1.232542, Batch Acc: 0.598768, Tokens per Sec:     4533, Lr: 0.000300
2025-05-28 18:33:30,910 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     1.228225, Batch Acc: 0.595891, Tokens per Sec:     4749, Lr: 0.000300
2025-05-28 18:33:30,911 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:33:30,911 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:33:53,279 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 22.3637[sec], evaluation: 0.0000[sec]
2025-05-28 18:33:53,491 - INFO - joeynmt.helpers - delete models/transformer_word_2000/21000.ckpt
2025-05-28 18:33:53,496 - INFO - joeynmt.training - Example #0
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '40', 'percent', '<unk>', '.', '</s>']
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk> , which for almost three million years , it has had the size of the United States <unk> , <unk> <unk> , 40 percent <unk> .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - Example #1
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> the problem because it doesn 't show the ice <unk> .
2025-05-28 18:33:53,496 - INFO - joeynmt.training - Example #2
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:33:53,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a way , the heart <unk> of climate system .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - Example #3
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - Example #4
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:33:53,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:33:53,497 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:34:09,633 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     1.307165, Batch Acc: 0.601237, Tokens per Sec:     4273, Lr: 0.000300
2025-05-28 18:34:26,406 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     1.491000, Batch Acc: 0.602242, Tokens per Sec:     4064, Lr: 0.000300
2025-05-28 18:34:43,174 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.416847, Batch Acc: 0.598051, Tokens per Sec:     4125, Lr: 0.000300
2025-05-28 18:35:00,478 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     1.299867, Batch Acc: 0.599951, Tokens per Sec:     3980, Lr: 0.000300
2025-05-28 18:35:16,522 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     1.248840, Batch Acc: 0.595102, Tokens per Sec:     4276, Lr: 0.000300
2025-05-28 18:35:16,523 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:35:16,523 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:35:39,423 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 22.8940[sec], evaluation: 0.0000[sec]
2025-05-28 18:35:39,548 - INFO - joeynmt.helpers - delete models/transformer_word_2000/23000.ckpt
2025-05-28 18:35:39,550 - INFO - joeynmt.helpers - delete /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/23000.ckpt
2025-05-28 18:35:39,550 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/23000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/23000.ckpt')
2025-05-28 18:35:39,551 - INFO - joeynmt.training - Example #0
2025-05-28 18:35:39,551 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:35:39,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:35:39,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> <unk> <unk> .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - Example #1
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'ice', '.', '</s>']
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the problem because it doesn 't show the <unk> of ice .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - Example #2
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', '<unk>', 'heart', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , <unk> heart of the global climate system .
2025-05-28 18:35:39,552 - INFO - joeynmt.training - Example #3
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:35:39,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:35:39,552 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:35:39,553 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:35:39,553 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:35:39,553 - INFO - joeynmt.training - Example #4
2025-05-28 18:35:39,553 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:35:39,553 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:35:39,553 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:35:39,553 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:35:39,553 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:35:39,553 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:35:55,820 - INFO - joeynmt.training - Epoch   8, Step:    23600, Batch Loss:     1.214639, Batch Acc: 0.597724, Tokens per Sec:     4126, Lr: 0.000300
2025-05-28 18:36:11,797 - INFO - joeynmt.training - Epoch   8, Step:    23700, Batch Loss:     1.260931, Batch Acc: 0.597239, Tokens per Sec:     4186, Lr: 0.000300
2025-05-28 18:36:27,726 - INFO - joeynmt.training - Epoch   8, Step:    23800, Batch Loss:     1.252257, Batch Acc: 0.596857, Tokens per Sec:     4382, Lr: 0.000300
2025-05-28 18:36:43,178 - INFO - joeynmt.training - Epoch   8, Step:    23900, Batch Loss:     1.227873, Batch Acc: 0.598942, Tokens per Sec:     4454, Lr: 0.000300
2025-05-28 18:36:58,710 - INFO - joeynmt.training - Epoch   8, Step:    24000, Batch Loss:     1.297876, Batch Acc: 0.594466, Tokens per Sec:     4354, Lr: 0.000300
2025-05-28 18:36:58,711 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:36:58,711 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:37:20,312 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.53, generation: 21.5952[sec], evaluation: 0.0000[sec]
2025-05-28 18:37:20,314 - INFO - joeynmt.training - Example #0
2025-05-28 18:37:20,314 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:37:20,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:37:20,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> 40 percent .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - Example #1
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this <unk> of the problem because it doesn 't show the <unk> of the ice .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - Example #2
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'global', 'system', '.', '</s>']
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart of the global system .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - Example #3
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> in summer .
2025-05-28 18:37:20,315 - INFO - joeynmt.training - Example #4
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:37:20,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:37:20,316 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:37:20,316 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:37:20,316 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 18:37:35,361 - INFO - joeynmt.training - Epoch   8, Step:    24100, Batch Loss:     1.196513, Batch Acc: 0.596205, Tokens per Sec:     4584, Lr: 0.000300
2025-05-28 18:37:50,546 - INFO - joeynmt.training - Epoch   8, Step:    24200, Batch Loss:     1.275883, Batch Acc: 0.599677, Tokens per Sec:     4564, Lr: 0.000300
2025-05-28 18:38:05,982 - INFO - joeynmt.training - Epoch   8, Step:    24300, Batch Loss:     1.318299, Batch Acc: 0.593772, Tokens per Sec:     4590, Lr: 0.000300
2025-05-28 18:38:21,641 - INFO - joeynmt.training - Epoch   8, Step:    24400, Batch Loss:     1.340732, Batch Acc: 0.595462, Tokens per Sec:     4320, Lr: 0.000300
2025-05-28 18:38:37,160 - INFO - joeynmt.training - Epoch   8, Step:    24500, Batch Loss:     1.509446, Batch Acc: 0.596412, Tokens per Sec:     4365, Lr: 0.000300
2025-05-28 18:38:37,160 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:38:37,160 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:39:01,000 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 23.8349[sec], evaluation: 0.0000[sec]
2025-05-28 18:39:01,003 - INFO - joeynmt.training - Example #0
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'had', 'the', 'size', 'of', 'three', 'million', 'years', ',', 'was', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:39:01,003 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:39:01,003 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:39:01,003 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years , has had the size of three million years , was the size of the United States <unk> , <unk> 40 percent .
2025-05-28 18:39:01,003 - INFO - joeynmt.training - Example #1
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:39:01,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '.', '</s>']
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Hypothesis: So , this <unk> the <unk> of the problem because it doesn 't show the ice .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - Example #2
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'some', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in some sense , the heart <unk> of global warming .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - Example #3
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> in summer .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - Example #4
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:39:01,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:39:01,004 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:39:17,172 - INFO - joeynmt.training - Epoch   8, Step:    24600, Batch Loss:     1.280009, Batch Acc: 0.590140, Tokens per Sec:     4291, Lr: 0.000300
2025-05-28 18:39:29,295 - INFO - joeynmt.training - Epoch   8: total training loss 3937.55
2025-05-28 18:39:29,296 - INFO - joeynmt.training - EPOCH 9
2025-05-28 18:39:33,088 - INFO - joeynmt.training - Epoch   9, Step:    24700, Batch Loss:     1.275642, Batch Acc: 0.623551, Tokens per Sec:     4370, Lr: 0.000300
2025-05-28 18:39:48,344 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     1.362582, Batch Acc: 0.621169, Tokens per Sec:     4463, Lr: 0.000300
2025-05-28 18:40:03,934 - INFO - joeynmt.training - Epoch   9, Step:    24900, Batch Loss:     1.241830, Batch Acc: 0.616131, Tokens per Sec:     4362, Lr: 0.000300
2025-05-28 18:40:19,672 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     1.276492, Batch Acc: 0.613197, Tokens per Sec:     4328, Lr: 0.000300
2025-05-28 18:40:19,673 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:40:19,673 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:40:40,218 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.52, acc:   0.54, generation: 20.5399[sec], evaluation: 0.0000[sec]
2025-05-28 18:40:40,329 - INFO - joeynmt.helpers - delete models/transformer_word_2000/22000.ckpt
2025-05-28 18:40:40,332 - INFO - joeynmt.training - Example #0
2025-05-28 18:40:40,332 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:40:40,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:40:40,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the <unk> <unk> <unk> , <unk> <unk> <unk> .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - Example #1
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> the <unk> of the problem because it 's not <unk> the <unk> of the ice .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - Example #2
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', '<unk>', 'heart', '<unk>', 'of', 'global', 'warming', 'system', '.', '</s>']
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is , in a sense , <unk> heart <unk> of global warming system .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - Example #3
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:40:40,333 - INFO - joeynmt.training - Example #4
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:40:40,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:40:40,334 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:40:40,334 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:40:40,334 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years .
2025-05-28 18:40:55,375 - INFO - joeynmt.training - Epoch   9, Step:    25100, Batch Loss:     1.174919, Batch Acc: 0.615839, Tokens per Sec:     4603, Lr: 0.000300
2025-05-28 18:41:10,201 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     1.218733, Batch Acc: 0.613893, Tokens per Sec:     4702, Lr: 0.000300
2025-05-28 18:41:24,932 - INFO - joeynmt.training - Epoch   9, Step:    25300, Batch Loss:     1.193092, Batch Acc: 0.612705, Tokens per Sec:     4667, Lr: 0.000300
2025-05-28 18:41:39,969 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     1.223372, Batch Acc: 0.607223, Tokens per Sec:     4442, Lr: 0.000300
2025-05-28 18:41:54,709 - INFO - joeynmt.training - Epoch   9, Step:    25500, Batch Loss:     1.238587, Batch Acc: 0.609438, Tokens per Sec:     4618, Lr: 0.000300
2025-05-28 18:41:54,710 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:41:54,711 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:42:13,732 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 19.0158[sec], evaluation: 0.0000[sec]
2025-05-28 18:42:13,839 - INFO - joeynmt.helpers - delete models/transformer_word_2000/23500.ckpt
2025-05-28 18:42:13,841 - INFO - joeynmt.training - Example #0
2025-05-28 18:42:13,841 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:42:13,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:42:13,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', 'been', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk> , which for almost three million years has been the size of the United States <unk> , has been <unk> 40 percent .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - Example #1
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'because', 'it', "'s", 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Hypothesis: And I 'm going to <unk> this <unk> because it 's not <unk> the ice <unk> .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - Example #2
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> of global warming .
2025-05-28 18:42:13,842 - INFO - joeynmt.training - Example #3
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:42:13,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:42:13,842 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:42:13,843 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:42:13,843 - INFO - joeynmt.training - 	Hypothesis: It 's <unk> and <unk> <unk> .
2025-05-28 18:42:13,843 - INFO - joeynmt.training - Example #4
2025-05-28 18:42:13,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:42:13,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:42:13,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:42:13,843 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:42:13,843 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:42:13,843 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:42:28,103 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     1.265077, Batch Acc: 0.610769, Tokens per Sec:     4838, Lr: 0.000300
2025-05-28 18:42:42,526 - INFO - joeynmt.training - Epoch   9, Step:    25700, Batch Loss:     1.222463, Batch Acc: 0.609326, Tokens per Sec:     4837, Lr: 0.000300
2025-05-28 18:42:56,768 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.172630, Batch Acc: 0.608072, Tokens per Sec:     4846, Lr: 0.000300
2025-05-28 18:43:10,922 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.300250, Batch Acc: 0.601152, Tokens per Sec:     4749, Lr: 0.000300
2025-05-28 18:43:25,945 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     1.283293, Batch Acc: 0.610212, Tokens per Sec:     4568, Lr: 0.000300
2025-05-28 18:43:25,947 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:43:25,947 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:43:47,592 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 21.6412[sec], evaluation: 0.0000[sec]
2025-05-28 18:43:47,596 - INFO - joeynmt.training - Example #0
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', '<unk>', 'of', 'the', 'United', 'States', ',', 'has', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has been the size of the United States <unk> , has <unk> of the United States , has <unk> <unk> .
2025-05-28 18:43:47,596 - INFO - joeynmt.training - Example #1
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:43:47,596 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'love', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'ice', '.', '</s>']
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:43:47,596 - INFO - joeynmt.training - 	Hypothesis: I love this <unk> the <unk> of the problem because it doesn 't show the <unk> of ice .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - Example #2
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is , in a sense , the heart <unk> of the global climate system .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - Example #3
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - Example #4
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:43:47,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:43:47,597 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:44:02,231 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.225873, Batch Acc: 0.610691, Tokens per Sec:     4577, Lr: 0.000210
2025-05-28 18:44:16,943 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     1.338099, Batch Acc: 0.614428, Tokens per Sec:     4790, Lr: 0.000210
2025-05-28 18:44:31,712 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     1.122420, Batch Acc: 0.615039, Tokens per Sec:     4768, Lr: 0.000210
2025-05-28 18:44:46,182 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     1.157251, Batch Acc: 0.615059, Tokens per Sec:     4816, Lr: 0.000210
2025-05-28 18:45:00,588 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.256117, Batch Acc: 0.612872, Tokens per Sec:     4799, Lr: 0.000210
2025-05-28 18:45:00,589 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:45:00,590 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:45:25,741 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.55, generation: 25.1464[sec], evaluation: 0.0000[sec]
2025-05-28 18:45:25,742 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:45:25,854 - INFO - joeynmt.helpers - delete models/transformer_word_2000/25500.ckpt
2025-05-28 18:45:25,857 - INFO - joeynmt.training - Example #0
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> that <unk> <unk> <unk> , which for about three million years has had the size of the United States <unk> , has <unk> <unk> <unk> .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - Example #1
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'we', "'re", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Hypothesis: So , we 're going to <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - Example #2
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:45:25,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:45:25,857 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> of global warming .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - Example #3
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - Example #4
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:45:25,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:45:25,858 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:45:42,181 - INFO - joeynmt.training - Epoch   9, Step:    26600, Batch Loss:     1.182756, Batch Acc: 0.605645, Tokens per Sec:     4097, Lr: 0.000210
2025-05-28 18:45:58,733 - INFO - joeynmt.training - Epoch   9, Step:    26700, Batch Loss:     1.203215, Batch Acc: 0.612188, Tokens per Sec:     4218, Lr: 0.000210
2025-05-28 18:46:18,299 - INFO - joeynmt.training - Epoch   9, Step:    26800, Batch Loss:     1.338425, Batch Acc: 0.610497, Tokens per Sec:     3575, Lr: 0.000210
2025-05-28 18:46:39,486 - INFO - joeynmt.training - Epoch   9, Step:    26900, Batch Loss:     1.280891, Batch Acc: 0.609675, Tokens per Sec:     3270, Lr: 0.000210
2025-05-28 18:46:58,983 - INFO - joeynmt.training - Epoch   9, Step:    27000, Batch Loss:     1.256764, Batch Acc: 0.612927, Tokens per Sec:     3535, Lr: 0.000210
2025-05-28 18:46:58,985 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:46:58,985 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:47:25,256 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.43, acc:   0.55, generation: 26.2656[sec], evaluation: 0.0000[sec]
2025-05-28 18:47:25,258 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 18:47:25,426 - INFO - joeynmt.helpers - delete models/transformer_word_2000/20000.ckpt
2025-05-28 18:47:25,432 - INFO - joeynmt.training - Example #0
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', '<unk>', 'percent', '.', '</s>']
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> that the <unk> <unk> <unk> , which for almost three million years , has had the size of the United States <unk> , has <unk> percent .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - Example #1
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', "'re", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - 	Hypothesis: We 're going to <unk> this <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:47:25,432 - INFO - joeynmt.training - Example #2
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:47:25,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'heart', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> heart of global warming .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - Example #3
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - Example #4
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:47:25,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:47:25,433 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:47:44,439 - INFO - joeynmt.training - Epoch   9, Step:    27100, Batch Loss:     1.161863, Batch Acc: 0.614637, Tokens per Sec:     3577, Lr: 0.000210
2025-05-28 18:48:03,079 - INFO - joeynmt.training - Epoch   9, Step:    27200, Batch Loss:     1.147535, Batch Acc: 0.611693, Tokens per Sec:     3714, Lr: 0.000210
2025-05-28 18:48:21,674 - INFO - joeynmt.training - Epoch   9, Step:    27300, Batch Loss:     1.151013, Batch Acc: 0.616440, Tokens per Sec:     3682, Lr: 0.000210
2025-05-28 18:48:40,217 - INFO - joeynmt.training - Epoch   9, Step:    27400, Batch Loss:     1.187016, Batch Acc: 0.611407, Tokens per Sec:     3689, Lr: 0.000210
2025-05-28 18:48:58,774 - INFO - joeynmt.training - Epoch   9, Step:    27500, Batch Loss:     1.098344, Batch Acc: 0.613046, Tokens per Sec:     3664, Lr: 0.000210
2025-05-28 18:48:58,775 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:48:58,775 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:49:24,866 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.54, generation: 26.0859[sec], evaluation: 0.0000[sec]
2025-05-28 18:49:25,051 - INFO - joeynmt.helpers - delete models/transformer_word_2000/20500.ckpt
2025-05-28 18:49:25,055 - INFO - joeynmt.training - Example #0
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:49:25,055 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:49:25,055 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:49:25,055 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> .
2025-05-28 18:49:25,055 - INFO - joeynmt.training - Example #1
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:49:25,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'the', 'problem', 'because', 'it', "'s", 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 18:49:25,055 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:49:25,055 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> the problem because it 's not <unk> the <unk> of the ice .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - Example #2
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', '<unk>', 'heart', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a certain sense , the <unk> heart of global warming .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - Example #3
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - Example #4
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:49:25,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:49:25,056 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:49:44,489 - INFO - joeynmt.training - Epoch   9, Step:    27600, Batch Loss:     1.366841, Batch Acc: 0.612666, Tokens per Sec:     3577, Lr: 0.000210
2025-05-28 18:50:03,032 - INFO - joeynmt.training - Epoch   9, Step:    27700, Batch Loss:     1.285782, Batch Acc: 0.613359, Tokens per Sec:     3827, Lr: 0.000210
2025-05-28 18:50:14,901 - INFO - joeynmt.training - Epoch   9: total training loss 3814.52
2025-05-28 18:50:14,903 - INFO - joeynmt.training - EPOCH 10
2025-05-28 18:50:21,700 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     1.149013, Batch Acc: 0.637298, Tokens per Sec:     3607, Lr: 0.000210
2025-05-28 18:50:40,656 - INFO - joeynmt.training - Epoch  10, Step:    27900, Batch Loss:     1.204260, Batch Acc: 0.637428, Tokens per Sec:     3678, Lr: 0.000210
2025-05-28 18:51:01,321 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     1.180174, Batch Acc: 0.637068, Tokens per Sec:     3271, Lr: 0.000210
2025-05-28 18:51:01,322 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:51:01,322 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:51:28,725 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.46, acc:   0.55, generation: 27.3968[sec], evaluation: 0.0000[sec]
2025-05-28 18:51:28,902 - INFO - joeynmt.helpers - delete models/transformer_word_2000/25000.ckpt
2025-05-28 18:51:28,906 - INFO - joeynmt.training - Example #0
2025-05-28 18:51:28,906 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:51:28,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:51:28,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'it', "'s", '<unk>', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years , it 's <unk> the size of the United States <unk> , has <unk> 40 percent .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - Example #1
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this problem because it doesn 't show the ice <unk> .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - Example #2
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', '<unk>', 'heart', '<unk>', '.', '</s>']
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , <unk> heart <unk> .
2025-05-28 18:51:28,907 - INFO - joeynmt.training - Example #3
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:51:28,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Hypothesis: You <unk> <unk> and <unk> <unk> .
2025-05-28 18:51:28,908 - INFO - joeynmt.training - Example #4
2025-05-28 18:51:28,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:51:28,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:51:28,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:51:28,908 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:51:49,204 - INFO - joeynmt.training - Epoch  10, Step:    28100, Batch Loss:     1.026000, Batch Acc: 0.632771, Tokens per Sec:     3356, Lr: 0.000210
2025-05-28 18:52:09,323 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     1.053765, Batch Acc: 0.637240, Tokens per Sec:     3484, Lr: 0.000210
2025-05-28 18:52:28,902 - INFO - joeynmt.training - Epoch  10, Step:    28300, Batch Loss:     1.442780, Batch Acc: 0.631605, Tokens per Sec:     3541, Lr: 0.000210
2025-05-28 18:52:48,309 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     1.132984, Batch Acc: 0.629474, Tokens per Sec:     3649, Lr: 0.000210
2025-05-28 18:53:07,591 - INFO - joeynmt.training - Epoch  10, Step:    28500, Batch Loss:     1.273694, Batch Acc: 0.634225, Tokens per Sec:     3520, Lr: 0.000210
2025-05-28 18:53:07,592 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:53:07,592 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:53:34,820 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.47, acc:   0.54, generation: 27.2215[sec], evaluation: 0.0000[sec]
2025-05-28 18:53:34,993 - INFO - joeynmt.helpers - delete models/transformer_word_2000/21500.ckpt
2025-05-28 18:53:34,996 - INFO - joeynmt.training - Example #0
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '40', 'percent', '.', '</s>']
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> , 40 percent .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - Example #1
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - 	Hypothesis: So this <unk> <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:53:34,997 - INFO - joeynmt.training - Example #2
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:53:34,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', '<unk>', 'heart', 'of', 'global', 'warming', '.', '</s>']
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , <unk> heart of global warming .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - Example #3
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - Example #4
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:53:34,998 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:53:34,998 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years .
2025-05-28 18:53:53,609 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     1.227647, Batch Acc: 0.630836, Tokens per Sec:     3639, Lr: 0.000210
2025-05-28 18:54:13,599 - INFO - joeynmt.training - Epoch  10, Step:    28700, Batch Loss:     1.183403, Batch Acc: 0.626807, Tokens per Sec:     3423, Lr: 0.000210
2025-05-28 18:54:32,697 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.107601, Batch Acc: 0.634726, Tokens per Sec:     3616, Lr: 0.000210
2025-05-28 18:54:51,779 - INFO - joeynmt.training - Epoch  10, Step:    28900, Batch Loss:     1.138876, Batch Acc: 0.629961, Tokens per Sec:     3674, Lr: 0.000210
2025-05-28 18:55:10,229 - INFO - joeynmt.training - Epoch  10, Step:    29000, Batch Loss:     1.080372, Batch Acc: 0.630069, Tokens per Sec:     3870, Lr: 0.000210
2025-05-28 18:55:10,231 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:55:10,231 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:55:37,019 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.48, acc:   0.55, generation: 26.7826[sec], evaluation: 0.0000[sec]
2025-05-28 18:55:37,022 - INFO - joeynmt.training - Example #0
2025-05-28 18:55:37,022 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:55:37,022 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:55:37,022 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:55:37,022 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> 40 percent .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - Example #1
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Hypothesis: I 'm going to <unk> this <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - Example #2
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', 'system', '.', '</s>']
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> of global warming system .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - Example #3
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:55:37,023 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'in', 'summer', '.', '</s>']
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - 	Hypothesis: They <unk> in summer .
2025-05-28 18:55:37,023 - INFO - joeynmt.training - Example #4
2025-05-28 18:55:37,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:55:37,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:55:37,024 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:55:37,024 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:55:37,024 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:55:37,024 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years .
2025-05-28 18:55:55,451 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     1.204451, Batch Acc: 0.630439, Tokens per Sec:     3766, Lr: 0.000210
2025-05-28 18:56:11,192 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     1.150539, Batch Acc: 0.626796, Tokens per Sec:     4342, Lr: 0.000210
2025-05-28 18:56:25,784 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     1.312014, Batch Acc: 0.627768, Tokens per Sec:     4692, Lr: 0.000210
2025-05-28 18:56:40,235 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     1.112022, Batch Acc: 0.628651, Tokens per Sec:     4746, Lr: 0.000210
2025-05-28 18:56:54,084 - INFO - joeynmt.training - Epoch  10, Step:    29500, Batch Loss:     1.207896, Batch Acc: 0.625366, Tokens per Sec:     5051, Lr: 0.000210
2025-05-28 18:56:54,085 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:56:54,085 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:57:12,280 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.48, acc:   0.54, generation: 18.1904[sec], evaluation: 0.0000[sec]
2025-05-28 18:57:12,283 - INFO - joeynmt.training - Example #0
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', '<unk>', '<unk>', '40', 'percent', '.', '</s>']
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , has <unk> <unk> 40 percent .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - Example #1
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', "'re", 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', "'s", 'not', 'showing', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Hypothesis: We 're going to <unk> the <unk> of the problem because it 's not showing the ice <unk> .
2025-05-28 18:57:12,283 - INFO - joeynmt.training - Example #2
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:57:12,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'global', 'climate', 'system', '.', '</s>']
2025-05-28 18:57:12,283 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart <unk> of global climate system .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - Example #3
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - Example #4
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:57:12,284 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:57:12,284 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:57:26,667 - INFO - joeynmt.training - Epoch  10, Step:    29600, Batch Loss:     1.229404, Batch Acc: 0.626873, Tokens per Sec:     4760, Lr: 0.000210
2025-05-28 18:57:41,736 - INFO - joeynmt.training - Epoch  10, Step:    29700, Batch Loss:     1.314541, Batch Acc: 0.622618, Tokens per Sec:     4461, Lr: 0.000210
2025-05-28 18:57:57,283 - INFO - joeynmt.training - Epoch  10, Step:    29800, Batch Loss:     1.277582, Batch Acc: 0.626928, Tokens per Sec:     4417, Lr: 0.000210
2025-05-28 18:58:12,549 - INFO - joeynmt.training - Epoch  10, Step:    29900, Batch Loss:     1.143465, Batch Acc: 0.624476, Tokens per Sec:     4626, Lr: 0.000210
2025-05-28 18:58:30,206 - INFO - joeynmt.training - Epoch  10, Step:    30000, Batch Loss:     1.225850, Batch Acc: 0.626257, Tokens per Sec:     3904, Lr: 0.000210
2025-05-28 18:58:30,207 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 18:58:30,207 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 18:58:52,976 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.46, acc:   0.55, generation: 22.7633[sec], evaluation: 0.0000[sec]
2025-05-28 18:58:53,167 - INFO - joeynmt.helpers - delete models/transformer_word_2000/28500.ckpt
2025-05-28 18:58:53,169 - INFO - joeynmt.helpers - delete /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/28500.ckpt
2025-05-28 18:58:53,169 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/28500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/28500.ckpt')
2025-05-28 18:58:53,171 - INFO - joeynmt.training - Example #0
2025-05-28 18:58:53,171 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 18:58:53,171 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 18:58:53,171 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> <unk> .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - Example #1
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'love', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', '<unk>', '.', '</s>']
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Hypothesis: I love this <unk> the <unk> of the problem because it doesn 't show the ice <unk> .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - Example #2
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'global', 'warming', 'warming', '.', '</s>']
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a sense , global warming warming .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - Example #3
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 18:58:53,172 - INFO - joeynmt.training - Example #4
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 18:58:53,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 18:58:53,173 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 18:58:53,173 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 18:58:53,173 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 18:59:10,815 - INFO - joeynmt.training - Epoch  10, Step:    30100, Batch Loss:     1.144658, Batch Acc: 0.624269, Tokens per Sec:     3875, Lr: 0.000210
2025-05-28 18:59:29,469 - INFO - joeynmt.training - Epoch  10, Step:    30200, Batch Loss:     1.271191, Batch Acc: 0.622058, Tokens per Sec:     3692, Lr: 0.000210
2025-05-28 18:59:45,874 - INFO - joeynmt.training - Epoch  10, Step:    30300, Batch Loss:     1.156998, Batch Acc: 0.620181, Tokens per Sec:     4005, Lr: 0.000210
2025-05-28 19:00:03,812 - INFO - joeynmt.training - Epoch  10, Step:    30400, Batch Loss:     1.226773, Batch Acc: 0.622345, Tokens per Sec:     3956, Lr: 0.000210
2025-05-28 19:00:22,953 - INFO - joeynmt.training - Epoch  10, Step:    30500, Batch Loss:     1.157412, Batch Acc: 0.623654, Tokens per Sec:     3567, Lr: 0.000210
2025-05-28 19:00:22,954 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 19:00:22,954 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 19:00:51,566 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.47, acc:   0.55, generation: 28.6059[sec], evaluation: 0.0000[sec]
2025-05-28 19:00:51,569 - INFO - joeynmt.training - Example #0
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> , which for almost three million years has had the size of the United States <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> of 40 percent .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - Example #1
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'love', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Hypothesis: I love this <unk> the <unk> of the problem because it doesn 't show the <unk> of the ice .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - Example #2
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-28 19:00:51,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'global', 'system', '.', '</s>']
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 19:00:51,570 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is , in a way , the heart of the global system .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - Example #3
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', "d'", 'inverno', 'e', 'si', 'ritira', "d'", 'estate', '.']
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk> .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - Example #4
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-28 19:00:51,571 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 19:00:51,571 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years .
2025-05-28 19:01:11,950 - INFO - joeynmt.training - Epoch  10, Step:    30600, Batch Loss:     1.216509, Batch Acc: 0.625060, Tokens per Sec:     3345, Lr: 0.000210
2025-05-28 19:01:29,009 - INFO - joeynmt.training - Epoch  10, Step:    30700, Batch Loss:     1.405338, Batch Acc: 0.619172, Tokens per Sec:     4003, Lr: 0.000210
2025-05-28 19:01:45,632 - INFO - joeynmt.training - Epoch  10, Step:    30800, Batch Loss:     1.092695, Batch Acc: 0.621935, Tokens per Sec:     4132, Lr: 0.000210
2025-05-28 19:01:54,015 - INFO - joeynmt.training - Epoch  10: total training loss 3659.24
2025-05-28 19:01:54,015 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-28 19:01:54,015 - INFO - joeynmt.training - Best validation result (greedy) at step    27000:   4.43 ppl.
2025-05-28 19:01:54,058 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 19:01:54,110 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 19:01:54,169 - INFO - joeynmt.helpers - Load model from /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/27000.ckpt.
2025-05-28 19:01:54,197 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=None)
2025-05-28 19:01:54,208 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-28 19:01:54,208 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 19:01:54,208 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 19:02:18,182 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 23.9693[sec], evaluation: 0.0000[sec]
2025-05-28 19:02:18,184 - INFO - joeynmt.prediction - Translations saved to: /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/00027000.hyps.dev.
2025-05-28 19:02:18,184 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-28 19:02:18,184 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 19:02:18,184 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 19:02:58,888 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 40.6952[sec], evaluation: 0.0000[sec]
2025-05-28 19:02:58,891 - INFO - joeynmt.prediction - Translations saved to: /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_word_2000/00027000.hyps.test.
