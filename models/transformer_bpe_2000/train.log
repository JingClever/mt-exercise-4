2025-05-28 19:54:01,140 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                           cfg.name : transformer_bpe_2k_config
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                     cfg.data.train : data_word_level/train
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                       cfg.data.dev : data_word_level/dev
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                      cfg.data.test : data_word_level/test
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : shared_models/joint-vocab.txt
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data_word_level/codes2000.bpe
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : shared_models/joint-vocab.txt
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data_word_level/codes2000.bpe
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-28 19:54:01,140 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/transformer_bpe_2000
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-28 19:54:01,141 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-28 19:54:01,143 - INFO - joeynmt.data - Building tokenizer...
2025-05-28 19:54:01,146 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 19:54:01,146 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 19:54:01,147 - INFO - joeynmt.data - Loading train set...
2025-05-28 19:54:01,238 - INFO - joeynmt.data - Building vocabulary...
2025-05-28 19:54:01,271 - INFO - joeynmt.data - Loading dev set...
2025-05-28 19:54:01,273 - INFO - joeynmt.data - Loading test set...
2025-05-28 19:54:01,275 - INFO - joeynmt.data - Data loaded.
2025-05-28 19:54:01,275 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 19:54:01,276 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 19:54:01,276 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-28 19:54:01,276 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ ore : ar@@ re@@ stiamo il ri@@ scal@@ d@@ amento glob@@ ale
	[TRG] A@@ l G@@ ore : A@@ ver@@ ting the cli@@ m@@ ate c@@ ris@@ is
2025-05-28 19:54:01,276 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) a (7) the (8) to (9) di
2025-05-28 19:54:01,276 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) a (7) the (8) to (9) di
2025-05-28 19:54:01,276 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2222
2025-05-28 19:54:01,276 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2222
2025-05-28 19:54:01,277 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 19:54:01,332 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 19:54:01,334 - INFO - joeynmt.model - Total params: 3468032
2025-05-28 19:54:01,334 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-28 19:54:01,334 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2222),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2222),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-28 19:54:01,334 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-28 19:54:01,335 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-28 19:54:01,335 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-28 19:54:01,335 - INFO - joeynmt.training - EPOCH 1
2025-05-28 19:54:16,297 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.951913, Batch Acc: 0.064256, Tokens per Sec:     4819, Lr: 0.000300
2025-05-28 19:54:30,603 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.852088, Batch Acc: 0.087274, Tokens per Sec:     5003, Lr: 0.000300
2025-05-28 19:54:44,913 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.606369, Batch Acc: 0.102401, Tokens per Sec:     5038, Lr: 0.000300
2025-05-28 19:54:59,798 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.572240, Batch Acc: 0.113124, Tokens per Sec:     4763, Lr: 0.000300
2025-05-28 19:55:14,118 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.400850, Batch Acc: 0.126686, Tokens per Sec:     5080, Lr: 0.000300
2025-05-28 19:55:14,119 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 19:55:14,119 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 19:56:33,051 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.46, ppl:  31.76, acc:   0.12, generation: 78.9204[sec], evaluation: 0.0000[sec]
2025-05-28 19:56:33,053 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 19:56:33,197 - INFO - joeynmt.training - Example #0
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', ',', 'I', 'think', ',', 'I', "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'world', ',', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'world', ',', 'and', 'the', 'world', '.', '</s>']
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Hypothesis: And I think , I think , I 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've the the the the the the the the the the the the world , the the the the the the the the the the the the the the the the the the the the the the the world , and the world .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - Example #1
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'that', 'that', 'that', 'that', 'that', 'I', 'think', 'that', 'that', 'that', 'that', 'I', 'think', 'I', 'think', ',', 'and', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'world', '.', '</s>']
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Hypothesis: And I think I think that that that that that I think that that that that I think I think , and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the world .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - Example #2
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", 'be', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 'w@@', 's', '.', '</s>']
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - 	Hypothesis: And I 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've be a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a wwwwwwwwwwwwws .
2025-05-28 19:56:33,198 - INFO - joeynmt.training - Example #3
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 19:56:33,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", 'be', '.', '</s>']
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Hypothesis: And I 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've be .
2025-05-28 19:56:33,199 - INFO - joeynmt.training - Example #4
2025-05-28 19:56:33,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 19:56:33,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 19:56:33,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", "'ve", 'be', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'f@@']
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 19:56:33,199 - INFO - joeynmt.training - 	Hypothesis: And I 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've 've be a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a f
2025-05-28 19:56:47,455 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.313848, Batch Acc: 0.139348, Tokens per Sec:     5080, Lr: 0.000300
2025-05-28 19:57:02,441 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.346397, Batch Acc: 0.144521, Tokens per Sec:     4852, Lr: 0.000300
2025-05-28 19:57:16,877 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.362329, Batch Acc: 0.156284, Tokens per Sec:     4834, Lr: 0.000300
2025-05-28 19:57:31,461 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.095882, Batch Acc: 0.164465, Tokens per Sec:     4942, Lr: 0.000300
2025-05-28 19:57:46,039 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.169305, Batch Acc: 0.180208, Tokens per Sec:     4926, Lr: 0.000300
2025-05-28 19:57:46,041 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 19:57:46,041 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 19:59:04,700 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.16, ppl:  23.48, acc:   0.18, generation: 78.6507[sec], evaluation: 0.0000[sec]
2025-05-28 19:59:04,703 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 19:59:04,849 - INFO - joeynmt.training - Example #0
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'world', '.', '</s>']
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Hypothesis: The The The The The The The The The The The The The The The The The The The The The The world .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - Example #1
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 19:59:04,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', '.', '</s>']
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - 	Hypothesis: So , the world is the world is the world is the world .
2025-05-28 19:59:04,849 - INFO - joeynmt.training - Example #2
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'world', '.', '</s>']
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Hypothesis: The The The The The The The The The The The The The The The world .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - Example #3
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'The', 'The', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 'ed', '.', '</s>']
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Hypothesis: The The The ssssssssssssssssssssssssssed .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - Example #4
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 19:59:04,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'the', 'world', '.', '</s>']
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 19:59:04,850 - INFO - joeynmt.training - 	Hypothesis: The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The the world .
2025-05-28 19:59:19,586 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.943269, Batch Acc: 0.195506, Tokens per Sec:     4835, Lr: 0.000300
2025-05-28 19:59:34,039 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.913331, Batch Acc: 0.208923, Tokens per Sec:     4884, Lr: 0.000300
2025-05-28 19:59:48,789 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.917241, Batch Acc: 0.220463, Tokens per Sec:     4908, Lr: 0.000300
2025-05-28 20:00:03,761 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.728441, Batch Acc: 0.228666, Tokens per Sec:     4837, Lr: 0.000300
2025-05-28 20:00:18,060 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.700515, Batch Acc: 0.235925, Tokens per Sec:     5200, Lr: 0.000300
2025-05-28 20:00:18,061 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:00:18,061 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:01:15,206 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.85, ppl:  17.25, acc:   0.23, generation: 57.1397[sec], evaluation: 0.0000[sec]
2025-05-28 20:01:15,208 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:01:15,352 - INFO - joeynmt.training - Example #0
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'I', 'was', 'a', 'little', 'bit', 'of', 'the', 'world', 'are', 'the', 'world', 'is', 'that', 'we', "'re", 'going', 'to', 'be', 'to', 'be', 'to', 'be', 'a', 'little', 'bit', 'of', 'the', 'world', ',', 'in', 'the', 'world', ',', 'in', 'the', 'world', ',', 'in', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', ',', 'in', 'the', 'world', 'is', ',', '</s>']
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Hypothesis: So , I was a little bit of the world are the world is that we 're going to be to be to be a little bit of the world , in the world , in the world , in the world is the world is the world , in the world is ,
2025-05-28 20:01:15,354 - INFO - joeynmt.training - Example #1
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:01:15,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', 'be', 'this', 'is', 'that', 'I', "'m", 'not', 'the', 'world', 'is', 'that', 'I', "'m", 'not', 'to', 'be', 'to', 'be', '.', '</s>']
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:01:15,354 - INFO - joeynmt.training - 	Hypothesis: I 'm going to be this is that I 'm not the world is that I 'm not to be to be .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - Example #2
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'a', 'little', 'bit', 'of', 'the', 'world', 'is', ',', 'is', ',', 'it', "'s", 'a', 'little', 'bit', 'of', 'the', 'world', ',', 'is', 'the', 'world', '.', '</s>']
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Hypothesis: The first thing is a little bit of the world is , is , it 's a little bit of the world , is the world .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - Example #3
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 'little', 'bit', 'of', 'the', 'world', '.', '</s>']
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Hypothesis: It 's a little bit of the world .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - Example #4
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:01:15,355 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', "'s", 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 'first', 'years', '.', '</s>']
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:01:15,355 - INFO - joeynmt.training - 	Hypothesis: And it 's a little bit of a little bit of a little bit of the first years .
2025-05-28 20:01:30,150 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.719842, Batch Acc: 0.243947, Tokens per Sec:     4790, Lr: 0.000300
2025-05-28 20:01:45,076 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.729240, Batch Acc: 0.254295, Tokens per Sec:     4817, Lr: 0.000300
2025-05-28 20:01:59,773 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.660307, Batch Acc: 0.264149, Tokens per Sec:     4815, Lr: 0.000300
2025-05-28 20:02:14,349 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.584093, Batch Acc: 0.273857, Tokens per Sec:     4814, Lr: 0.000300
2025-05-28 20:02:28,908 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.595798, Batch Acc: 0.282411, Tokens per Sec:     4934, Lr: 0.000300
2025-05-28 20:02:28,909 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:02:28,909 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:03:11,166 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.66, ppl:  14.29, acc:   0.27, generation: 42.2503[sec], evaluation: 0.0000[sec]
2025-05-28 20:03:11,168 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:03:11,316 - INFO - joeynmt.training - Example #0
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'time', 'I', 'was', 'I', 'was', 'a', 'co@@', 'ver@@', 'ed', 'to', 'do', 'this', 'is', 'the', 'first', 'time', 'that', 'the', 'c@@', 'ar', 'of', 'the', 'first', 'time', ',', 'for', 'three', 'million', ',', 'which', 'was', 'three', 'million', 'years', ',', 'which', 'was', 'the', 'last', '1@@', '0@@', '0@@', '0@@', 's', 'of', 'the', 'first', 'time', ',', 'the', 'first', 'time', '.', '</s>']
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Hypothesis: The first time I was I was a covered to do this is the first time that the car of the first time , for three million , which was three million years , which was the last 1000s of the first time , the first time .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - Example #1
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', 'be', 'this', 'is', 'the', 'c@@', 'le@@', 'ast', 'of', 'the', 'c@@', 'ity', 'because', 'I', 'don', "'t", 'have', 'a', 'lot', 'of', 'the', 'w@@', 'a@@', 'a@@', 'a@@', 'il@@', 'ed', '.', '</s>']
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Hypothesis: I 'm going to be this is the cleast of the city because I don 't have a lot of the waaailed .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - Example #2
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:03:11,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'b@@', 'ad', 'of', 'the', 'c@@', 'ar', '.', 'It', "'s", 'a', 'c@@', 'ar', '.', 'It', "'s", 'a', 'lot', 'of', 'the', 'c@@', 'ar', 'of', 'the', 'c@@', 'l@@', 'l@@', 'l@@', 'ater', '.', '</s>']
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - 	Hypothesis: The bad of the car . It 's a car . It 's a lot of the car of the clllater .
2025-05-28 20:03:11,317 - INFO - joeynmt.training - Example #3
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 'w@@', 'in@@', 'side', 'of', 'the', 'c@@', 'ar', 'and', 'the', 'c@@', 'ar', '.', '</s>']
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Hypothesis: It 's a winside of the car and the car .
2025-05-28 20:03:11,318 - INFO - joeynmt.training - Example #4
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:03:11,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'not', 'a', 'h@@', 'un@@', 'ti@@', 'l', 'of', 'the', 'h@@', 'un@@', 'ti@@', 'c', ',', 'I', "'m", 'going', 'to', 'the', 'first', 'years', '.', '</s>']
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:03:11,318 - INFO - joeynmt.training - 	Hypothesis: The first thing is not a huntil of the huntic , I 'm going to the first years .
2025-05-28 20:03:26,308 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.547971, Batch Acc: 0.287170, Tokens per Sec:     4757, Lr: 0.000300
2025-05-28 20:03:41,413 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.572250, Batch Acc: 0.292332, Tokens per Sec:     4781, Lr: 0.000300
2025-05-28 20:03:56,601 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.406354, Batch Acc: 0.299104, Tokens per Sec:     4724, Lr: 0.000300
2025-05-28 20:04:11,368 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.466162, Batch Acc: 0.313093, Tokens per Sec:     4968, Lr: 0.000300
2025-05-28 20:04:26,575 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.461536, Batch Acc: 0.316069, Tokens per Sec:     4651, Lr: 0.000300
2025-05-28 20:04:26,576 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:04:26,576 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:05:18,010 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.51, ppl:  12.34, acc:   0.30, generation: 51.4267[sec], evaluation: 0.0000[sec]
2025-05-28 20:05:18,012 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:05:18,146 - INFO - joeynmt.training - Example #0
2025-05-28 20:05:18,146 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:05:18,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:05:18,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'other', 'thing', 'I', "'ve", 'got', 'to', 'show', 'these', 'these', 'are', 'the', 'w@@', 'a@@', 'king', 'to', 're@@', 'memb@@', 'er', 'that', 'the', 'c@@', 'ro@@', 'w@@', 'th', 'of', 'the', 'res@@', 's', ',', 'which', 'is', 'the', 'last', 'million', 'years', ',', 'which', 'has', 'been', '1@@', '0@@', '0@@', '0@@', '0@@', '0@@', 's', ',', 'is', 'the', '19@@', '6@@', '0@@', 's', ',', 'is', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Hypothesis: The other thing I 've got to show these these are the waking to remember that the crowth of the ress , which is the last million years , which has been 100000s , is the 1960s , is the United States .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - Example #1
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', 'this', 'is', 'the', 'problem', 'of', 'the', 'problem', 'because', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'have', 'to', 'do', 'it', '.', '</s>']
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Hypothesis: I 'm going to this is the problem of the problem because of the problem because I don 't have to do it .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - Example #2
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 'qu@@', 'ick@@', 'ly', ',', 'is', 'a', 'c@@', 'le@@', 'ar', ',', 'is', 'a', 'sense', 'of', 'the', 'way', ',', 'is', 'the', 's@@', 'aying', 'of', 'the', 'world', '.', '</s>']
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Hypothesis: The squickly , is a clear , is a sense of the way , is the saying of the world .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - Example #3
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 's@@', 'qu@@', 'ick@@', 'ly', ',', 'and', 'you', 'can', 'see', 'the', 'w@@', 'as@@', 'n', '.', '</s>']
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - 	Hypothesis: It 's a squickly , and you can see the wasn .
2025-05-28 20:05:18,147 - INFO - joeynmt.training - Example #4
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:05:18,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'is', 'the', 'w@@', 'a@@', 'th@@', 'y', 'is', 'a', 'little', 'bit', 'of', 'the', 'last', 'last', 'last', 'last', 'last', '1@@', '0@@', '0@@', '0@@', '0@@', 's', '.', '</s>']
2025-05-28 20:05:18,148 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:05:18,148 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:05:18,148 - INFO - joeynmt.training - 	Hypothesis: The first is the wathy is a little bit of the last last last last last 10000s .
2025-05-28 20:05:32,831 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.399510, Batch Acc: 0.325946, Tokens per Sec:     4920, Lr: 0.000300
2025-05-28 20:05:47,429 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.366287, Batch Acc: 0.336817, Tokens per Sec:     4982, Lr: 0.000300
2025-05-28 20:06:02,756 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.203589, Batch Acc: 0.336274, Tokens per Sec:     4586, Lr: 0.000300
2025-05-28 20:06:17,622 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.204587, Batch Acc: 0.343622, Tokens per Sec:     4736, Lr: 0.000300
2025-05-28 20:06:32,789 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.319894, Batch Acc: 0.355649, Tokens per Sec:     4730, Lr: 0.000300
2025-05-28 20:06:32,789 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:06:32,789 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:07:05,992 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.38, ppl:  10.82, acc:   0.34, generation: 33.1969[sec], evaluation: 0.0000[sec]
2025-05-28 20:07:05,995 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:07:06,133 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/500.ckpt
2025-05-28 20:07:06,136 - INFO - joeynmt.training - Example #0
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'went', 'to', 'show', 'you', 'these', 'these', 'are', 'going', 'to', 'show', 'you', 'that', 'the', 'c@@', 'l@@', 'l@@', 'ap@@', 'e', 'that', 'the', 'h@@', 'y@@', 'g@@', 'ets', ',', 'which', 'is', 'three', 'million', 'years', ',', 'which', 'was', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'is', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:07:06,136 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:07:06,136 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:07:06,136 - INFO - joeynmt.training - 	Hypothesis: The year I went to show you these these are going to show you that the cllape that the hygets , which is three million years , which was three million years of the United States , the United States is the United States .
2025-05-28 20:07:06,136 - INFO - joeynmt.training - Example #1
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:07:06,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', "'m", 'going', 'to', 'be', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'you', 'don', "'t", 'want', 'to', 'show', 'you', 'to', 'be', 'a', 't@@', 'ac@@', 'i@@', 'ous', '.', '</s>']
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Hypothesis: And I 'm going to be the gravity of the problem because you don 't want to show you to be a tacious .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - Example #2
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'l@@', 'l@@', 'l@@', 'l@@', 'l@@', 'ater', 'is', ',', 'the', 'p@@', 'ut@@', 'ting', ',', 'the', 'p@@', 'ur@@', 'ther', 'of', 'the', 'system', '.', '</s>']
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Hypothesis: The gllllllater is , the putting , the purther of the system .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - Example #3
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'h@@', 'un@@', 'dre@@', 'ds', 'of', 'the', 'p@@', 'ur@@', 'ther', 'and', 'you', "'re", 'going', 'to', 'be', '.', '</s>']
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Hypothesis: It 's hundreds of the purther and you 're going to be .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - Example #4
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:07:06,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'co@@', 'p@@', 'ed', 'to', 'be', 'a', 'b@@', 'l@@', 'l@@', 'l@@', 'l@@', 'l@@', 'ater', ',', 'the', 'last', '1@@', '5', 'years', 'ago', '.', '</s>']
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:07:06,137 - INFO - joeynmt.training - 	Hypothesis: The next next to be a coped to be a blllllater , the last 15 years ago .
2025-05-28 20:07:21,127 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.246029, Batch Acc: 0.361042, Tokens per Sec:     4819, Lr: 0.000300
2025-05-28 20:07:36,021 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.431013, Batch Acc: 0.365153, Tokens per Sec:     4785, Lr: 0.000300
2025-05-28 20:07:50,759 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.136117, Batch Acc: 0.373495, Tokens per Sec:     4830, Lr: 0.000300
2025-05-28 20:08:05,991 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.104300, Batch Acc: 0.376755, Tokens per Sec:     4775, Lr: 0.000300
2025-05-28 20:08:20,717 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.072519, Batch Acc: 0.388173, Tokens per Sec:     4968, Lr: 0.000300
2025-05-28 20:08:20,718 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:08:20,718 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:09:09,819 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.26, ppl:   9.61, acc:   0.37, generation: 49.0945[sec], evaluation: 0.0000[sec]
2025-05-28 20:09:09,821 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:09:09,965 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/1000.ckpt
2025-05-28 20:09:09,969 - INFO - joeynmt.training - Example #0
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'got', 'this', 're@@', 'memb@@', 'er', 'of', 'the', 'g@@', 'ame', 'for', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'is', 'a', 'lot', 'of', 'the', 'si@@', 'x', 'million', 'years', 'of', 'the', 'si@@', 'ons', 'of', 'the', 'si@@', 'ons', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Hypothesis: The year I 've got this remember of the game for show that the glacial , which is a lot of the six million years of the sions of the sions of the United States , it 's the United States , it 's the United States .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - Example #1
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'see', 'this', 'is', 'this', 'mo@@', 'ving', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'want', 'to', 'show', 'it', '.', '</s>']
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Hypothesis: You see this is this moving , the gravity of the problem because I don 't want to show it .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - Example #2
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'is', 'a', 'sense', 'of', 'the', 'world', ',', 'in', 'a', 'sense', 'of', 'the', 'world', ',', 'the', 'ma@@', 'the@@', 'm@@', 'selves', 'system', '.', '</s>']
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - 	Hypothesis: The glacial , is a sense of the world , in a sense of the world , the mathemselves system .
2025-05-28 20:09:09,970 - INFO - joeynmt.training - Example #3
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:09:09,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 'b@@', 'lu@@', 'e', 'and', 'you', 'get', 'up', 'and', 'you', 'get', 'the', 'g@@', 'ame', '.', '</s>']
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Hypothesis: It 's a blue and you get up and you get the game .
2025-05-28 20:09:09,971 - INFO - joeynmt.training - Example #4
2025-05-28 20:09:09,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:09:09,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:09:09,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'ther', 'is', 'going', 'to', 'be', 'a', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:09:09,971 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rather is going to be a couple of the last 25 years .
2025-05-28 20:09:24,823 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.097064, Batch Acc: 0.393114, Tokens per Sec:     4791, Lr: 0.000300
2025-05-28 20:09:39,815 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.082943, Batch Acc: 0.399497, Tokens per Sec:     4774, Lr: 0.000300
2025-05-28 20:09:54,607 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.079313, Batch Acc: 0.397312, Tokens per Sec:     4914, Lr: 0.000300
2025-05-28 20:10:09,362 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.088471, Batch Acc: 0.407747, Tokens per Sec:     4811, Lr: 0.000300
2025-05-28 20:10:23,930 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.326707, Batch Acc: 0.416780, Tokens per Sec:     4878, Lr: 0.000300
2025-05-28 20:10:23,931 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:10:23,932 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:11:05,777 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.18, ppl:   8.83, acc:   0.40, generation: 41.8393[sec], evaluation: 0.0000[sec]
2025-05-28 20:11:05,779 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:11:05,919 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/1500.ckpt
2025-05-28 20:11:05,923 - INFO - joeynmt.training - Example #0
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'year', 'I', "'ve", 'sho@@', 'wing', 'these', 'di@@', 'ag@@', 'ed', 'to', 'show', 'you', 'that', 'the', 't@@', 'est', 'sho@@', 'ws', 'that', 'the', 'c@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'l@@', 'ac@@', 'i@@', 'ous', 'three', 'million', 'years', 'of', 'years', '.', '</s>']
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Hypothesis: And the year I 've showing these diaged to show you that the test shows that the clacial clacious three million years of years .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - Example #1
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'that', "'s", 'this', 'mo@@', 'ving', 'the', 'mo@@', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', '.', '</s>']
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - 	Hypothesis: So that 's this moving the mogravity of the problem because I don 't show it the problem because I don 't show it .
2025-05-28 20:11:05,923 - INFO - joeynmt.training - Example #2
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:11:05,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ma@@', 'the@@', 'mat@@', 'ics', 'is', ',', 'is', ',', 'in', 'a', 'way', ',', 'in', 'a', 'sense', 'of', 'course', ',', 'the', 'cu@@', 'm@@', 'ul@@', 'ts', 'of', 'the', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Hypothesis: The mathematics is , is , in a way , in a sense of course , the cumults of the climate system .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - Example #3
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 'de@@', 'ad', 'and', 'you', 'can', 'be', 'on', 'the', 'g@@', 'est@@', 'er@@', 't', '.', '</s>']
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Hypothesis: It 's a dead and you can be on the gestert .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - Example #4
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:11:05,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '2@@', '5', ',', 'it', "'s", 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'ece', 'of', 'de@@', 'de@@', 'n', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:11:05,924 - INFO - joeynmt.training - 	Hypothesis: The next 25 , it 's going to be a rapiece of deden the last 25 years .
2025-05-28 20:11:20,593 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.034629, Batch Acc: 0.420287, Tokens per Sec:     4801, Lr: 0.000300
2025-05-28 20:11:35,591 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.335746, Batch Acc: 0.426598, Tokens per Sec:     4828, Lr: 0.000300
2025-05-28 20:11:50,075 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.015370, Batch Acc: 0.428951, Tokens per Sec:     5010, Lr: 0.000300
2025-05-28 20:11:58,377 - INFO - joeynmt.training - Epoch   1: total training loss 11662.18
2025-05-28 20:11:58,379 - INFO - joeynmt.training - EPOCH 2
2025-05-28 20:12:04,991 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.907395, Batch Acc: 0.442324, Tokens per Sec:     4699, Lr: 0.000300
2025-05-28 20:12:20,007 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.837845, Batch Acc: 0.444520, Tokens per Sec:     4818, Lr: 0.000300
2025-05-28 20:12:20,008 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:12:20,008 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:13:07,619 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.06, acc:   0.42, generation: 47.6032[sec], evaluation: 0.0000[sec]
2025-05-28 20:13:07,621 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:13:07,760 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/2000.ckpt
2025-05-28 20:13:07,765 - INFO - joeynmt.training - Example #0
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'di@@', 'a@@', 'p', 'of', 'the', 's@@', 'li@@', 'ves', 'that', 'the', 'b@@', 'lu@@', 'e', 'is', 'b@@', 'al@@', 'lo@@', 'wer', ',', 'which', 'for', 'a', 'lot', 'of', 'years', 'of', 'years', ',', 'which', 'has', 'had', 'the', '4@@', '8', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 'to', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diap of the slives that the blue is ballower , which for a lot of years of years , which has had the 48 years of the United States , is to the United States .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - Example #1
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['T@@', 'w@@', 'o', 'this', 'is', 'that', 'so@@', 'und', 'the', 'val@@', 'u@@', 'es', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', 'it', 'to', 'the', 't@@', 'ac@@', 'k@@', 'er', '.', '</s>']
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Hypothesis: Two this is that sound the values of the problem because I don 't show it it to the tacker .
2025-05-28 20:13:07,765 - INFO - joeynmt.training - Example #2
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:13:07,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'ut', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'is', ',', 'in', 'a', 'sense', 'of', 'a', 'sense', 'of', 'the', 'cli@@', 'm@@', 'ate', 'system', ',', 'the', 'glob@@', 'al', 'system', '.', '</s>']
2025-05-28 20:13:07,765 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Hypothesis: The cut glacial is , in a sense of a sense of the climate system , the global system .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - Example #3
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'actually', 's@@', 'w@@', 'el@@', 'd', 'and', 'you', 'get', 'out', 'of', 'the', 'w@@', 'in@@', 'do@@', 'w', '.', '</s>']
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Hypothesis: You can actually sweld and you get out of the window .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - Example #4
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:13:07,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'l@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:13:07,766 - INFO - joeynmt.training - 	Hypothesis: The next is going to be a rapiled by the last 25 years .
2025-05-28 20:13:22,390 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.891237, Batch Acc: 0.452396, Tokens per Sec:     4891, Lr: 0.000300
2025-05-28 20:13:36,927 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.975639, Batch Acc: 0.451432, Tokens per Sec:     4990, Lr: 0.000300
2025-05-28 20:13:51,393 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.959078, Batch Acc: 0.452250, Tokens per Sec:     4963, Lr: 0.000300
2025-05-28 20:14:06,432 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.010668, Batch Acc: 0.458275, Tokens per Sec:     4868, Lr: 0.000300
2025-05-28 20:14:21,272 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.719664, Batch Acc: 0.459170, Tokens per Sec:     4771, Lr: 0.000300
2025-05-28 20:14:21,273 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:14:21,273 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:15:13,774 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.55, acc:   0.44, generation: 52.4930[sec], evaluation: 0.0000[sec]
2025-05-28 20:15:13,776 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:15:13,913 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/2500.ckpt
2025-05-28 20:15:13,916 - INFO - joeynmt.training - Example #0
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'di@@', 'a@@', 'si@@', 'ons', 'to', 'show', 'you', 'that', 'the', 'b@@', 'lu@@', 'i@@', 'al', 'c@@', 'l@@', 'ac@@', 'i@@', 'al', 'year', ',', 'which', 'is', 'a', 'si@@', 'ze', 'of', 'years', 'of', 'years', ',', 'which', 'is', 'the', '4@@', '8', 'million', 'years', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 'to', '40', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:15:13,916 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:15:13,916 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:15:13,916 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasions to show you that the bluial clacial year , which is a size of years of years , which is the 48 million years of the 48 percent of the United States , is to 40 percent of the United States .
2025-05-28 20:15:13,916 - INFO - joeynmt.training - Example #1
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:15:13,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', "'s", 'this', 'so@@', 'und', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'w@@', 'on', "'t", 'show', 'it', 'it', 'to', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Hypothesis: And that 's this sound , the gravity of the problem because I won 't show it it to the ice of the ice .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - Example #2
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', 'of', 'the', 'cli@@', 'm@@', 'ate', 'system', 'of', 'the', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Hypothesis: The calculartics is , in a sense , in a sense of the climate system of the climate system .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - Example #3
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'a', 's@@', 'w@@', 'in@@', 'd', 'and', 'you', 'get', 'out', 'of', 'the', 'est@@', 'ate', '.', '</s>']
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Hypothesis: It 's a swind and you get out of the estate .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - Example #4
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:15:13,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'l@@', 'ated', 'car@@', 'car@@', 's', ',', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:15:13,917 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapilated carcars , the last 25 years .
2025-05-28 20:15:28,877 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.912784, Batch Acc: 0.463718, Tokens per Sec:     4835, Lr: 0.000300
2025-05-28 20:15:43,737 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.820186, Batch Acc: 0.460594, Tokens per Sec:     4789, Lr: 0.000300
2025-05-28 20:15:59,166 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.923597, Batch Acc: 0.473081, Tokens per Sec:     4641, Lr: 0.000300
2025-05-28 20:16:13,882 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.849481, Batch Acc: 0.470120, Tokens per Sec:     4879, Lr: 0.000300
2025-05-28 20:16:29,421 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.808957, Batch Acc: 0.474940, Tokens per Sec:     4616, Lr: 0.000300
2025-05-28 20:16:29,422 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:16:29,422 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:17:12,563 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.23, acc:   0.45, generation: 43.1349[sec], evaluation: 0.0000[sec]
2025-05-28 20:17:12,565 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:17:12,694 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/3000.ckpt
2025-05-28 20:17:12,698 - INFO - joeynmt.training - Example #0
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', 'went', 'to', 'show', 'these', 'dis@@', 'ap@@', 'pe@@', 'ar@@', 'ed', 'these', 's@@', 'li@@', 'ves', 'to', 'show', 'that', 'the', 'b@@', 'al@@', 'lo@@', 'wer', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'a', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 're@@', 'cor@@', 'd', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Hypothesis: They went to show these disappeared these slives to show that the ballower glacial , which for a size of the 48 million years had the United States of the United States , is rerecord of the United States .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - Example #1
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', "'s", 'this', 'so@@', 'und', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'the', 'g@@', 'ets', 'of', 'the', 'g@@', 'l@@', 'ess@@', 'or', '.', '</s>']
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - 	Hypothesis: And that 's this sound , the gravity of the problem because it doesn 't show it the gets of the glessor .
2025-05-28 20:17:12,698 - INFO - joeynmt.training - Example #2
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:17:12,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', 'of', 'the', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is , in a sense , in a sense of the climate system .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - Example #3
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 's@@', 'av@@', 'es', 'and', 'you', 'get', 'out', 'of', 'the', 'est@@', 'ate', '.', '</s>']
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Hypothesis: It 's saves and you get out of the estate .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - Example #4
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:17:12,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'l@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:17:12,699 - INFO - joeynmt.training - 	Hypothesis: The next next is going to be a rapid carled by the last 25 years .
2025-05-28 20:17:27,697 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.923558, Batch Acc: 0.480018, Tokens per Sec:     4642, Lr: 0.000300
2025-05-28 20:17:42,737 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.810445, Batch Acc: 0.478455, Tokens per Sec:     4788, Lr: 0.000300
2025-05-28 20:17:57,677 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.964127, Batch Acc: 0.479106, Tokens per Sec:     4828, Lr: 0.000300
2025-05-28 20:18:12,492 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.911601, Batch Acc: 0.480501, Tokens per Sec:     4810, Lr: 0.000300
2025-05-28 20:18:27,562 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.785633, Batch Acc: 0.484556, Tokens per Sec:     4697, Lr: 0.000300
2025-05-28 20:18:27,562 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:18:27,562 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:19:14,593 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.91, acc:   0.46, generation: 47.0246[sec], evaluation: 0.0000[sec]
2025-05-28 20:19:14,595 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:19:14,741 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/3500.ckpt
2025-05-28 20:19:14,745 - INFO - joeynmt.training - Example #0
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'ves', 'to', 'de@@', 'pen@@', 'd@@', 'ent', 'that', 'the', 'he@@', 'ad', 'of', 'the', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'three', 'million', 'years', 'of', 'years', 'of', 'years', 'had', 'had', 'the', 'si@@', 'ons', 'of', '4@@', '8', 'years', 'of', 'the', 'contin@@', 'ent', ',', 'is', 're@@', 'duc@@', 'ed', 'by', '40', 'percent', '.', '</s>']
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slives to dependent that the head of the calculartics , which for three million years of years of years had had the sions of 48 years of the continent , is reduced by 40 percent .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - Example #1
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', "'s", 'going', 'to', 'have', 'this', 'so@@', 'und', 'for', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'the', 'sp@@', 'ess@@', 'or', 'of', 'the', 'g@@', 'ame', '.', '</s>']
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - 	Hypothesis: But that 's going to have this sound for the problem because it doesn 't show it the spessor of the game .
2025-05-28 20:19:14,745 - INFO - joeynmt.training - Example #2
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:19:14,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'al', 'system', '.', '</s>']
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Hypothesis: The calculartics is , in a sense , in a sense , the cummmmmmmmmmmmal system .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - Example #3
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 't@@', 'rem@@', 'end@@', 'ous', 'and', 'you', 'can', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Hypothesis: You can extremendous and you can reverse .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - Example #4
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:19:14,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'l@@', 'ate', 'to', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:19:14,746 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapid carlate to the last 25 years .
2025-05-28 20:19:29,841 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.762664, Batch Acc: 0.489867, Tokens per Sec:     4784, Lr: 0.000300
2025-05-28 20:19:45,226 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.741508, Batch Acc: 0.487071, Tokens per Sec:     4782, Lr: 0.000300
2025-05-28 20:20:00,889 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.890953, Batch Acc: 0.492048, Tokens per Sec:     4517, Lr: 0.000300
2025-05-28 20:20:16,129 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.868868, Batch Acc: 0.482558, Tokens per Sec:     4660, Lr: 0.000300
2025-05-28 20:20:31,619 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     1.806233, Batch Acc: 0.495079, Tokens per Sec:     4651, Lr: 0.000300
2025-05-28 20:20:31,621 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:20:31,621 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:21:14,558 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.69, acc:   0.47, generation: 42.9311[sec], evaluation: 0.0000[sec]
2025-05-28 20:21:14,561 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:21:14,712 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/4000.ckpt
2025-05-28 20:21:14,717 - INFO - joeynmt.training - Example #0
2025-05-28 20:21:14,717 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:21:14,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:21:14,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'year', 'I', "'ve", 'sho@@', 'wn', 'these', 'are', 'going', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', ',', 'which', 'is', 'a', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'mil@@', 'li@@', 'ons', 'of', 'years', 'had', 'had', 'had', 'the', 'si@@', 'mil@@', 'li@@', 'ons', 'of', '4@@', '8', 'cent@@', '-@@', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ue', ',', 'is', 're@@', 'cor@@', 'd', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 20:21:14,717 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:21:14,717 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:21:14,717 - INFO - joeynmt.training - 	Hypothesis: The year year I 've shown these are going to show that the calculartics , which is a glacial simillions of years had had had the simillions of 48 cent-continental continental continue , is record of the United States .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - Example #1
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', 'so@@', 'und', 'this', 'so@@', 'und', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', '.', '</s>']
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Hypothesis: But that sound this sound is the gravity of the problem because I don 't show it .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - Example #2
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'it', "'s", 'a', 'sense', ',', 'the', 'cu@@', 'm@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Hypothesis: The glacial artics is , in a sense , the cuit 's a sense , the cummate system .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - Example #3
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 't@@', 'ent', 'and', 'you', 'can', 're@@', 'ver@@', 'ed', '.', '</s>']
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Hypothesis: You can extent and you can revered .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - Example #4
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:21:14,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:21:14,718 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:21:14,719 - INFO - joeynmt.training - 	Hypothesis: The next next 25 years .
2025-05-28 20:21:29,516 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.719601, Batch Acc: 0.498444, Tokens per Sec:     4706, Lr: 0.000300
2025-05-28 20:21:44,740 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     1.833578, Batch Acc: 0.498319, Tokens per Sec:     4729, Lr: 0.000300
2025-05-28 20:22:00,911 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.857524, Batch Acc: 0.494739, Tokens per Sec:     4432, Lr: 0.000300
2025-05-28 20:22:16,170 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     1.790179, Batch Acc: 0.501745, Tokens per Sec:     4807, Lr: 0.000300
2025-05-28 20:22:31,267 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.966762, Batch Acc: 0.502479, Tokens per Sec:     4784, Lr: 0.000300
2025-05-28 20:22:31,268 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:22:31,268 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:23:09,188 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.50, acc:   0.47, generation: 37.9140[sec], evaluation: 0.0000[sec]
2025-05-28 20:23:09,191 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:23:09,324 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/4500.ckpt
2025-05-28 20:23:09,327 - INFO - joeynmt.training - Example #0
2025-05-28 20:23:09,327 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'sho@@', 'w@@', 'ed', 'these', 'de@@', 'gre@@', 'es', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'duc@@', 'ed', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'cor@@', 'ds', 'about', '40', 'percent', '.', '</s>']
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Hypothesis: The year I 've showed these degrees to show that the calculate the glacial artics , which for almost three million years of the 48 million years of the United States , is reduced the United States , is records about 40 percent .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - Example #1
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', "'s", 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'f@@', 'lo@@', 'or', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Hypothesis: And that 's the gravity of the floor of the problem because I don 't show it the ice of the ice .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - Example #2
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'p@@', 'ut@@', 'ting', 'sense', ',', 'the', 'cu@@', 'p@@', 'atter@@', 'n', '.', '</s>']
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - 	Hypothesis: The calculate artics is , in a sense , the cuputting sense , the cupattern .
2025-05-28 20:23:09,328 - INFO - joeynmt.training - Example #3
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:23:09,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 's@@', 'av@@', 'es', 'and', 'you', 'get', 'out', 'of', 'the', 'ex@@', 't@@', 'rem@@', 'ely', '.', '</s>']
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Hypothesis: It 's saves and you get out of the extremely .
2025-05-28 20:23:09,329 - INFO - joeynmt.training - Example #4
2025-05-28 20:23:09,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:23:09,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:23:09,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'ece', 'of', 'car@@', 'l@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:23:09,329 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapiece of carled by the last 25 years .
2025-05-28 20:23:24,115 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.694745, Batch Acc: 0.504338, Tokens per Sec:     4803, Lr: 0.000300
2025-05-28 20:23:39,152 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.778297, Batch Acc: 0.510338, Tokens per Sec:     4786, Lr: 0.000300
2025-05-28 20:23:54,299 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.656486, Batch Acc: 0.497411, Tokens per Sec:     4871, Lr: 0.000300
2025-05-28 20:24:09,316 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.732887, Batch Acc: 0.502004, Tokens per Sec:     4785, Lr: 0.000300
2025-05-28 20:24:24,067 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.597132, Batch Acc: 0.504304, Tokens per Sec:     4804, Lr: 0.000300
2025-05-28 20:24:24,068 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:24:24,068 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:25:05,546 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.48, generation: 41.4713[sec], evaluation: 0.0000[sec]
2025-05-28 20:25:05,549 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:25:05,697 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/5000.ckpt
2025-05-28 20:25:05,703 - INFO - joeynmt.training - Example #0
2025-05-28 20:25:05,703 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:25:05,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:25:05,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'di@@', 'a@@', 'si@@', 'ons', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 20:25:05,703 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:25:05,703 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:25:05,703 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasions to show that the calculate glacial artics , which for almost three million years of years of years of years of years of the 48 States , it 's reduced 40 percent .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - Example #1
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', "'s", 'this', 'so@@', 'und', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Hypothesis: And that 's this sound , the gravity of the problem because I don 't show the ice of the ice of the ice .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - Example #2
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'las@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'way', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Hypothesis: The glasial artics is , in a certain way , the heart of global climate system .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - Example #3
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 's@@', 'w@@', 'a@@', 'it', 'and', 'you', 'can', 'get', 'up', '.', '</s>']
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - 	Hypothesis: You can swait and you can get up .
2025-05-28 20:25:05,704 - INFO - joeynmt.training - Example #4
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:25:05,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'ece', 'is', 'a', 'f@@', 'ree', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:25:05,705 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:25:05,705 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:25:05,705 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapiece is a free of the last 25 years .
2025-05-28 20:25:20,612 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.650728, Batch Acc: 0.509924, Tokens per Sec:     4886, Lr: 0.000300
2025-05-28 20:25:35,251 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.875627, Batch Acc: 0.516763, Tokens per Sec:     4970, Lr: 0.000300
2025-05-28 20:25:50,364 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.700381, Batch Acc: 0.510297, Tokens per Sec:     4653, Lr: 0.000300
2025-05-28 20:26:05,276 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.872474, Batch Acc: 0.508907, Tokens per Sec:     4872, Lr: 0.000300
2025-05-28 20:26:20,252 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.717805, Batch Acc: 0.512949, Tokens per Sec:     4719, Lr: 0.000300
2025-05-28 20:26:20,253 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:26:20,253 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:27:02,967 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.20, acc:   0.49, generation: 42.7073[sec], evaluation: 0.0000[sec]
2025-05-28 20:27:02,970 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:27:03,123 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/5500.ckpt
2025-05-28 20:27:03,127 - INFO - joeynmt.training - Example #0
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de', ',', 'and', 'sho@@', 'w@@', 'ed', 'these', 'positi@@', 've', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ic@@', 'y', ',', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'and', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Hypothesis: They 've shown these slide , and showed these positive glacial articy , which is almost three million years had the size of 48 million years of the 48 States , and it 's reduced 40 percent .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - Example #1
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:27:03,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'so@@', 'und', ',', 'this', 'so@@', 'und', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - 	Hypothesis: But this sound , this sound , the gravity of the problem because it doesn 't show it spessor of the ice .
2025-05-28 20:27:03,127 - INFO - joeynmt.training - Example #2
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ic@@', 'y', 'is', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial articy is , the heart of global climate system .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - Example #3
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'be', 's@@', 'li@@', 'de', 'and', 'you', 'get', 'up', '.', '</s>']
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Hypothesis: You can be slide and you get up .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - Example #4
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:27:03,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'car@@', 'l@@', 'ated', 'car@@', 'l@@', 'ated', ',', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:27:03,128 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a carlated carlated , the last 25 years .
2025-05-28 20:27:18,182 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.601369, Batch Acc: 0.513367, Tokens per Sec:     4642, Lr: 0.000300
2025-05-28 20:27:33,161 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.703658, Batch Acc: 0.515191, Tokens per Sec:     4791, Lr: 0.000300
2025-05-28 20:27:48,749 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.700265, Batch Acc: 0.515905, Tokens per Sec:     4621, Lr: 0.000300
2025-05-28 20:28:03,665 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.520964, Batch Acc: 0.514012, Tokens per Sec:     4715, Lr: 0.000300
2025-05-28 20:28:19,079 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.793368, Batch Acc: 0.513840, Tokens per Sec:     4613, Lr: 0.000300
2025-05-28 20:28:19,080 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:28:19,080 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:29:05,180 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.02, acc:   0.49, generation: 46.0929[sec], evaluation: 0.0000[sec]
2025-05-28 20:29:05,183 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:29:05,328 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/6000.ckpt
2025-05-28 20:29:05,332 - INFO - joeynmt.training - Example #0
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'de@@', 'gre@@', 'es', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'you', 'have', 'been', 're@@', 't@@', 'end@@', 'ed', 'to', '40', 'percent', '.', '</s>']
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Hypothesis: The year I showed these degrees to show that the calculate glacial artics , which for almost three million years , which for almost three million years of the United States , you have been retended to 40 percent .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - Example #1
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'m", 'going', 'to', 'show', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'sp@@', 'ess@@', 'es', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Hypothesis: I 'm going to show the gravity of the problem because it doesn 't show it spesses of the ice .
2025-05-28 20:29:05,332 - INFO - joeynmt.training - Example #2
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:29:05,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:29:05,332 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Hypothesis: The calculate is , in a sense , in a sense , in a sense , a sense , the heart of the global climate system .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - Example #3
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', "'s", 'in@@', 'ver@@', 'n', 'and', 'you', 'get', 'the', 'ex@@', 't@@', 'ate', '.', '</s>']
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Hypothesis: You can see it 's invern and you get the extate .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - Example #4
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:29:05,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'di@@', 'a@@', 'positi@@', 've', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'ece', 'of', 'car@@', 's', 'from', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:29:05,333 - INFO - joeynmt.training - 	Hypothesis: The next diapositive is going to be a rapiece of cars from the last 25 years .
2025-05-28 20:29:20,566 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.501818, Batch Acc: 0.511446, Tokens per Sec:     4728, Lr: 0.000300
2025-05-28 20:29:35,834 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.745651, Batch Acc: 0.519540, Tokens per Sec:     4745, Lr: 0.000300
2025-05-28 20:29:37,647 - INFO - joeynmt.training - Epoch   2: total training loss 7919.80
2025-05-28 20:29:37,647 - INFO - joeynmt.training - EPOCH 3
2025-05-28 20:29:50,874 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.843572, Batch Acc: 0.536301, Tokens per Sec:     4703, Lr: 0.000300
2025-05-28 20:30:06,007 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.454270, Batch Acc: 0.540898, Tokens per Sec:     4679, Lr: 0.000300
2025-05-28 20:30:20,933 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.606563, Batch Acc: 0.538852, Tokens per Sec:     4871, Lr: 0.000300
2025-05-28 20:30:20,934 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:30:20,934 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:31:07,700 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.92, acc:   0.50, generation: 46.7581[sec], evaluation: 0.0000[sec]
2025-05-28 20:31:07,702 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:31:07,845 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/6500.ckpt
2025-05-28 20:31:07,847 - INFO - joeynmt.training - Example #0
2025-05-28 20:31:07,847 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:31:07,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:31:07,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'de@@', 'mon@@', 'str@@', 'ate', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'r@@', 'ate', ',', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'dimen@@', 'si@@', 'ons', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstrate demonstrate that the glacial rate , which is almost three million years had the size of 48 million years had the United States continental dimensions , it 's reduced 40 percent .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - Example #1
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'that', 'gr@@', 'av@@', 'ity', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'the', 'sp@@', 'ec@@', 'ac@@', 'tion', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Hypothesis: But this is that gravity , the gravity of the problem because I don 't show the specaction of the ice .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - Example #2
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', ',', 'a', 'very', 'ar@@', 't@@', 'ical', 'way', ',', 'a', 'sense', ',', 'the', 'cu@@', 'p@@', 'ut@@', 'ting', 'sense', ',', 'the', 'cu@@', 'b@@', 'ul@@', 'ating', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Hypothesis: The calculation is , a very artical way , a sense , the cuputting sense , the cubulating climate system .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - Example #3
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 't@@', 'inc@@', 'ed', 'and', 'you', 'can', 'get', 'to', 'ex@@', 't@@', 'ate', '.', '</s>']
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - 	Hypothesis: You can extinced and you can get to extate .
2025-05-28 20:31:07,848 - INFO - joeynmt.training - Example #4
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:31:07,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'is', 'going', 'to', 'be', 'a', 'f@@', 'oc@@', 'used', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:31:07,849 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:31:07,849 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:31:07,849 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapid is going to be a focused on the last 25 years .
2025-05-28 20:31:22,809 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.727194, Batch Acc: 0.536283, Tokens per Sec:     4783, Lr: 0.000300
2025-05-28 20:31:37,634 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.732445, Batch Acc: 0.530563, Tokens per Sec:     4865, Lr: 0.000300
2025-05-28 20:31:52,538 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.554813, Batch Acc: 0.528870, Tokens per Sec:     4689, Lr: 0.000300
2025-05-28 20:32:07,457 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.595899, Batch Acc: 0.539514, Tokens per Sec:     4799, Lr: 0.000300
2025-05-28 20:32:22,412 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.647849, Batch Acc: 0.537565, Tokens per Sec:     4855, Lr: 0.000300
2025-05-28 20:32:22,413 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:32:22,413 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:33:04,468 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.83, acc:   0.51, generation: 42.0478[sec], evaluation: 0.0000[sec]
2025-05-28 20:33:04,471 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:33:04,618 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/7000.ckpt
2025-05-28 20:33:04,622 - INFO - joeynmt.training - Example #0
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'de@@', 'gre@@', 'es', 'to', 'show', 'that', 'the', 's@@', 'li@@', 'ves', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '.', '</s>']
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Hypothesis: The year I showed these degrees to show that the slives the glacial artics , which for almost three million years of the United States continental size of 48 percent of the United States continental continental .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - Example #1
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'this', 'sub@@', 'ject', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'to', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'it', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - 	Hypothesis: So this subject the gravity of the problem because it doesn 't show it to the problem because it doesn 't show it spessor of the ice .
2025-05-28 20:33:04,622 - INFO - joeynmt.training - Example #2
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:33:04,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'hear@@', 't', ',', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Hypothesis: The calcalculate is , in a sense , the heart heart , the heart heart of the global climate system .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - Example #3
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 't@@', 'ate', 'it', 'and', 'you', 'get', 'up', '.', '</s>']
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Hypothesis: You can extate it and you get up .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - Example #4
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:33:04,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'de@@', 'de@@', 'b@@', 'ing', 'will', 'be', 'a', 'car@@', 'l@@', 'ed', 'car@@', 'l@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:33:04,623 - INFO - joeynmt.training - 	Hypothesis: The next next dededebing will be a carled carled on the last 25 years .
2025-05-28 20:33:19,923 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.447160, Batch Acc: 0.538570, Tokens per Sec:     4804, Lr: 0.000300
2025-05-28 20:33:34,789 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.532238, Batch Acc: 0.542999, Tokens per Sec:     4943, Lr: 0.000300
2025-05-28 20:33:49,958 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.713227, Batch Acc: 0.533410, Tokens per Sec:     4693, Lr: 0.000300
2025-05-28 20:34:05,423 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.846493, Batch Acc: 0.539568, Tokens per Sec:     4684, Lr: 0.000300
2025-05-28 20:34:20,184 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.592301, Batch Acc: 0.538076, Tokens per Sec:     4839, Lr: 0.000300
2025-05-28 20:34:20,185 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:34:20,185 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:35:01,386 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.77, acc:   0.51, generation: 41.1940[sec], evaluation: 0.0000[sec]
2025-05-28 20:35:01,388 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:35:01,550 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/7500.ckpt
2025-05-28 20:35:01,551 - INFO - joeynmt.training - Example #0
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'the', 's@@', 'li@@', 'de', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to show the slide that the calculate glacial , which for almost three million years of the 48 percent of the 48 percent of the 48 percent of the 48 percent .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - Example #1
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'ject', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'I', 'don', "'t", 'show', 'it', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Hypothesis: But this subject the gravity of the problem because I don 't show it the ice of the ice .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - Example #2
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial is , in a certain sense , the heart of the global climate system .
2025-05-28 20:35:01,552 - INFO - joeynmt.training - Example #3
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:35:01,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'in@@', 'ver@@', 'su@@', 's', 'and', 'you', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Hypothesis: You can get inversus and you reverse it .
2025-05-28 20:35:01,553 - INFO - joeynmt.training - Example #4
2025-05-28 20:35:01,553 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:35:01,553 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:35:01,553 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'car@@', 'd', 'of', 'a', 'car@@', 'd', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:35:01,553 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a card of a card on the last 25 years .
2025-05-28 20:35:16,490 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.491588, Batch Acc: 0.538153, Tokens per Sec:     4661, Lr: 0.000300
2025-05-28 20:35:31,611 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.945849, Batch Acc: 0.539470, Tokens per Sec:     4815, Lr: 0.000300
2025-05-28 20:35:46,811 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.567522, Batch Acc: 0.539077, Tokens per Sec:     4679, Lr: 0.000300
2025-05-28 20:36:01,928 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.717689, Batch Acc: 0.541393, Tokens per Sec:     4839, Lr: 0.000300
2025-05-28 20:36:17,579 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.766843, Batch Acc: 0.535910, Tokens per Sec:     4514, Lr: 0.000300
2025-05-28 20:36:17,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:36:17,580 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:36:55,939 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.51, generation: 38.3517[sec], evaluation: 0.0000[sec]
2025-05-28 20:36:55,941 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:36:56,093 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/8000.ckpt
2025-05-28 20:36:56,099 - INFO - joeynmt.training - Example #0
2025-05-28 20:36:56,099 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:36:56,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:36:56,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'of', 'the', '4@@', '8', 'of', 'the', '4@@', '8', ',', 'you', 'have', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '.', '</s>']
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the glacial glacial , which for almost three million years of the 48 of the 48 of the 48 , you have the United States continental continental .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - Example #1
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', 'is', 'that', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Hypothesis: But that is that subjected the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - Example #2
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - 	Hypothesis: The calculus is , in a certain sense , in a certain sense , the heart heart of global climate system .
2025-05-28 20:36:56,100 - INFO - joeynmt.training - Example #3
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:36:56,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'ex@@', 'ac@@', 'tly', 'in@@', 'ver@@', 'no@@', 'thing', 'and', 'you', 're@@', 'ali@@', 'zed', '.', '</s>']
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Hypothesis: It 's exactly invernothing and you realized .
2025-05-28 20:36:56,101 - INFO - joeynmt.training - Example #4
2025-05-28 20:36:56,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:36:56,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:36:56,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'b@@', 'ing', 'is', 'going', 'to', 'be', 'a', 'car@@', 'l@@', 'ed', 'car@@', 'l@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:36:56,101 - INFO - joeynmt.training - 	Hypothesis: The next next debing is going to be a carled carled on the last 25 years .
2025-05-28 20:37:11,419 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.630327, Batch Acc: 0.543969, Tokens per Sec:     4678, Lr: 0.000300
2025-05-28 20:37:26,398 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.612814, Batch Acc: 0.545431, Tokens per Sec:     4851, Lr: 0.000300
2025-05-28 20:37:42,093 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.688921, Batch Acc: 0.546412, Tokens per Sec:     4727, Lr: 0.000300
2025-05-28 20:37:58,174 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.555670, Batch Acc: 0.545941, Tokens per Sec:     4502, Lr: 0.000300
2025-05-28 20:38:13,663 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.719306, Batch Acc: 0.550383, Tokens per Sec:     4765, Lr: 0.000300
2025-05-28 20:38:13,665 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:38:13,665 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:38:52,945 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.59, acc:   0.51, generation: 39.2722[sec], evaluation: 0.0000[sec]
2025-05-28 20:38:52,947 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:38:53,088 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/8500.ckpt
2025-05-28 20:38:53,090 - INFO - joeynmt.training - Example #0
2025-05-28 20:38:53,090 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:38:53,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:38:53,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', ',', 'you', 'have', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '.', '</s>']
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Hypothesis: And the year I showed these slide to show that the glacial glacial , which for almost three million years of the 48 of the United States continental continental , you have the United States continental continental .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - Example #1
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', "'s", 'the', 'f@@', 'lo@@', 'or', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Hypothesis: But that 's the floor of the problem because it doesn 't show the ice of the ice because it doesn 't show the ice of the ice .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - Example #2
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'way', ',', 'in', 'a', 'way', ',', 'cu@@', 'p', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Hypothesis: The calcalculus is , in a way , in a way , cup of the global climate system .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - Example #3
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:38:53,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'be', 's@@', 'li@@', 'gh@@', 't@@', '-@@', 'up', 'and', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - 	Hypothesis: You can be slight-up and reverse it .
2025-05-28 20:38:53,091 - INFO - joeynmt.training - Example #4
2025-05-28 20:38:53,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:38:53,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:38:53,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'car@@', 'l@@', 'ated', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:38:53,092 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:38:53,092 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:38:53,092 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a carlated carlated on the last 25 years .
2025-05-28 20:39:07,655 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.546884, Batch Acc: 0.543560, Tokens per Sec:     4932, Lr: 0.000300
2025-05-28 20:39:22,220 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.527927, Batch Acc: 0.542577, Tokens per Sec:     4776, Lr: 0.000300
2025-05-28 20:39:37,089 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.556253, Batch Acc: 0.546582, Tokens per Sec:     4909, Lr: 0.000300
2025-05-28 20:39:52,022 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.634293, Batch Acc: 0.544880, Tokens per Sec:     4804, Lr: 0.000300
2025-05-28 20:40:06,909 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.775494, Batch Acc: 0.545930, Tokens per Sec:     4936, Lr: 0.000300
2025-05-28 20:40:06,910 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:40:06,910 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:40:47,373 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.52, acc:   0.52, generation: 40.4563[sec], evaluation: 0.0000[sec]
2025-05-28 20:40:47,375 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:40:47,540 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/9000.ckpt
2025-05-28 20:40:47,541 - INFO - joeynmt.training - Example #0
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ics', ',', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'which', 'is', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'you', "'re", 're@@', 'cor@@', 'd', ',', 'it', "'s", 're@@', 'cor@@', 'd', 'of', '40', 'percent', '.', '</s>']
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the glacial artics , which is almost three million years of the 48 States , which is the size of 48 States continental , you 're record , it 's record of 40 percent .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - Example #1
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the spector of the ice .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - Example #2
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'a', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'b@@', 'ul@@', 'ar@@', 'c@@', 'ti@@', 'c', ',', 'the', 'cu@@', 'st@@', 'om@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - 	Hypothesis: The calculus is , a calculation is , in a sense , the cubularctic , the custommate system .
2025-05-28 20:40:47,542 - INFO - joeynmt.training - Example #3
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:40:47,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'ex@@', 'ac@@', 'tly', 's@@', 'w@@', 'it@@', 'ch', 'and', 'you', 're@@', 'ach', '.', '</s>']
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Hypothesis: It 's exactly switch and you reach .
2025-05-28 20:40:47,543 - INFO - joeynmt.training - Example #4
2025-05-28 20:40:47,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:40:47,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:40:47,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'car@@', 'l@@', 'ate', 'car@@', 'l@@', 'au@@', 'gh@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:40:47,543 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a carlate carlaughed on the last 25 years .
2025-05-28 20:41:02,426 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.568274, Batch Acc: 0.546593, Tokens per Sec:     4732, Lr: 0.000300
2025-05-28 20:41:17,472 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.592869, Batch Acc: 0.546887, Tokens per Sec:     4827, Lr: 0.000300
2025-05-28 20:41:32,457 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.560661, Batch Acc: 0.550361, Tokens per Sec:     4785, Lr: 0.000300
2025-05-28 20:41:47,229 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.502430, Batch Acc: 0.548796, Tokens per Sec:     4794, Lr: 0.000300
2025-05-28 20:42:02,011 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.709157, Batch Acc: 0.552195, Tokens per Sec:     4738, Lr: 0.000300
2025-05-28 20:42:02,012 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:42:02,012 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:42:41,849 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.52, generation: 39.8304[sec], evaluation: 0.0000[sec]
2025-05-28 20:42:41,851 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:42:42,010 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/9500.ckpt
2025-05-28 20:42:42,014 - INFO - joeynmt.training - Example #0
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '.', '</s>']
2025-05-28 20:42:42,014 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:42:42,014 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:42:42,014 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the calculus to show that the glacial glacial , which for almost three million years of the United States continental continental .
2025-05-28 20:42:42,014 - INFO - joeynmt.training - Example #1
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:42:42,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:42:42,014 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:42:42,014 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - Example #2
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'cu@@', 'b@@', 'ul@@', 'ts', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Hypothesis: The calculial glacial calculus is , in a certain sense , the cubults of global climate system .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - Example #3
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'd', 'and', 'you', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Hypothesis: It 's wind and you reverse it .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - Example #4
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:42:42,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'si@@', 'li@@', 'gh@@', 'ten@@', 'ed', 'car@@', 'l@@', 'ate', ',', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:42:42,015 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-silightened carlate , the last 25 years .
2025-05-28 20:42:57,387 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.450764, Batch Acc: 0.548413, Tokens per Sec:     4673, Lr: 0.000300
2025-05-28 20:43:12,729 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.603775, Batch Acc: 0.552466, Tokens per Sec:     4650, Lr: 0.000300
2025-05-28 20:43:27,647 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.482987, Batch Acc: 0.554345, Tokens per Sec:     4707, Lr: 0.000300
2025-05-28 20:43:43,022 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.567910, Batch Acc: 0.554413, Tokens per Sec:     4663, Lr: 0.000300
2025-05-28 20:43:58,095 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.550589, Batch Acc: 0.553438, Tokens per Sec:     4686, Lr: 0.000300
2025-05-28 20:43:58,096 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:43:58,096 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:44:37,069 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.38, acc:   0.53, generation: 38.9659[sec], evaluation: 0.0000[sec]
2025-05-28 20:44:37,071 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:44:37,225 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/10000.ckpt
2025-05-28 20:44:37,226 - INFO - joeynmt.training - Example #0
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'de@@', 'mon@@', 'str@@', 'ate', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'has', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'qui@@', 'red', '40', 'percent', '.', '</s>']
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstrate demonstrate that the calculate glacial , which for almost three million years of has had the United States continental , it 's required 40 percent .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - Example #1
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'tive', 'so@@', 'und', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Hypothesis: But this subjective sound the gravity of the problem because it doesn 't show the spector of the ice .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - Example #2
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'is', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'b@@', 'ul@@', 'l', 'cu@@', 'b@@', 'ul@@', 'l', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - 	Hypothesis: The artics is artics is , in a sense , in a sense , the cubull cubull of the global climate system .
2025-05-28 20:44:37,227 - INFO - joeynmt.training - Example #3
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:44:37,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:44:37,228 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'in@@', 'ver@@', 'n@@', 'al', 'and', 'you', 'get', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Hypothesis: It 's invernal and you get reverse .
2025-05-28 20:44:37,228 - INFO - joeynmt.training - Example #4
2025-05-28 20:44:37,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:44:37,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:44:37,228 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 's@@', 'li@@', 'gh@@', 'tly', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:44:37,228 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast slightly on the last 25 years .
2025-05-28 20:44:53,066 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.387834, Batch Acc: 0.550268, Tokens per Sec:     4573, Lr: 0.000300
2025-05-28 20:45:08,391 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.766474, Batch Acc: 0.560133, Tokens per Sec:     4760, Lr: 0.000300
2025-05-28 20:45:23,547 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.603261, Batch Acc: 0.550368, Tokens per Sec:     4712, Lr: 0.000300
2025-05-28 20:45:38,811 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.662867, Batch Acc: 0.554655, Tokens per Sec:     4809, Lr: 0.000300
2025-05-28 20:45:54,196 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.590268, Batch Acc: 0.554870, Tokens per Sec:     4732, Lr: 0.000300
2025-05-28 20:45:54,197 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:45:54,197 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:46:32,409 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.33, acc:   0.53, generation: 38.2054[sec], evaluation: 0.0000[sec]
2025-05-28 20:46:32,412 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:46:32,585 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/10500.ckpt
2025-05-28 20:46:32,588 - INFO - joeynmt.training - Example #0
2025-05-28 20:46:32,588 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:46:32,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:46:32,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'r@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'r@@', 'ace', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'd', 'of', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the rate glacial race , which for almost three million years of the United States continental continental , it 's record of 48 percent .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - Example #1
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'ject', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'problem', 'because', 'it', "'s", 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Hypothesis: But this subject the gravity of the problem because it doesn 't show the spector of the problem because it 's the spector of the ice .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - Example #2
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'is', ',', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'b@@', 'ab@@', 'y', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Hypothesis: The artics is , the artics is , in a sense , the baby of global climate system .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - Example #3
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:46:32,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'ex@@', 'ac@@', 'tly', 'in@@', 'ver@@', 'n@@', 'y', 'and', 'you', 'get', 're@@', 'co@@', 'gn@@', 'i@@', 'ze', '.', '</s>']
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - 	Hypothesis: It 's exactly inverny and you get recognize .
2025-05-28 20:46:32,589 - INFO - joeynmt.training - Example #4
2025-05-28 20:46:32,590 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:46:32,590 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:46:32,590 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'r@@', 'un', 'of', 'the', 'last', '2@@', '5', 'years', ',', '</s>']
2025-05-28 20:46:32,590 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:46:32,590 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:46:32,590 - INFO - joeynmt.training - 	Hypothesis: The next slide is a run of the last 25 years ,
2025-05-28 20:46:41,902 - INFO - joeynmt.training - Epoch   3: total training loss 6992.99
2025-05-28 20:46:41,902 - INFO - joeynmt.training - EPOCH 4
2025-05-28 20:46:47,787 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.314441, Batch Acc: 0.576718, Tokens per Sec:     4838, Lr: 0.000300
2025-05-28 20:47:02,850 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.660918, Batch Acc: 0.569603, Tokens per Sec:     4826, Lr: 0.000300
2025-05-28 20:47:18,024 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.715045, Batch Acc: 0.567381, Tokens per Sec:     4755, Lr: 0.000300
2025-05-28 20:47:32,572 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.574664, Batch Acc: 0.573851, Tokens per Sec:     4946, Lr: 0.000300
2025-05-28 20:47:47,243 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.601609, Batch Acc: 0.573326, Tokens per Sec:     4912, Lr: 0.000300
2025-05-28 20:47:47,243 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:47:47,243 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:48:26,903 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.53, generation: 39.6525[sec], evaluation: 0.0000[sec]
2025-05-28 20:48:26,905 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:48:27,037 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/11000.ckpt
2025-05-28 20:48:27,039 - INFO - joeynmt.training - Example #0
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'ves', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', ',', 'has', 'been', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slives to demonstrate that the glacial glacial , which for almost three million years had the size of 48 , has been the United States continental continental , it 's reduced 40 percent .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - Example #1
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'under@@', 'gro@@', 'und', ',', 'because', 'it', "'s", 'not', 'sho@@', 'ws', 'the', 'g@@', 'a@@', 'x@@', 'i@@', 'ac@@', 'y', '.', '</s>']
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - 	Hypothesis: But this subjects this underground , because it 's not shows the gaxiacy .
2025-05-28 20:48:27,039 - INFO - joeynmt.training - Example #2
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:48:27,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Hypothesis: The artics of the artics is , in a sense , the heart of the global climate system .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - Example #3
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', "'s", 'in@@', 'ver@@', 'n@@', 'y', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Hypothesis: You can see it 's inverny and you reverse .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - Example #4
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:48:27,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'one', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'rel@@', 'ated', 'to', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:48:27,040 - INFO - joeynmt.training - 	Hypothesis: The next one is going to be a rapid related to the last 25 years .
2025-05-28 20:48:42,654 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.414475, Batch Acc: 0.569403, Tokens per Sec:     4564, Lr: 0.000300
2025-05-28 20:48:57,866 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.416238, Batch Acc: 0.573219, Tokens per Sec:     4736, Lr: 0.000300
2025-05-28 20:49:13,061 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.426259, Batch Acc: 0.571760, Tokens per Sec:     4676, Lr: 0.000300
2025-05-28 20:49:27,880 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.504724, Batch Acc: 0.572649, Tokens per Sec:     4788, Lr: 0.000300
2025-05-28 20:49:43,172 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.473894, Batch Acc: 0.569395, Tokens per Sec:     4768, Lr: 0.000300
2025-05-28 20:49:43,174 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:49:43,174 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:50:22,501 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.26, acc:   0.54, generation: 39.3202[sec], evaluation: 0.0000[sec]
2025-05-28 20:50:22,504 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:50:22,627 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/11500.ckpt
2025-05-28 20:50:22,630 - INFO - joeynmt.training - Example #0
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'duc@@', 'ed', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the artics of the artics , which for almost three million years had had the size of 48 States , is reduced the 48 percent of the United States , it 's reduced 40 percent .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - Example #1
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - 	Hypothesis: But this subjected the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 20:50:22,630 - INFO - joeynmt.training - Example #2
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:50:22,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'is', ',', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'cu@@', 'or', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Hypothesis: The artics is , the artics is , in a sense , the cuor of global climate system .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - Example #3
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'in@@', 'ver@@', 'n@@', 'y', 'and', 'you', 'get', 'to', 'the', 'out@@', 'side', '.', '</s>']
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Hypothesis: It 's inverny and you get to the outside .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - Example #4
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:50:22,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'air@@', 'ly', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:50:22,631 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fairly on the last 25 years .
2025-05-28 20:50:37,706 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.526984, Batch Acc: 0.565361, Tokens per Sec:     4629, Lr: 0.000300
2025-05-28 20:50:53,091 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.549892, Batch Acc: 0.566269, Tokens per Sec:     4610, Lr: 0.000300
2025-05-28 20:51:08,351 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.671706, Batch Acc: 0.570321, Tokens per Sec:     4834, Lr: 0.000300
2025-05-28 20:51:23,620 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.427405, Batch Acc: 0.570432, Tokens per Sec:     4648, Lr: 0.000300
2025-05-28 20:51:38,664 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.524563, Batch Acc: 0.573610, Tokens per Sec:     4771, Lr: 0.000300
2025-05-28 20:51:38,665 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:51:38,665 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:52:15,530 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.23, acc:   0.53, generation: 36.8579[sec], evaluation: 0.0000[sec]
2025-05-28 20:52:15,532 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:52:15,667 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/12000.ckpt
2025-05-28 20:52:15,670 - INFO - joeynmt.training - Example #0
2025-05-28 20:52:15,670 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:52:15,670 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:52:15,670 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'the', 'whole', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'duc@@', 'ed', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '.', '</s>']
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide demonstrate that the glacial glacial , which for the whole three million years had the size of 48 States , is reduced the United States continental continental .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - Example #1
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'ject', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Hypothesis: But this subject of the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - Example #2
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ations', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'cu@@', 'b@@', 'ul@@', 'ous', 'system', '.', '</s>']
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial calculations is , in a certain sense , the cubulous system .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - Example #3
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:52:15,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'in@@', 'ver@@', 'n@@', 'y', 'and', 'you', 'get', 're@@', 'ver@@', 'ed', '.', '</s>']
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:52:15,671 - INFO - joeynmt.training - 	Hypothesis: It 's inverny and you get revered .
2025-05-28 20:52:15,672 - INFO - joeynmt.training - Example #4
2025-05-28 20:52:15,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:52:15,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:52:15,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'air@@', 'ly', 'b@@', 'ad', 'b@@', 'ad', 'b@@', 'ad', 'l@@', 'ad@@', 'y', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:52:15,672 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:52:15,672 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:52:15,672 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fairly bad bad bad lady on the last 25 years .
2025-05-28 20:52:31,024 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.618927, Batch Acc: 0.573072, Tokens per Sec:     4718, Lr: 0.000300
2025-05-28 20:52:45,785 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.353762, Batch Acc: 0.571587, Tokens per Sec:     4762, Lr: 0.000300
2025-05-28 20:53:01,298 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.441709, Batch Acc: 0.569781, Tokens per Sec:     4651, Lr: 0.000300
2025-05-28 20:53:16,690 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.564089, Batch Acc: 0.574633, Tokens per Sec:     4692, Lr: 0.000300
2025-05-28 20:53:31,701 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.463844, Batch Acc: 0.571078, Tokens per Sec:     4826, Lr: 0.000300
2025-05-28 20:53:31,702 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:53:31,702 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:54:11,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.16, acc:   0.54, generation: 40.2876[sec], evaluation: 0.0000[sec]
2025-05-28 20:54:11,999 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:54:12,168 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/12500.ckpt
2025-05-28 20:54:12,171 - INFO - joeynmt.training - Example #0
2025-05-28 20:54:12,171 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:54:12,171 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:54:12,171 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'is', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'which', 'is', 're@@', 'tro@@', 'ver@@', 'ed', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'tro@@', 'ver@@', 'se', ',', 'it', "'s", 're@@', 'tro@@', 'ver@@', 'se', '40', 'percent', '.', '</s>']
2025-05-28 20:54:12,171 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:54:12,171 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:54:12,171 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the glacial calculate , which is for almost three million years of the United States , which is retrovered the United States continental , it 's retroverse , it 's retroverse 40 percent .
2025-05-28 20:54:12,171 - INFO - joeynmt.training - Example #1
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'ject', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Hypothesis: But this subject is the gravity of the problem because it doesn 't show the spector of the ice .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - Example #2
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Hypothesis: The glacial calculus is , in a sense , the heart of global climate system .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - Example #3
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'n', 'and', 'it', "'s", 'going', 'to', 'be', 'su@@', 'm@@', 'er', '.', '</s>']
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Hypothesis: It 's winn and it 's going to be sumer .
2025-05-28 20:54:12,172 - INFO - joeynmt.training - Example #4
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:54:12,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'years', 'old', '.', '</s>']
2025-05-28 20:54:12,172 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:54:12,173 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:54:12,173 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid on the last 25 years old .
2025-05-28 20:54:27,174 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.454771, Batch Acc: 0.572034, Tokens per Sec:     4773, Lr: 0.000300
2025-05-28 20:54:42,841 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.383883, Batch Acc: 0.571607, Tokens per Sec:     4491, Lr: 0.000300
2025-05-28 20:54:58,225 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.424100, Batch Acc: 0.571479, Tokens per Sec:     4775, Lr: 0.000300
2025-05-28 20:55:14,231 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.398778, Batch Acc: 0.568180, Tokens per Sec:     4567, Lr: 0.000300
2025-05-28 20:55:30,146 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.435575, Batch Acc: 0.570379, Tokens per Sec:     4515, Lr: 0.000300
2025-05-28 20:55:30,148 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:55:30,148 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:56:16,686 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.13, acc:   0.54, generation: 46.5315[sec], evaluation: 0.0000[sec]
2025-05-28 20:56:16,688 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:56:16,841 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/13000.ckpt
2025-05-28 20:56:16,844 - INFO - joeynmt.training - Example #0
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'co@@', 'gn@@', 'i@@', 'ze', ',', 'it', "'s", 're@@', 'cent@@', 'ly', '40', 'percent', '.', '</s>']
2025-05-28 20:56:16,844 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:56:16,844 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:56:16,844 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the glacial glacial , which for almost three million years of years had had the size of 48 States , it 's recognize , it 's recently 40 percent .
2025-05-28 20:56:16,844 - INFO - joeynmt.training - Example #1
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:56:16,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the problem because it doesn 't show the spector of the ice .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - Example #2
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'the', 'cu@@', 'it', 'is', ',', 'the', 'glob@@', 'al', 'cu@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial calculate , the cuit is , the global cumate system .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - Example #3
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'w@@', 'a@@', 'it', 'and', 'you', 'get', 'down', '.', '</s>']
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Hypothesis: You can wait and you get down .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - Example #4
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:56:16,845 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'lo@@', 'ad@@', 'ed', 'car@@', 'l@@', 'ate', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:56:16,845 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a floaded carlate on the last 25 years .
2025-05-28 20:56:31,523 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.399012, Batch Acc: 0.573964, Tokens per Sec:     5035, Lr: 0.000300
2025-05-28 20:56:46,575 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.600505, Batch Acc: 0.577208, Tokens per Sec:     4875, Lr: 0.000300
2025-05-28 20:57:02,192 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.426934, Batch Acc: 0.573473, Tokens per Sec:     4654, Lr: 0.000300
2025-05-28 20:57:17,366 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.465883, Batch Acc: 0.572216, Tokens per Sec:     4713, Lr: 0.000300
2025-05-28 20:57:32,546 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.507375, Batch Acc: 0.574410, Tokens per Sec:     4676, Lr: 0.000300
2025-05-28 20:57:32,548 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:57:32,548 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 20:58:12,417 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.54, generation: 39.8627[sec], evaluation: 0.0000[sec]
2025-05-28 20:58:12,420 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 20:58:12,579 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/13500.ckpt
2025-05-28 20:58:12,581 - INFO - joeynmt.training - Example #0
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'and', 'it', "'s", 're@@', 'cor@@', 'd', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cent@@', 'ly', '40', 'percent', '.', '</s>']
2025-05-28 20:58:12,581 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 20:58:12,581 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 20:58:12,581 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide slide to show that the glacial glacial , which is almost three million years of the United States , and it 's record of the United States continental , it 's recently 40 percent .
2025-05-28 20:58:12,581 - INFO - joeynmt.training - Example #1
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 20:58:12,581 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', ',', 'this', 'is', 'not', 'sho@@', 'wing', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Hypothesis: But this subjected , this is not showing the problem because it doesn 't show the ice of the ice .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - Example #2
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Hypothesis: The artics of the artics is , in a sense , the heart of global climate system .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - Example #3
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'side', 'and', 'you', 'get', 'su@@', 'm@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'er', '.', '</s>']
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Hypothesis: It 's winside and you get sumer and you get sumer .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - Example #4
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 20:58:12,582 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'f@@', 'air@@', 'ly', 'r@@', 'ai@@', 's@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 20:58:12,582 - INFO - joeynmt.training - 	Hypothesis: The next slide is a fairly raised on the last 25 years .
2025-05-28 20:58:27,732 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.298952, Batch Acc: 0.570591, Tokens per Sec:     4742, Lr: 0.000300
2025-05-28 20:58:42,828 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.664128, Batch Acc: 0.571199, Tokens per Sec:     4578, Lr: 0.000300
2025-05-28 20:58:58,047 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.651799, Batch Acc: 0.564716, Tokens per Sec:     4669, Lr: 0.000300
2025-05-28 20:59:13,167 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.592322, Batch Acc: 0.576209, Tokens per Sec:     4647, Lr: 0.000300
2025-05-28 20:59:27,983 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.353824, Batch Acc: 0.577455, Tokens per Sec:     4727, Lr: 0.000300
2025-05-28 20:59:27,984 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 20:59:27,984 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:00:07,926 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.08, acc:   0.54, generation: 39.9345[sec], evaluation: 0.0000[sec]
2025-05-28 21:00:07,928 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:00:08,078 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/14000.ckpt
2025-05-28 21:00:08,080 - INFO - joeynmt.training - Example #0
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'de@@', 'mon@@', 'str@@', 'ate', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'of', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'cor@@', 'ds', ',', 'it', "'s", 're@@', 'cor@@', 'ds', 'of', '40', 'percent', '.', '</s>']
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Hypothesis: And the year I showed these slides demonstrate the calculate calculate , which for almost three million years of years of had the United States , the United States , it 's records , it 's records of 40 percent .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - Example #1
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'end@@', 'ing', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Hypothesis: This is the gravity of the problem because it doesn 't show the spending of the ice .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - Example #2
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'way', ',', 'in', 'a', 'way', ',', 'the', 'cu@@', 'p', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - 	Hypothesis: The artical calculate is , in a way , in a way , the cup of global climate system .
2025-05-28 21:00:08,081 - INFO - joeynmt.training - Example #3
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:00:08,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ed', 'and', 'it', "'s", 're@@', 'ver@@', 'ted', 'and', 'it', "'s", 're@@', 'mark@@', 'able', '.', '</s>']
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Hypothesis: It 's expanded and it 's reverted and it 's remarkable .
2025-05-28 21:00:08,082 - INFO - joeynmt.training - Example #4
2025-05-28 21:00:08,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:00:08,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:00:08,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'r@@', 'ai@@', 'se', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:00:08,082 - INFO - joeynmt.training - 	Hypothesis: The next slide is a raise of the last 25 years .
2025-05-28 21:00:23,146 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.443120, Batch Acc: 0.575905, Tokens per Sec:     4614, Lr: 0.000300
2025-05-28 21:00:38,808 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.492813, Batch Acc: 0.573410, Tokens per Sec:     4580, Lr: 0.000300
2025-05-28 21:00:53,574 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.348465, Batch Acc: 0.579456, Tokens per Sec:     4795, Lr: 0.000300
2025-05-28 21:01:08,719 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.505960, Batch Acc: 0.572271, Tokens per Sec:     4770, Lr: 0.000300
2025-05-28 21:01:23,307 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.585076, Batch Acc: 0.579326, Tokens per Sec:     5084, Lr: 0.000300
2025-05-28 21:01:23,308 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:01:23,308 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:02:06,880 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.02, acc:   0.55, generation: 43.5650[sec], evaluation: 0.0000[sec]
2025-05-28 21:02:06,882 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:02:07,085 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/14500.ckpt
2025-05-28 21:02:07,088 - INFO - joeynmt.training - Example #0
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'had', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ded', '40', 'percent', '.', '</s>']
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Hypothesis: The year I 've shown these slide slide to show that the glacial calculation , which for almost three million years of years had had the United States of the United States continental , it 's recorded 40 percent .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - Example #1
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:02:07,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:02:07,088 - INFO - joeynmt.training - Example #2
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'ar', 'system', '.', '</s>']
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Hypothesis: The articy glacial calculus is , in a sense , the car system .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - Example #3
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'd', 'and', 'you', 're@@', 'ver@@', 'no@@', 'r', 'and', 'you', 're@@', 'ver@@', 'no@@', 'r', '.', '</s>']
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Hypothesis: It 's wind and you revernor and you revernor .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - Example #4
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:02:07,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'lo@@', 'or', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:02:07,089 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a floor of the last 25 years .
2025-05-28 21:02:22,098 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.570508, Batch Acc: 0.578443, Tokens per Sec:     4894, Lr: 0.000300
2025-05-28 21:02:37,391 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.569718, Batch Acc: 0.579059, Tokens per Sec:     4637, Lr: 0.000300
2025-05-28 21:02:52,668 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     1.564134, Batch Acc: 0.575783, Tokens per Sec:     4633, Lr: 0.000300
2025-05-28 21:03:08,020 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     1.658628, Batch Acc: 0.577990, Tokens per Sec:     4706, Lr: 0.000300
2025-05-28 21:03:10,228 - INFO - joeynmt.training - Epoch   4: total training loss 6528.66
2025-05-28 21:03:10,228 - INFO - joeynmt.training - EPOCH 5
2025-05-28 21:03:23,364 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.486169, Batch Acc: 0.596995, Tokens per Sec:     4728, Lr: 0.000300
2025-05-28 21:03:23,364 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:03:23,365 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:04:04,803 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.99, acc:   0.55, generation: 41.4313[sec], evaluation: 0.0000[sec]
2025-05-28 21:04:04,806 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:04:04,974 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/15000.ckpt
2025-05-28 21:04:04,976 - INFO - joeynmt.training - Example #0
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', ',', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'tur@@', 'ns', 'up', '40', 'percent', '.', '</s>']
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to show that the glacial calculartics , which is almost three million years of the United States , the United States continental , it 's returns up 40 percent .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - Example #1
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'it', "'s", 'a', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Hypothesis: But it 's a gravity of the problem because it doesn 't show the ice of the ice because it doesn 't show the ice of the ice .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - Example #2
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:04:04,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'c@@', 'alc@@', 'ul@@', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'c@@', 'rit@@', 'ical', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - 	Hypothesis: The artics calculartics is , in a certain sense , the critical heart of global climate system .
2025-05-28 21:04:04,977 - INFO - joeynmt.training - Example #3
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'd', 'and', 'it', "'s", 'r@@', 'iti@@', 've', 'up', '.', '</s>']
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Hypothesis: It 's wind and it 's ritive up .
2025-05-28 21:04:04,978 - INFO - joeynmt.training - Example #4
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:04:04,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'f@@', 'ast', 'car@@', 'l@@', 'ated', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:04:04,978 - INFO - joeynmt.training - 	Hypothesis: The next slide is a fast carlated carlated on the last 25 years .
2025-05-28 21:04:20,776 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.500756, Batch Acc: 0.600274, Tokens per Sec:     4482, Lr: 0.000300
2025-05-28 21:04:35,836 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.384936, Batch Acc: 0.595114, Tokens per Sec:     4689, Lr: 0.000300
2025-05-28 21:04:50,425 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.346368, Batch Acc: 0.592950, Tokens per Sec:     4861, Lr: 0.000300
2025-05-28 21:05:05,724 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.915437, Batch Acc: 0.596961, Tokens per Sec:     4745, Lr: 0.000300
2025-05-28 21:05:20,779 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.516296, Batch Acc: 0.594634, Tokens per Sec:     4781, Lr: 0.000300
2025-05-28 21:05:20,780 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:05:20,780 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:06:01,287 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.96, acc:   0.55, generation: 40.5002[sec], evaluation: 0.0000[sec]
2025-05-28 21:06:01,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:06:01,461 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/15500.ckpt
2025-05-28 21:06:01,464 - INFO - joeynmt.training - Example #0
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ds', 'to', '40', 'percent', '.', '</s>']
2025-05-28 21:06:01,464 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:06:01,464 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:06:01,464 - INFO - joeynmt.training - 	Hypothesis: I showed this last year I showed these slides to show that the glacial calculate , which for almost three million years had the size of 48 million years had the United States continental , it 's records to 40 percent .
2025-05-28 21:06:01,464 - INFO - joeynmt.training - Example #1
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:06:01,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'tive', ',', 'this', 'is', 'not', 'going', 'to', 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Hypothesis: But this subjective , this is not going to show the ice of the ice because it doesn 't show the ice of the ice .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - Example #2
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'cal@@', 'ot@@', 'ter', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'rit@@', 'ical', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Hypothesis: The artics calotter is , in a sense , the critical heart of the global climate system .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - Example #3
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'in@@', 'ver@@', 'n', 'it', 'and', 'you', 're@@', 'ver@@', 'no@@', 'thing', '.', '</s>']
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Hypothesis: You can see invern it and you revernothing .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - Example #4
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:06:01,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:06:01,465 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid of the last 25 years .
2025-05-28 21:06:17,691 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.580858, Batch Acc: 0.591610, Tokens per Sec:     4478, Lr: 0.000300
2025-05-28 21:06:33,282 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.315510, Batch Acc: 0.597785, Tokens per Sec:     4564, Lr: 0.000300
2025-05-28 21:06:48,763 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.307481, Batch Acc: 0.596711, Tokens per Sec:     4648, Lr: 0.000300
2025-05-28 21:07:04,147 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.417732, Batch Acc: 0.590673, Tokens per Sec:     4716, Lr: 0.000300
2025-05-28 21:07:19,510 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.401541, Batch Acc: 0.591195, Tokens per Sec:     4624, Lr: 0.000300
2025-05-28 21:07:19,511 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:07:19,511 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:07:56,251 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.55, generation: 36.7319[sec], evaluation: 0.0000[sec]
2025-05-28 21:07:56,253 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:07:56,433 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/16000.ckpt
2025-05-28 21:07:56,436 - INFO - joeynmt.training - Example #0
2025-05-28 21:07:56,436 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:07:56,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:07:56,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'tur@@', 'ned', 'out', 'of', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:07:56,436 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:07:56,436 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:07:56,436 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slides to show that the glacial calculate , which for almost three million years had the size of 48 States , is returned out of 48 percent .
2025-05-28 21:07:56,436 - INFO - joeynmt.training - Example #1
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Hypothesis: But this subjected the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - Example #2
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'cu@@', 'p', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Hypothesis: The artical calculate is , in a certain sense , the cup of global climate system .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - Example #3
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'in@@', 'n', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - 	Hypothesis: It 's winn and you get summer .
2025-05-28 21:07:56,437 - INFO - joeynmt.training - Example #4
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:07:56,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:07:56,438 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:07:56,438 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:07:56,438 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quick carled on the last 25 years .
2025-05-28 21:08:11,982 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.950432, Batch Acc: 0.591077, Tokens per Sec:     4587, Lr: 0.000300
2025-05-28 21:08:27,976 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.395911, Batch Acc: 0.594704, Tokens per Sec:     4425, Lr: 0.000300
2025-05-28 21:08:42,682 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.466019, Batch Acc: 0.590665, Tokens per Sec:     4846, Lr: 0.000300
2025-05-28 21:08:58,258 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.564063, Batch Acc: 0.588992, Tokens per Sec:     4548, Lr: 0.000300
2025-05-28 21:09:13,525 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.375375, Batch Acc: 0.593127, Tokens per Sec:     4782, Lr: 0.000300
2025-05-28 21:09:13,526 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:09:13,526 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:09:56,453 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.92, acc:   0.55, generation: 42.9186[sec], evaluation: 0.0000[sec]
2025-05-28 21:09:56,455 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:09:56,639 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/16500.ckpt
2025-05-28 21:09:56,642 - INFO - joeynmt.training - Example #0
2025-05-28 21:09:56,642 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:09:56,642 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:09:56,642 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'p@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 21:09:56,642 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:09:56,642 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:09:56,642 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to show that the glacial calculate , which for almost three million years had the size of 48 of the United States continental , it 's repted 40 percent .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - Example #1
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ac@@', 'tion', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the thaction of the ice .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - Example #2
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Hypothesis: The artics of the artics is , in a sense , the heart of the global climate system .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - Example #3
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'it', 'to', 'be', 'su@@', 'm@@', 'm@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Hypothesis: You can expand it to be summer and you reverse .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - Example #4
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:09:56,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'li@@', 'p', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:09:56,643 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:09:56,644 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a flip on the last 25 years .
2025-05-28 21:10:11,843 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.453587, Batch Acc: 0.592551, Tokens per Sec:     4859, Lr: 0.000300
2025-05-28 21:10:26,587 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.361691, Batch Acc: 0.590284, Tokens per Sec:     4901, Lr: 0.000300
2025-05-28 21:10:41,684 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.443014, Batch Acc: 0.598448, Tokens per Sec:     4832, Lr: 0.000300
2025-05-28 21:10:56,755 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.490806, Batch Acc: 0.594693, Tokens per Sec:     4787, Lr: 0.000300
2025-05-28 21:11:11,617 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.470479, Batch Acc: 0.594154, Tokens per Sec:     4917, Lr: 0.000300
2025-05-28 21:11:11,618 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:11:11,618 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:11:51,283 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.87, acc:   0.55, generation: 39.6581[sec], evaluation: 0.0000[sec]
2025-05-28 21:11:51,285 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:11:51,470 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/17000.ckpt
2025-05-28 21:11:51,474 - INFO - joeynmt.training - Example #0
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'tur@@', 'ned', 'out', '40', 'percent', '.', '</s>']
2025-05-28 21:11:51,474 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:11:51,474 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:11:51,474 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to demonstrate that the calculate glacial , which for almost three million years had the size of the 48 States , it 's returned out 40 percent .
2025-05-28 21:11:51,474 - INFO - joeynmt.training - Example #1
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:11:51,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Hypothesis: But this subjected the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - Example #2
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'ar', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Hypothesis: The articy glacial car is , in a sense , the heart of global climate system .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - Example #3
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'going', 'to', 'w@@', 'on', 'and', 'you', 'get', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Hypothesis: It 's going to won and you get reverse .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - Example #4
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:11:51,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', 'old', '.', '</s>']
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:11:51,475 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carled on the last 25 years old .
2025-05-28 21:12:06,466 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.619793, Batch Acc: 0.591786, Tokens per Sec:     4898, Lr: 0.000300
2025-05-28 21:12:21,698 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.518571, Batch Acc: 0.587199, Tokens per Sec:     4724, Lr: 0.000300
2025-05-28 21:12:36,659 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.357059, Batch Acc: 0.588416, Tokens per Sec:     4740, Lr: 0.000300
2025-05-28 21:12:51,561 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.350457, Batch Acc: 0.594446, Tokens per Sec:     4850, Lr: 0.000300
2025-05-28 21:13:06,488 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.564247, Batch Acc: 0.596429, Tokens per Sec:     4766, Lr: 0.000300
2025-05-28 21:13:06,489 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:13:06,489 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:13:50,256 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.84, acc:   0.56, generation: 43.7595[sec], evaluation: 0.0000[sec]
2025-05-28 21:13:50,257 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:13:50,429 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/17500.ckpt
2025-05-28 21:13:50,432 - INFO - joeynmt.training - Example #0
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'been', 'the', 'dimen@@', 'si@@', 'ons', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'p@@', 'air', ',', 'it', "'s", '40', 'percent', '.', '</s>']
2025-05-28 21:13:50,432 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:13:50,432 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:13:50,432 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to demonstrate that the glacial glacial , which for almost three million years has been the dimensions of the United States continental , it 's repair , it 's 40 percent .
2025-05-28 21:13:50,432 - INFO - joeynmt.training - Example #1
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:13:50,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Hypothesis: But this subjects the gravity of the problem because it doesn 't show the ice of the ice of the ice of the ice .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - Example #2
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'i@@', 'al', 'cu@@', 'or', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Hypothesis: The artics is , in a certain sense , the cleanial cuor of the global climate system .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - Example #3
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'w@@', 'alk', 'up', 'and', 'you', 'get', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Hypothesis: You can walk up and you get reverse .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - Example #4
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:13:50,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'lo@@', 'ad@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:13:50,433 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a floaded on the last 25 years .
2025-05-28 21:14:05,769 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.249938, Batch Acc: 0.593863, Tokens per Sec:     4685, Lr: 0.000300
2025-05-28 21:14:20,589 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.504878, Batch Acc: 0.593652, Tokens per Sec:     4801, Lr: 0.000300
2025-05-28 21:14:35,437 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.552793, Batch Acc: 0.584240, Tokens per Sec:     4726, Lr: 0.000300
2025-05-28 21:14:51,036 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.465566, Batch Acc: 0.586396, Tokens per Sec:     4663, Lr: 0.000300
2025-05-28 21:15:05,612 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.555848, Batch Acc: 0.587865, Tokens per Sec:     4843, Lr: 0.000300
2025-05-28 21:15:05,613 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:15:05,613 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:15:45,231 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.55, generation: 39.6118[sec], evaluation: 0.0000[sec]
2025-05-28 21:15:45,386 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/18000.ckpt
2025-05-28 21:15:45,388 - INFO - joeynmt.training - Example #0
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', ',', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', ',', 'in', 'the', 'last', 'year', ',', 'we', "'re", 'c@@', 'ut', 'over', '40', 'percent', '.', '</s>']
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slides to demonstrate that the glacial calots , which for almost three million years , had the size of 48 , in the last year , we 're cut over 40 percent .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - Example #1
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - 	Hypothesis: But this underneath this underneath gravity of the problem because it doesn 't show the spessor of the ice .
2025-05-28 21:15:45,388 - INFO - joeynmt.training - Example #2
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:15:45,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'the', 'c@@', 'rit@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Hypothesis: The artical calots is , the critical calots is , the heart of the global climate system .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - Example #3
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'w@@', 'int@@', 'er', 'is', 'r@@', 'un@@', 's', '.', '</s>']
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Hypothesis: You can expand winter is runs .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - Example #4
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:15:45,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'qu@@', 'ick@@', 'ly', 's@@', 'li@@', 'de', 'on', 'the', 'av@@', 'el@@', 'ing', '2@@', '5', 'years', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:15:45,389 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quickly slide on the aveling 25 years of the last 25 years .
2025-05-28 21:16:00,837 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.417839, Batch Acc: 0.587443, Tokens per Sec:     4626, Lr: 0.000300
2025-05-28 21:16:15,624 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.196925, Batch Acc: 0.591064, Tokens per Sec:     4827, Lr: 0.000300
2025-05-28 21:16:30,782 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.480074, Batch Acc: 0.589947, Tokens per Sec:     4799, Lr: 0.000300
2025-05-28 21:16:45,601 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.451414, Batch Acc: 0.589166, Tokens per Sec:     4746, Lr: 0.000300
2025-05-28 21:17:00,141 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.454110, Batch Acc: 0.588571, Tokens per Sec:     4926, Lr: 0.000300
2025-05-28 21:17:00,142 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:17:00,142 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:17:39,340 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.56, generation: 39.1906[sec], evaluation: 0.0000[sec]
2025-05-28 21:17:39,342 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:17:39,487 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/18500.ckpt
2025-05-28 21:17:39,490 - INFO - joeynmt.training - Example #0
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'positi@@', 've', 'de@@', 'mon@@', 'str@@', 'ate', 'ar@@', 'c@@', 't@@', 'ical', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ated', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ded', '40', 'percent', '.', '</s>']
2025-05-28 21:17:39,490 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:17:39,490 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:17:39,490 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these positive demonstrate arctical calculate that the glacial calculated , which for almost three million years had the United States continental , it 's recorded 40 percent .
2025-05-28 21:17:39,490 - INFO - joeynmt.training - Example #1
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:17:39,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Hypothesis: But this subjects the gravity of the problem because it doesn 't show the spessor of the ice .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - Example #2
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'ar', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'c@@', 'ar', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Hypothesis: The articy glacial car , in a certain sense , the car heart of global climate system .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - Example #3
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'an@@', 'ding', 'it', 'and', 'you', 'get', 'to', 'ex@@', 't@@', 'rem@@', 'ely', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Hypothesis: You expanding it and you get to extremely reverse .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - Example #4
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:17:39,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 's@@', 'li@@', 'de', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:17:39,491 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a quickly slide on the last 25 years .
2025-05-28 21:17:54,257 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.295624, Batch Acc: 0.587026, Tokens per Sec:     4859, Lr: 0.000300
2025-05-28 21:18:09,133 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.486484, Batch Acc: 0.596867, Tokens per Sec:     4892, Lr: 0.000300
2025-05-28 21:18:24,061 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.291361, Batch Acc: 0.588496, Tokens per Sec:     4716, Lr: 0.000300
2025-05-28 21:18:38,530 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.344123, Batch Acc: 0.586065, Tokens per Sec:     4950, Lr: 0.000300
2025-05-28 21:18:53,504 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.453451, Batch Acc: 0.592315, Tokens per Sec:     4906, Lr: 0.000300
2025-05-28 21:18:53,505 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:18:53,505 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:19:31,270 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.56, generation: 37.7583[sec], evaluation: 0.0000[sec]
2025-05-28 21:19:31,274 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:19:31,436 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/19000.ckpt
2025-05-28 21:19:31,439 - INFO - joeynmt.training - Example #0
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'this', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Hypothesis: The year I showed this year , I showed these slide to demonstrate that the arctic glacial , which for almost three million years had the United States continental , it 's refted 40 percent .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - Example #1
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'end@@', 'ing', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the spending of the ice .
2025-05-28 21:19:31,439 - INFO - joeynmt.training - Example #2
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:19:31,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'of', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'c@@', 'ar', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Hypothesis: The arct of the artical calots is , in a way , the car of the global climate system .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - Example #3
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'and', 'it', "'s", 'r@@', 'iti@@', 've', '.', '</s>']
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Hypothesis: It 's expanding and it 's ritive .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - Example #4
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:19:31,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'f@@', 'ast@@', 'ed', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:19:31,440 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-fasted carlated on the last 25 years .
2025-05-28 21:19:46,497 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     1.383720, Batch Acc: 0.590883, Tokens per Sec:     4718, Lr: 0.000300
2025-05-28 21:20:01,455 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     1.274759, Batch Acc: 0.592036, Tokens per Sec:     4857, Lr: 0.000300
2025-05-28 21:20:10,975 - INFO - joeynmt.training - Epoch   5: total training loss 6204.94
2025-05-28 21:20:10,976 - INFO - joeynmt.training - EPOCH 6
2025-05-28 21:20:16,337 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.328394, Batch Acc: 0.609552, Tokens per Sec:     4781, Lr: 0.000300
2025-05-28 21:20:31,690 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.174167, Batch Acc: 0.613512, Tokens per Sec:     4689, Lr: 0.000300
2025-05-28 21:20:47,010 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.273076, Batch Acc: 0.615751, Tokens per Sec:     4893, Lr: 0.000300
2025-05-28 21:20:47,011 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:20:47,011 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:21:24,490 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.76, acc:   0.56, generation: 37.4716[sec], evaluation: 0.0000[sec]
2025-05-28 21:21:24,493 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:21:24,649 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/19500.ckpt
2025-05-28 21:21:24,649 - INFO - joeynmt.training - Example #0
2025-05-28 21:21:24,649 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'of', 'the', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ds', 'of', '40', 'percent', '.', '</s>']
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide demonstrate that the arct glacial calculate , which for almost three million years had the United States of the 48 States continental , it 's records of 40 percent .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - Example #1
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 't@@', 'le', 'this', 'under@@', 'l@@', 'ying', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Hypothesis: But this subjectle this underlying gravity of the problem because it doesn 't show the spector of the ice .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - Example #2
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'y', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'le@@', 'an@@', 'ing', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'system', '.', '</s>']
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Hypothesis: The articy calculate cleaning is , in a sense , the heart of the global system .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - Example #3
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:21:24,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'an@@', 'ding', 'on', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'r@@', 'ai@@', 'se', '.', '</s>']
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:21:24,650 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:21:24,651 - INFO - joeynmt.training - 	Hypothesis: You expanding on the winter and you raise .
2025-05-28 21:21:24,651 - INFO - joeynmt.training - Example #4
2025-05-28 21:21:24,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:21:24,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:21:24,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:21:24,651 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:21:24,651 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:21:24,651 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-25 years .
2025-05-28 21:21:39,386 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.266334, Batch Acc: 0.615743, Tokens per Sec:     4953, Lr: 0.000300
2025-05-28 21:21:54,057 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.344992, Batch Acc: 0.607125, Tokens per Sec:     4916, Lr: 0.000300
2025-05-28 21:22:08,675 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.257636, Batch Acc: 0.615722, Tokens per Sec:     4876, Lr: 0.000300
2025-05-28 21:22:23,258 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.450472, Batch Acc: 0.609383, Tokens per Sec:     4748, Lr: 0.000300
2025-05-28 21:22:38,187 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.372875, Batch Acc: 0.603782, Tokens per Sec:     4782, Lr: 0.000300
2025-05-28 21:22:38,189 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:22:38,189 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:23:19,182 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.79, acc:   0.56, generation: 40.9862[sec], evaluation: 0.0000[sec]
2025-05-28 21:23:19,372 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/20500.ckpt
2025-05-28 21:23:19,375 - INFO - joeynmt.training - Example #0
2025-05-28 21:23:19,375 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:23:19,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:23:19,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'positi@@', 've', 'de@@', 'mon@@', 'str@@', 'ate', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'ar@@', 't@@', 'ical', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'p@@', 'ly', '40', 'percent', '.', '</s>']
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these positive demonstrate slide to show that the artical glacial , which for almost three million years of the United States , it 's reply 40 percent .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - Example #1
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', ',', '</s>']
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Hypothesis: However this subjected the gravity of the problem because it doesn 't show the ice of the ice of the ice ,
2025-05-28 21:23:19,376 - INFO - joeynmt.training - Example #2
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'ar', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Hypothesis: The arct glacial calots is , in a sense , the clear of the global climate system .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - Example #3
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', "'s", 'in@@', 'ver@@', 'int@@', 'er', 'and', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - 	Hypothesis: You can see it 's inverinter and reverse it .
2025-05-28 21:23:19,376 - INFO - joeynmt.training - Example #4
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:23:19,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'f@@', 'ast@@', 'ed', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:23:19,377 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:23:19,377 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:23:19,377 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-fasted carlated on the last 25 years .
2025-05-28 21:23:34,283 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.402352, Batch Acc: 0.605482, Tokens per Sec:     4712, Lr: 0.000300
2025-05-28 21:23:49,192 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.400771, Batch Acc: 0.602479, Tokens per Sec:     4768, Lr: 0.000300
2025-05-28 21:24:04,196 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.400723, Batch Acc: 0.601787, Tokens per Sec:     4762, Lr: 0.000300
2025-05-28 21:24:18,839 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.462285, Batch Acc: 0.606192, Tokens per Sec:     4944, Lr: 0.000300
2025-05-28 21:24:33,373 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.331023, Batch Acc: 0.608007, Tokens per Sec:     4919, Lr: 0.000300
2025-05-28 21:24:33,374 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:24:33,374 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:25:14,004 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.56, generation: 40.6222[sec], evaluation: 0.0000[sec]
2025-05-28 21:25:14,187 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/20000.ckpt
2025-05-28 21:25:14,190 - INFO - joeynmt.training - Example #0
2025-05-28 21:25:14,190 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:25:14,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:25:14,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'U@@', '.@@', 'S@@', '.', '.', '.', '</s>']
2025-05-28 21:25:14,190 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slide demonstrate slide to demonstrate that the calculate glacial , which for almost three million years had the size of the U.S. . .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - Example #1
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'g@@', 'a@@', 'x@@', 'i@@', 'et@@', 'y', '.', '</s>']
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Hypothesis: But this subjects the gravity of the problem because it doesn 't show the gaxiety .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - Example #2
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'ing', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Hypothesis: The artics calculate is , in a sense , the cleaning of the global climate system .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - Example #3
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'it', 'to', 'be', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - 	Hypothesis: You can expand it to be summer .
2025-05-28 21:25:14,191 - INFO - joeynmt.training - Example #4
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:25:14,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:25:14,192 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:25:14,192 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:25:14,192 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast carlated on the last 25 years .
2025-05-28 21:25:29,193 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.349169, Batch Acc: 0.611135, Tokens per Sec:     4757, Lr: 0.000300
2025-05-28 21:25:43,962 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.444465, Batch Acc: 0.606523, Tokens per Sec:     4850, Lr: 0.000300
2025-05-28 21:25:58,759 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.401930, Batch Acc: 0.603415, Tokens per Sec:     4853, Lr: 0.000300
2025-05-28 21:26:13,806 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.281800, Batch Acc: 0.604928, Tokens per Sec:     4720, Lr: 0.000300
2025-05-28 21:26:28,550 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.377855, Batch Acc: 0.602450, Tokens per Sec:     4844, Lr: 0.000300
2025-05-28 21:26:28,551 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:26:28,551 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:27:06,892 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.72, acc:   0.56, generation: 38.3341[sec], evaluation: 0.0000[sec]
2025-05-28 21:27:06,895 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:27:07,120 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/21000.ckpt
2025-05-28 21:27:07,122 - INFO - joeynmt.training - Example #0
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'is', 'about', '40', 'million', 'years', 'had', 'had', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slides to demonstrate that the glacial glacial , which is about 40 million years had had had the size of the United States continental , it 's refed 40 percent .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - Example #1
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:27:07,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - 	Hypothesis: But that is the gravity of the problem because it doesn 't show the ice of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:27:07,122 - INFO - joeynmt.training - Example #2
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an@@', 'ing', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Hypothesis: The artics calculate glacial cleaning is , in a sense , the heart of the global climate system .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - Example #3
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', 'in@@', 'ver@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'no@@', 'w@@', 'ad@@', 's', '.', '</s>']
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Hypothesis: You can see it inverinter and you revernowads .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - Example #4
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:27:07,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'f@@', 'ast@@', 'er@@', 'n', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:27:07,123 - INFO - joeynmt.training - 	Hypothesis: The next slide is a fastern going to be a faster on the last 25 years .
2025-05-28 21:27:21,997 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.538484, Batch Acc: 0.601914, Tokens per Sec:     4774, Lr: 0.000300
2025-05-28 21:27:37,164 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.506032, Batch Acc: 0.611059, Tokens per Sec:     4882, Lr: 0.000300
2025-05-28 21:27:51,925 - INFO - joeynmt.training - Epoch   6, Step:    23800, Batch Loss:     1.375419, Batch Acc: 0.605580, Tokens per Sec:     4801, Lr: 0.000300
2025-05-28 21:28:07,342 - INFO - joeynmt.training - Epoch   6, Step:    23900, Batch Loss:     1.359054, Batch Acc: 0.605389, Tokens per Sec:     4675, Lr: 0.000300
2025-05-28 21:28:22,158 - INFO - joeynmt.training - Epoch   6, Step:    24000, Batch Loss:     1.243055, Batch Acc: 0.603160, Tokens per Sec:     4849, Lr: 0.000300
2025-05-28 21:28:22,158 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:28:22,158 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:29:00,741 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.72, acc:   0.56, generation: 38.5754[sec], evaluation: 0.0000[sec]
2025-05-28 21:29:00,913 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/22500.ckpt
2025-05-28 21:29:00,916 - INFO - joeynmt.training - Example #0
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'cor@@', 'ds', 'of', '40', 'percent', '.', '</s>']
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to demonstrate that the glacial calculus , which for almost three million years had the size of 48 million years had the United States , is records of 40 percent .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - Example #1
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'l@@', 'ying', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - 	Hypothesis: But this underlying this subjects the gravity of the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 21:29:00,916 - INFO - joeynmt.training - Example #2
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:29:00,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'c@@', 'alc@@', 'ul@@', 'us', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'ing', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Hypothesis: The artics calculus is , in a certain sense , the cleaning of global climate system .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - Example #3
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and@@', 'ing', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Hypothesis: You can expanding winter and you reverse it .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - Example #4
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:29:00,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'ast', 'car@@', 'd', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:29:00,917 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fast card of the last 25 years .
2025-05-28 21:29:16,045 - INFO - joeynmt.training - Epoch   6, Step:    24100, Batch Loss:     1.268176, Batch Acc: 0.606876, Tokens per Sec:     4655, Lr: 0.000300
2025-05-28 21:29:30,764 - INFO - joeynmt.training - Epoch   6, Step:    24200, Batch Loss:     1.381761, Batch Acc: 0.605615, Tokens per Sec:     4964, Lr: 0.000300
2025-05-28 21:29:45,931 - INFO - joeynmt.training - Epoch   6, Step:    24300, Batch Loss:     1.390494, Batch Acc: 0.599348, Tokens per Sec:     4776, Lr: 0.000300
2025-05-28 21:30:00,923 - INFO - joeynmt.training - Epoch   6, Step:    24400, Batch Loss:     1.510864, Batch Acc: 0.599391, Tokens per Sec:     4710, Lr: 0.000300
2025-05-28 21:30:15,696 - INFO - joeynmt.training - Epoch   6, Step:    24500, Batch Loss:     1.359779, Batch Acc: 0.608751, Tokens per Sec:     4968, Lr: 0.000300
2025-05-28 21:30:15,697 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:30:15,697 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:30:56,646 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.56, generation: 40.9425[sec], evaluation: 0.0000[sec]
2025-05-28 21:30:56,649 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:30:56,821 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/21500.ckpt
2025-05-28 21:30:56,824 - INFO - joeynmt.training - Example #0
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calculation , which for almost three million years had had the size of 48 percent .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - Example #1
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'is', 'a', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'end', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - 	Hypothesis: But this subjects this is a gravity of the problem because it doesn 't show the spend of the ice .
2025-05-28 21:30:56,824 - INFO - joeynmt.training - Example #2
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:30:56,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'cu@@', 'p', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Hypothesis: The arct glacial calots is , in a certain sense , the cup of the global climate system .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - Example #3
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'going', 'to', 'in@@', 'ver@@', 'int@@', 'en@@', 'se', 'and', 'you', 'get', 'to', 'the', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Hypothesis: It 's going to inverintense and you get to the summer .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - Example #4
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:30:56,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'de@@', 'gre@@', 'e', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:30:56,825 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-degree carlated on the last 25 years .
2025-05-28 21:31:12,152 - INFO - joeynmt.training - Epoch   6, Step:    24600, Batch Loss:     1.235692, Batch Acc: 0.603990, Tokens per Sec:     4578, Lr: 0.000300
2025-05-28 21:31:26,681 - INFO - joeynmt.training - Epoch   6, Step:    24700, Batch Loss:     1.217426, Batch Acc: 0.601804, Tokens per Sec:     4862, Lr: 0.000300
2025-05-28 21:31:41,544 - INFO - joeynmt.training - Epoch   6, Step:    24800, Batch Loss:     1.318331, Batch Acc: 0.601788, Tokens per Sec:     4838, Lr: 0.000300
2025-05-28 21:31:56,092 - INFO - joeynmt.training - Epoch   6, Step:    24900, Batch Loss:     1.364241, Batch Acc: 0.607521, Tokens per Sec:     4919, Lr: 0.000300
2025-05-28 21:32:10,755 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.483996, Batch Acc: 0.602262, Tokens per Sec:     4795, Lr: 0.000300
2025-05-28 21:32:10,756 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:32:10,756 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:32:46,720 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.68, acc:   0.57, generation: 35.9570[sec], evaluation: 0.0000[sec]
2025-05-28 21:32:46,723 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:32:46,922 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/23000.ckpt
2025-05-28 21:32:46,925 - INFO - joeynmt.training - Example #0
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'U@@', '.@@', 'S@@', '.', 'contin@@', 'ent@@', 'al', '1@@', '0@@', '-@@', 'si@@', 'ze', 'ar@@', 'c@@', 'tion', ',', 'you', "'re", 're@@', 'p@@', 'ut@@', 's', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the glacial calculation , which for almost three million years had the size of the U.S. continental 10-size arction , you 're reputs of the 48 percent .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - Example #1
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - 	Hypothesis: But this subjects the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:32:46,925 - INFO - joeynmt.training - Example #2
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:32:46,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'world', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Hypothesis: The glacial calculate is , in a certain sense , the world heart of the global climate system .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - Example #3
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'an@@', 'ding', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Hypothesis: It 's expanding winter and you reverse .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - Example #4
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:32:46,926 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'er', 'car@@', 'l@@', 'ated', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:32:46,926 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a faster carlated on the last 25 years .
2025-05-28 21:33:01,971 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.171658, Batch Acc: 0.608646, Tokens per Sec:     4818, Lr: 0.000300
2025-05-28 21:33:16,624 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.310994, Batch Acc: 0.603466, Tokens per Sec:     4973, Lr: 0.000300
2025-05-28 21:33:31,721 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.341532, Batch Acc: 0.610619, Tokens per Sec:     4755, Lr: 0.000300
2025-05-28 21:33:46,636 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.319762, Batch Acc: 0.613202, Tokens per Sec:     4896, Lr: 0.000300
2025-05-28 21:34:02,041 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.465126, Batch Acc: 0.601023, Tokens per Sec:     4672, Lr: 0.000300
2025-05-28 21:34:02,042 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:34:02,042 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:34:48,209 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.67, acc:   0.56, generation: 46.1600[sec], evaluation: 0.0000[sec]
2025-05-28 21:34:48,212 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:34:48,401 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/22000.ckpt
2025-05-28 21:34:48,403 - INFO - joeynmt.training - Example #0
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to show that the glacial calculate , which for almost three million years had the size of 48 years had the United States of the 48 percent of the United States .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - Example #1
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 21:34:48,403 - INFO - joeynmt.training - Example #2
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:34:48,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an@@', 'ing', 'is', ',', 'in', 'a', 'way', ',', 'the', 'cu@@', 'or', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Hypothesis: The arctic glacial cleaning is , in a way , the cuor of the global climate system .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - Example #3
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'you', 'get', 'out', 'of', 'su@@', 'm@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Hypothesis: You can expand you get out of summmer .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - Example #4
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:34:48,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ar@@', 'ter@@', 's', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:34:48,404 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quarters of the last 25 years .
2025-05-28 21:35:03,057 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.322897, Batch Acc: 0.605305, Tokens per Sec:     4879, Lr: 0.000300
2025-05-28 21:35:18,353 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.356012, Batch Acc: 0.608512, Tokens per Sec:     4870, Lr: 0.000300
2025-05-28 21:35:33,507 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.165913, Batch Acc: 0.611376, Tokens per Sec:     4810, Lr: 0.000300
2025-05-28 21:35:48,596 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.488492, Batch Acc: 0.608696, Tokens per Sec:     4741, Lr: 0.000300
2025-05-28 21:36:03,702 - INFO - joeynmt.training - Epoch   6, Step:    26000, Batch Loss:     1.402975, Batch Acc: 0.603079, Tokens per Sec:     4774, Lr: 0.000300
2025-05-28 21:36:03,702 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:36:03,702 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:36:46,684 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.57, generation: 42.9745[sec], evaluation: 0.0000[sec]
2025-05-28 21:36:46,689 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:36:46,927 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/24000.ckpt
2025-05-28 21:36:46,930 - INFO - joeynmt.training - Example #0
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'ven@@', 'i@@', 'al', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides to demonstrate that the glacial calculation , which for most three million years had the size of 48 States continental , it 's revenial 48 percent .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - Example #1
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - 	Hypothesis: However this subjected gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:36:46,930 - INFO - joeynmt.training - Example #2
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:36:46,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Hypothesis: The arct glacial calots is , in a way , the heart of the global climate system .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - Example #3
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'it', 'it', "'s", 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Hypothesis: You can expand it it 's reverse .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - Example #4
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:36:46,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'r@@', 'ich', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:36:46,931 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a rapid rich on the last 25 years .
2025-05-28 21:37:02,075 - INFO - joeynmt.training - Epoch   6, Step:    26100, Batch Loss:     1.300627, Batch Acc: 0.600775, Tokens per Sec:     4512, Lr: 0.000300
2025-05-28 21:37:03,962 - INFO - joeynmt.training - Epoch   6: total training loss 5967.77
2025-05-28 21:37:03,962 - INFO - joeynmt.training - EPOCH 7
2025-05-28 21:37:17,146 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.185463, Batch Acc: 0.628013, Tokens per Sec:     4836, Lr: 0.000300
2025-05-28 21:37:32,546 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.429707, Batch Acc: 0.628445, Tokens per Sec:     4742, Lr: 0.000300
2025-05-28 21:37:48,044 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.401643, Batch Acc: 0.621758, Tokens per Sec:     4710, Lr: 0.000300
2025-05-28 21:38:02,699 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.190554, Batch Acc: 0.628153, Tokens per Sec:     4778, Lr: 0.000300
2025-05-28 21:38:02,700 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:38:02,700 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:38:44,318 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.66, acc:   0.57, generation: 41.6087[sec], evaluation: 0.0000[sec]
2025-05-28 21:38:44,531 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/23500.ckpt
2025-05-28 21:38:44,533 - INFO - joeynmt.training - Example #0
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ations', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 21:38:44,533 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:38:44,533 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:38:44,533 - INFO - joeynmt.training - 	Hypothesis: And I showed this year I showed these slides to show that the glacial calculations , which for almost three million years had the size of the United States , it 's reduced 40 percent .
2025-05-28 21:38:44,533 - INFO - joeynmt.training - Example #1
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:38:44,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - Example #2
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'c@@', 'alc@@', 'ul@@', 'ations', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Hypothesis: The artics calculations is , in a way , the heart of global climate system .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - Example #3
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'int@@', 'er', 'exp@@', 'an@@', 'ding', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Hypothesis: It 's winter expanding and you get summer .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - Example #4
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:38:44,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', '2@@', '5', 'years', 'old', '.', '</s>']
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:38:44,534 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-25 years old .
2025-05-28 21:38:59,126 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.218572, Batch Acc: 0.623535, Tokens per Sec:     4799, Lr: 0.000300
2025-05-28 21:39:13,574 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.392961, Batch Acc: 0.620434, Tokens per Sec:     5001, Lr: 0.000300
2025-05-28 21:39:29,010 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.404350, Batch Acc: 0.622416, Tokens per Sec:     4588, Lr: 0.000300
2025-05-28 21:39:44,440 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.395660, Batch Acc: 0.620171, Tokens per Sec:     4733, Lr: 0.000300
2025-05-28 21:39:59,348 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.388893, Batch Acc: 0.612305, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 21:39:59,349 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:39:59,350 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:40:42,457 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.57, generation: 43.0999[sec], evaluation: 0.0000[sec]
2025-05-28 21:40:42,656 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/24500.ckpt
2025-05-28 21:40:42,657 - INFO - joeynmt.training - Example #0
2025-05-28 21:40:42,657 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:40:42,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:40:42,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'the', 'ar@@', 't@@', 'ical', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'cor@@', 'ds', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Hypothesis: The last year I 've shown these slides to demonstrate the artical calculation , which for almost three million years had the size of the 48 States , is records , it 's refted 40 percent .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - Example #1
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'that', 'gr@@', 'av@@', 'ity', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Hypothesis: But this subjects that gravity the problem because it doesn 't show the ice of the ice of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - Example #2
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'i@@', 'al', 'cu@@', 'le@@', 'gi@@', 'ant', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Hypothesis: The artical calots is , in a certain sense , the cleanial culegiant climate system .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - Example #3
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'w@@', 'on', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'to', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - 	Hypothesis: You won the winter and you get to summer .
2025-05-28 21:40:42,658 - INFO - joeynmt.training - Example #4
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:40:42,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:40:42,659 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'f@@', 'ast@@', '-@@', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:40:42,659 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:40:42,659 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:40:42,659 - INFO - joeynmt.training - 	Hypothesis: The next slide is a fast-couple of the last 25 years .
2025-05-28 21:40:57,164 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.253981, Batch Acc: 0.614681, Tokens per Sec:     4916, Lr: 0.000300
2025-05-28 21:41:11,917 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.167545, Batch Acc: 0.622002, Tokens per Sec:     4842, Lr: 0.000300
2025-05-28 21:41:26,898 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.212564, Batch Acc: 0.616627, Tokens per Sec:     4868, Lr: 0.000300
2025-05-28 21:41:42,445 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.366084, Batch Acc: 0.622023, Tokens per Sec:     4634, Lr: 0.000300
2025-05-28 21:41:57,681 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.295904, Batch Acc: 0.622993, Tokens per Sec:     4580, Lr: 0.000300
2025-05-28 21:41:57,684 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:41:57,684 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:42:42,340 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.63, acc:   0.57, generation: 44.6475[sec], evaluation: 0.0000[sec]
2025-05-28 21:42:42,342 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:42:42,555 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/25000.ckpt
2025-05-28 21:42:42,558 - INFO - joeynmt.training - Example #0
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'U@@', '.@@', 'S@@', '.', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Hypothesis: I showed this last year I showed these slides to demonstrate that the glacial glacial , which for almost three million years has had the size of the U.S. continental , it 's reduced 40 percent .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - Example #1
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'that', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Hypothesis: But this subjects that gravity of the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - Example #2
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an@@', 'ing', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial cleaning is , in a way , the heart of the global climate system .
2025-05-28 21:42:42,559 - INFO - joeynmt.training - Example #3
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:42:42,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'int@@', 'er', 'and', 're@@', 'ver@@', 'se', 'it', '.', '</s>']
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Hypothesis: It 's winter and reverse it .
2025-05-28 21:42:42,560 - INFO - joeynmt.training - Example #4
2025-05-28 21:42:42,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:42:42,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:42:42,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ated', 'on', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:42:42,560 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlated on on the last 25 years .
2025-05-28 21:42:57,327 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.328374, Batch Acc: 0.625792, Tokens per Sec:     4813, Lr: 0.000300
2025-05-28 21:43:11,915 - INFO - joeynmt.training - Epoch   7, Step:    27700, Batch Loss:     1.172109, Batch Acc: 0.624658, Tokens per Sec:     4884, Lr: 0.000300
2025-05-28 21:43:26,486 - INFO - joeynmt.training - Epoch   7, Step:    27800, Batch Loss:     1.321082, Batch Acc: 0.617960, Tokens per Sec:     4804, Lr: 0.000300
2025-05-28 21:43:41,249 - INFO - joeynmt.training - Epoch   7, Step:    27900, Batch Loss:     1.433847, Batch Acc: 0.612721, Tokens per Sec:     4932, Lr: 0.000300
2025-05-28 21:43:55,802 - INFO - joeynmt.training - Epoch   7, Step:    28000, Batch Loss:     1.389416, Batch Acc: 0.620754, Tokens per Sec:     4772, Lr: 0.000300
2025-05-28 21:43:55,803 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:43:55,803 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:44:35,443 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.64, acc:   0.57, generation: 39.6326[sec], evaluation: 0.0000[sec]
2025-05-28 21:44:35,688 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/25500.ckpt
2025-05-28 21:44:35,689 - INFO - joeynmt.training - Example #0
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'been', 'sho@@', 'wn', 'this', 'last', 'year', ',', 'I', "'ve", 'been', 'sho@@', 'wn', 'this', 's@@', 'li@@', 'de', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'it', "'s", 'about', '40', 'percent', '.', '</s>']
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Hypothesis: I 've been shown this last year , I 've been shown this slide that the glacial glacial , which for almost three million years had the size of the United States , it 's about 40 percent .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - Example #1
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ated', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - 	Hypothesis: But this underestimated the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:44:35,690 - INFO - joeynmt.training - Example #2
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:44:35,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'C@@', 'm@@', 'm@@', 'm@@', 'ess', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Hypothesis: The Cmmmess glacial calots is , in a certain sense , the heart of the global climate system .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - Example #3
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'w@@', 'int@@', 'er', 'down', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Hypothesis: You can expand winter down and you reverse .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - Example #4
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:44:35,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:44:35,691 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a quickly on the last 25 years .
2025-05-28 21:44:50,642 - INFO - joeynmt.training - Epoch   7, Step:    28100, Batch Loss:     1.396187, Batch Acc: 0.611001, Tokens per Sec:     4891, Lr: 0.000300
2025-05-28 21:45:05,829 - INFO - joeynmt.training - Epoch   7, Step:    28200, Batch Loss:     1.403018, Batch Acc: 0.612578, Tokens per Sec:     4705, Lr: 0.000300
2025-05-28 21:45:20,631 - INFO - joeynmt.training - Epoch   7, Step:    28300, Batch Loss:     1.511537, Batch Acc: 0.617187, Tokens per Sec:     4728, Lr: 0.000300
2025-05-28 21:45:36,808 - INFO - joeynmt.training - Epoch   7, Step:    28400, Batch Loss:     1.438654, Batch Acc: 0.615104, Tokens per Sec:     4411, Lr: 0.000300
2025-05-28 21:45:53,277 - INFO - joeynmt.training - Epoch   7, Step:    28500, Batch Loss:     1.366021, Batch Acc: 0.614435, Tokens per Sec:     4337, Lr: 0.000300
2025-05-28 21:45:53,278 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:45:53,278 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:46:37,342 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.57, generation: 44.0567[sec], evaluation: 0.0000[sec]
2025-05-28 21:46:37,346 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:46:37,458 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/26500.ckpt
2025-05-28 21:46:37,463 - INFO - joeynmt.training - Example #0
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'last', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'f@@', 'ted', 'about', '40', 'percent', '.', '</s>']
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Hypothesis: I showed this last year , I showed these slides to show that the glacial glacial , which for almost three million years had the size of the United States , is refted about 40 percent .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - Example #1
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', ',', 'this', 'sub@@', 'j@@', 'ec@@', 'tive', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - 	Hypothesis: However , this subjective gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:46:37,463 - INFO - joeynmt.training - Example #2
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:46:37,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Hypothesis: The artical calots is , in a way , the heart of the global climate system .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - Example #3
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'of', 'the', 'su@@', 'm@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Hypothesis: You can expand of the sumer and you reverse .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - Example #4
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:46:37,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'qu@@', 'ick', 'f@@', 'res@@', 'h', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:46:37,464 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quick fresh of the last 25 years .
2025-05-28 21:46:52,604 - INFO - joeynmt.training - Epoch   7, Step:    28600, Batch Loss:     1.323746, Batch Acc: 0.620508, Tokens per Sec:     4730, Lr: 0.000300
2025-05-28 21:47:07,550 - INFO - joeynmt.training - Epoch   7, Step:    28700, Batch Loss:     1.165055, Batch Acc: 0.606891, Tokens per Sec:     4825, Lr: 0.000300
2025-05-28 21:47:22,366 - INFO - joeynmt.training - Epoch   7, Step:    28800, Batch Loss:     1.431191, Batch Acc: 0.618277, Tokens per Sec:     4882, Lr: 0.000300
2025-05-28 21:47:37,070 - INFO - joeynmt.training - Epoch   7, Step:    28900, Batch Loss:     1.325194, Batch Acc: 0.615452, Tokens per Sec:     4848, Lr: 0.000300
2025-05-28 21:47:51,903 - INFO - joeynmt.training - Epoch   7, Step:    29000, Batch Loss:     1.326966, Batch Acc: 0.618182, Tokens per Sec:     4815, Lr: 0.000300
2025-05-28 21:47:51,904 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:47:51,904 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:48:30,359 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.57, acc:   0.57, generation: 38.4468[sec], evaluation: 0.0000[sec]
2025-05-28 21:48:30,361 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:48:30,500 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/27000.ckpt
2025-05-28 21:48:30,504 - INFO - joeynmt.training - Example #0
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'this', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'he@@', 'at', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Hypothesis: I 've shown this last year I showed these slides to show that the glacial glacial heat , which for almost three million years had the size of the United States continental , it 's refted 40 percent .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - Example #1
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ro@@', 'w', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn 't show the throw of the ice .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - Example #2
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:48:30,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ica', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:48:30,504 - INFO - joeynmt.training - 	Hypothesis: The artica glacial calots is , in a way , the heart of the global climate system .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - Example #3
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'to', 'in@@', 'ver@@', 'ther', 'ex@@', 'ten@@', 'si@@', 'ons', '.', '</s>']
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Hypothesis: You can expand to inverther extensions .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - Example #4
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:48:30,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'er@@', 'n', 'lo@@', 'wer', 'to', 'be', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:48:30,505 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fastern lower to be 25 years .
2025-05-28 21:48:45,393 - INFO - joeynmt.training - Epoch   7, Step:    29100, Batch Loss:     1.259845, Batch Acc: 0.615090, Tokens per Sec:     4778, Lr: 0.000300
2025-05-28 21:49:00,754 - INFO - joeynmt.training - Epoch   7, Step:    29200, Batch Loss:     1.348759, Batch Acc: 0.614689, Tokens per Sec:     4600, Lr: 0.000300
2025-05-28 21:49:16,011 - INFO - joeynmt.training - Epoch   7, Step:    29300, Batch Loss:     1.316506, Batch Acc: 0.611855, Tokens per Sec:     4642, Lr: 0.000300
2025-05-28 21:49:31,405 - INFO - joeynmt.training - Epoch   7, Step:    29400, Batch Loss:     1.278822, Batch Acc: 0.618766, Tokens per Sec:     4551, Lr: 0.000300
2025-05-28 21:49:47,897 - INFO - joeynmt.training - Epoch   7, Step:    29500, Batch Loss:     1.492536, Batch Acc: 0.611855, Tokens per Sec:     4445, Lr: 0.000300
2025-05-28 21:49:47,897 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:49:47,897 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:50:32,245 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.57, generation: 44.3395[sec], evaluation: 0.0000[sec]
2025-05-28 21:50:32,247 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:50:32,346 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/26000.ckpt
2025-05-28 21:50:32,350 - INFO - joeynmt.training - Example #0
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'w@@', 'ed', 'this', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'this', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'U@@', '.@@', 'S@@', '.', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ed', 'over', '40', 'percent', '.', '</s>']
2025-05-28 21:50:32,350 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:50:32,350 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:50:32,350 - INFO - joeynmt.training - 	Hypothesis: I 've showed this last year I showed this slide to demonstrate that glacial glacial , which for almost three million years has been the size of the U.S. continental , it 's refed over 40 percent .
2025-05-28 21:50:32,350 - INFO - joeynmt.training - Example #1
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:50:32,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:50:32,350 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:50:32,350 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - Example #2
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Hypothesis: The artics glacial calots is , in a way , the heart of global climate system .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - Example #3
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'in@@', 'ver@@', 's', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Hypothesis: You can get invers and you get summer .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - Example #4
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:50:32,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'car@@', 'l@@', 'ate', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:50:32,351 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast carlate on the last 25 years .
2025-05-28 21:50:48,084 - INFO - joeynmt.training - Epoch   7, Step:    29600, Batch Loss:     1.292979, Batch Acc: 0.618406, Tokens per Sec:     4642, Lr: 0.000300
2025-05-28 21:51:04,068 - INFO - joeynmt.training - Epoch   7, Step:    29700, Batch Loss:     1.389891, Batch Acc: 0.613406, Tokens per Sec:     4553, Lr: 0.000300
2025-05-28 21:51:19,690 - INFO - joeynmt.training - Epoch   7, Step:    29800, Batch Loss:     1.231755, Batch Acc: 0.614188, Tokens per Sec:     4553, Lr: 0.000300
2025-05-28 21:51:35,439 - INFO - joeynmt.training - Epoch   7, Step:    29900, Batch Loss:     1.184351, Batch Acc: 0.612396, Tokens per Sec:     4687, Lr: 0.000300
2025-05-28 21:51:52,168 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.321325, Batch Acc: 0.617467, Tokens per Sec:     4212, Lr: 0.000300
2025-05-28 21:51:52,170 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:51:52,170 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:52:29,676 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.57, generation: 37.4989[sec], evaluation: 0.0000[sec]
2025-05-28 21:52:29,678 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:52:29,768 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/28000.ckpt
2025-05-28 21:52:29,772 - INFO - joeynmt.training - Example #0
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'p@@', 'ut@@', 'ed', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Hypothesis: I showed this year , I showed these slides to show that the glacial calculate , which for almost three million years had been the size of the 48 States , is reputed the 48 percent .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - Example #1
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'that', 'gr@@', 'av@@', 'ity', ',', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Hypothesis: But this subjects that gravity , because it doesn 't show the spessor of the ice .
2025-05-28 21:52:29,773 - INFO - joeynmt.training - Example #2
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:52:29,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'way', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:52:29,773 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Hypothesis: The arctics is , in a way , in a way , the heart sense , the heart of the global climate system .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - Example #3
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'in@@', 'ver@@', 'int@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Hypothesis: It 's inverinter and you get summer .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - Example #4
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:52:29,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ess@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:52:29,774 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlessed on the last 25 years .
2025-05-28 21:52:44,617 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.366628, Batch Acc: 0.612154, Tokens per Sec:     4789, Lr: 0.000300
2025-05-28 21:52:59,586 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.319376, Batch Acc: 0.607926, Tokens per Sec:     4829, Lr: 0.000300
2025-05-28 21:53:14,454 - INFO - joeynmt.training - Epoch   7, Step:    30300, Batch Loss:     1.566275, Batch Acc: 0.616374, Tokens per Sec:     4773, Lr: 0.000300
2025-05-28 21:53:29,408 - INFO - joeynmt.training - Epoch   7, Step:    30400, Batch Loss:     1.497396, Batch Acc: 0.622191, Tokens per Sec:     4934, Lr: 0.000300
2025-05-28 21:53:39,492 - INFO - joeynmt.training - Epoch   7: total training loss 5791.43
2025-05-28 21:53:39,493 - INFO - joeynmt.training - EPOCH 8
2025-05-28 21:53:44,085 - INFO - joeynmt.training - Epoch   8, Step:    30500, Batch Loss:     1.424145, Batch Acc: 0.638659, Tokens per Sec:     5054, Lr: 0.000300
2025-05-28 21:53:44,086 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:53:44,086 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:54:20,610 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.57, generation: 36.5162[sec], evaluation: 0.0000[sec]
2025-05-28 21:54:20,697 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/27500.ckpt
2025-05-28 21:54:20,699 - INFO - joeynmt.training - Example #0
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'ates', ',', 'is', 're@@', 'cor@@', 'd', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 21:54:20,699 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:54:20,699 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:54:20,699 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calculation , which for almost three million years had the size of the 48 States , is record of the 48 percent .
2025-05-28 21:54:20,699 - INFO - joeynmt.training - Example #1
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:54:20,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:54:20,699 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:54:20,699 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Hypothesis: But this undergravity of the problem because it doesn 't show the ice of the ice because it doesn 't show the ice of the ice .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - Example #2
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Hypothesis: The artical calots is , in a sense , the heart of the global climate system .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - Example #3
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'w@@', 'int@@', 'er', 'down', 'and', 'you', 're@@', 'ach', '.', '</s>']
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Hypothesis: You can winter down and you reach .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - Example #4
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:54:20,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'in@@', '-@@', 'year@@', '-@@', 'old', ',', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:54:20,700 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-in-year-old , the last 25 years .
2025-05-28 21:54:35,648 - INFO - joeynmt.training - Epoch   8, Step:    30600, Batch Loss:     1.390141, Batch Acc: 0.631065, Tokens per Sec:     4866, Lr: 0.000300
2025-05-28 21:54:50,938 - INFO - joeynmt.training - Epoch   8, Step:    30700, Batch Loss:     1.238366, Batch Acc: 0.633990, Tokens per Sec:     4622, Lr: 0.000300
2025-05-28 21:55:05,621 - INFO - joeynmt.training - Epoch   8, Step:    30800, Batch Loss:     1.228513, Batch Acc: 0.631627, Tokens per Sec:     4836, Lr: 0.000300
2025-05-28 21:55:20,052 - INFO - joeynmt.training - Epoch   8, Step:    30900, Batch Loss:     1.342606, Batch Acc: 0.633023, Tokens per Sec:     5146, Lr: 0.000300
2025-05-28 21:55:34,777 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.257211, Batch Acc: 0.633996, Tokens per Sec:     4856, Lr: 0.000300
2025-05-28 21:55:34,778 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:55:34,778 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:56:14,786 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.59, acc:   0.57, generation: 39.9993[sec], evaluation: 0.0000[sec]
2025-05-28 21:56:14,888 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/28500.ckpt
2025-05-28 21:56:14,890 - INFO - joeynmt.training - Example #0
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'he@@', 'at', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'a', 'whole', 'thing', 'about', '40', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Hypothesis: The last year I showed this year I showed to show that the glacial heat , which for almost three million years had the size of the United States , a whole thing about 40 percent of the United States continental , it 's refted 40 percent .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - Example #1
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'that', 'sub@@', 'j@@', 'ects', 'that', 'under@@', 'ne@@', 'at@@', 'h', 'gr@@', 'av@@', 'ity', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Hypothesis: But that subjects that underneath gravity because it doesn 't show the spector of the ice .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - Example #2
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - 	Hypothesis: The arctic calots is , in a sense , the heart of the global climate system .
2025-05-28 21:56:14,891 - INFO - joeynmt.training - Example #3
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:56:14,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ach', '.', '</s>']
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Hypothesis: You can see the winter and you reach .
2025-05-28 21:56:14,892 - INFO - joeynmt.training - Example #4
2025-05-28 21:56:14,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:56:14,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:56:14,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:56:14,892 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-couple of the last 25 years .
2025-05-28 21:56:29,102 - INFO - joeynmt.training - Epoch   8, Step:    31100, Batch Loss:     1.108297, Batch Acc: 0.632804, Tokens per Sec:     4997, Lr: 0.000300
2025-05-28 21:56:44,465 - INFO - joeynmt.training - Epoch   8, Step:    31200, Batch Loss:     1.214614, Batch Acc: 0.636514, Tokens per Sec:     4844, Lr: 0.000300
2025-05-28 21:56:59,163 - INFO - joeynmt.training - Epoch   8, Step:    31300, Batch Loss:     1.444365, Batch Acc: 0.626806, Tokens per Sec:     4903, Lr: 0.000300
2025-05-28 21:57:13,994 - INFO - joeynmt.training - Epoch   8, Step:    31400, Batch Loss:     1.257844, Batch Acc: 0.634154, Tokens per Sec:     4735, Lr: 0.000300
2025-05-28 21:57:29,087 - INFO - joeynmt.training - Epoch   8, Step:    31500, Batch Loss:     1.439995, Batch Acc: 0.632513, Tokens per Sec:     4873, Lr: 0.000300
2025-05-28 21:57:29,088 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:57:29,088 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 21:58:07,845 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.57, generation: 38.7504[sec], evaluation: 0.0000[sec]
2025-05-28 21:58:07,848 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 21:58:07,942 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/31000.ckpt
2025-05-28 21:58:07,944 - INFO - joeynmt.training - Example #0
2025-05-28 21:58:07,944 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 21:58:07,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 21:58:07,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'duc@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calots , that for almost three million years had the size 48 States continental , it 's reduced 40 percent .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - Example #1
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', ',', 'this', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ed', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Hypothesis: However , this underestablished gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - Example #2
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'le', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - 	Hypothesis: The article calots is , in a way , the heart of the global climate system .
2025-05-28 21:58:07,945 - INFO - joeynmt.training - Example #3
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 21:58:07,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'and', 'w@@', 'int@@', 'er', 'exp@@', 'lo@@', 'd', 'and', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Hypothesis: You expand winter explod and summer .
2025-05-28 21:58:07,946 - INFO - joeynmt.training - Example #4
2025-05-28 21:58:07,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 21:58:07,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 21:58:07,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 21:58:07,946 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carled on the last 25 years .
2025-05-28 21:58:22,758 - INFO - joeynmt.training - Epoch   8, Step:    31600, Batch Loss:     1.346965, Batch Acc: 0.626675, Tokens per Sec:     4879, Lr: 0.000300
2025-05-28 21:58:37,275 - INFO - joeynmt.training - Epoch   8, Step:    31700, Batch Loss:     1.403801, Batch Acc: 0.624023, Tokens per Sec:     5079, Lr: 0.000300
2025-05-28 21:58:52,117 - INFO - joeynmt.training - Epoch   8, Step:    31800, Batch Loss:     1.196826, Batch Acc: 0.629073, Tokens per Sec:     4979, Lr: 0.000300
2025-05-28 21:59:07,472 - INFO - joeynmt.training - Epoch   8, Step:    31900, Batch Loss:     1.359976, Batch Acc: 0.626907, Tokens per Sec:     4662, Lr: 0.000300
2025-05-28 21:59:22,531 - INFO - joeynmt.training - Epoch   8, Step:    32000, Batch Loss:     1.307290, Batch Acc: 0.626803, Tokens per Sec:     4793, Lr: 0.000300
2025-05-28 21:59:22,532 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 21:59:22,532 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:00:01,058 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.57, acc:   0.57, generation: 38.5183[sec], evaluation: 0.0000[sec]
2025-05-28 22:00:01,162 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/29000.ckpt
2025-05-28 22:00:01,167 - INFO - joeynmt.training - Example #0
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'p@@', 'ly', '40', 'percent', '.', '</s>']
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calots , which for almost three million years had the size of 48 States continental , it 's reply 40 percent .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - Example #1
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the problem because it doesn 't show the ice of the ice .
2025-05-28 22:00:01,167 - INFO - joeynmt.training - Example #2
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:00:01,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'ul@@', 'ty', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Hypothesis: The articulty glacial calots is , in a way , the heart of the global climate system .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - Example #3
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'into', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Hypothesis: It 's expanding into the winter and you get summmer .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - Example #4
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:00:01,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'oni@@', 'sh@@', 'ing', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:00:01,168 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fastonishing on the last 25 years .
2025-05-28 22:00:15,330 - INFO - joeynmt.training - Epoch   8, Step:    32100, Batch Loss:     1.190180, Batch Acc: 0.626594, Tokens per Sec:     5055, Lr: 0.000300
2025-05-28 22:00:29,915 - INFO - joeynmt.training - Epoch   8, Step:    32200, Batch Loss:     1.383381, Batch Acc: 0.628513, Tokens per Sec:     4987, Lr: 0.000300
2025-05-28 22:00:44,998 - INFO - joeynmt.training - Epoch   8, Step:    32300, Batch Loss:     1.086981, Batch Acc: 0.622683, Tokens per Sec:     4635, Lr: 0.000300
2025-05-28 22:00:59,367 - INFO - joeynmt.training - Epoch   8, Step:    32400, Batch Loss:     1.240766, Batch Acc: 0.622675, Tokens per Sec:     4980, Lr: 0.000300
2025-05-28 22:01:13,947 - INFO - joeynmt.training - Epoch   8, Step:    32500, Batch Loss:     1.345311, Batch Acc: 0.619480, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 22:01:13,948 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:01:13,948 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:01:57,624 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.51, acc:   0.57, generation: 43.6698[sec], evaluation: 0.0000[sec]
2025-05-28 22:01:57,626 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:01:57,727 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/32000.ckpt
2025-05-28 22:01:57,729 - INFO - joeynmt.training - Example #0
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'p@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calots , which for almost three million years had the size of 48 States continental , it 's reped 40 percent .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - Example #1
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'in', 'the', 'way', ',', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - 	Hypothesis: But in the way , this subjects the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:01:57,729 - INFO - joeynmt.training - Example #2
2025-05-28 22:01:57,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'way', ',', 'the', 'ar@@', 't@@', 'ics', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Hypothesis: The artics is , in a way , the artics of the global climate system .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - Example #3
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'it', 'it', 're@@', 'ver@@', 's', '.', '</s>']
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Hypothesis: You can expand it it revers .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - Example #4
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:01:57,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'very', 'qu@@', 'ick@@', 'ly', 'car@@', 'l@@', 'ay', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:01:57,730 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a very quickly carlay on the last 25 years .
2025-05-28 22:02:12,430 - INFO - joeynmt.training - Epoch   8, Step:    32600, Batch Loss:     1.392642, Batch Acc: 0.628236, Tokens per Sec:     4896, Lr: 0.000300
2025-05-28 22:02:27,239 - INFO - joeynmt.training - Epoch   8, Step:    32700, Batch Loss:     1.404536, Batch Acc: 0.628647, Tokens per Sec:     5050, Lr: 0.000300
2025-05-28 22:02:42,078 - INFO - joeynmt.training - Epoch   8, Step:    32800, Batch Loss:     1.332914, Batch Acc: 0.628791, Tokens per Sec:     4691, Lr: 0.000300
2025-05-28 22:02:57,172 - INFO - joeynmt.training - Epoch   8, Step:    32900, Batch Loss:     1.281427, Batch Acc: 0.626710, Tokens per Sec:     4844, Lr: 0.000300
2025-05-28 22:03:12,016 - INFO - joeynmt.training - Epoch   8, Step:    33000, Batch Loss:     1.078124, Batch Acc: 0.629915, Tokens per Sec:     4743, Lr: 0.000300
2025-05-28 22:03:12,018 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:03:12,018 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:03:54,131 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.57, generation: 42.1062[sec], evaluation: 0.0000[sec]
2025-05-28 22:03:54,233 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/30500.ckpt
2025-05-28 22:03:54,235 - INFO - joeynmt.training - Example #0
2025-05-28 22:03:54,235 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:03:54,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:03:54,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'this', 'last', 'year', ',', 'and', 'I', 'sho@@', 'w@@', 'ed', 'this', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 't@@', 'ics', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'tre@@', 'es', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:03:54,235 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:03:54,235 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:03:54,235 - INFO - joeynmt.training - 	Hypothesis: I 've shown this last year , and I showed this slides to show that the artics glacial , which for almost three million years had the size of the 48 million years had the United States continental , it 's retrees of the 48 percent .
2025-05-28 22:03:54,235 - INFO - joeynmt.training - Example #1
2025-05-28 22:03:54,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:03:54,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'ice', '.', '</s>']
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Hypothesis: But this subjects the gravity of the problem because it doesn 't show the ice of ice .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - Example #2
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', 'he@@', 'at', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Hypothesis: The artics of the artics heat is , in a sense , the heart of the global climate system .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - Example #3
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'of', 'in@@', 'ver@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Hypothesis: You can expand of inverinter and you reverse .
2025-05-28 22:03:54,236 - INFO - joeynmt.training - Example #4
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:03:54,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd', 'on', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:03:54,236 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:03:54,237 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:03:54,237 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick card on on the last 25 years .
2025-05-28 22:04:09,080 - INFO - joeynmt.training - Epoch   8, Step:    33100, Batch Loss:     1.274382, Batch Acc: 0.624187, Tokens per Sec:     4731, Lr: 0.000300
2025-05-28 22:04:23,548 - INFO - joeynmt.training - Epoch   8, Step:    33200, Batch Loss:     1.384990, Batch Acc: 0.620428, Tokens per Sec:     4846, Lr: 0.000300
2025-05-28 22:04:38,364 - INFO - joeynmt.training - Epoch   8, Step:    33300, Batch Loss:     1.325906, Batch Acc: 0.623984, Tokens per Sec:     4773, Lr: 0.000300
2025-05-28 22:04:52,731 - INFO - joeynmt.training - Epoch   8, Step:    33400, Batch Loss:     1.090929, Batch Acc: 0.630958, Tokens per Sec:     4888, Lr: 0.000300
2025-05-28 22:05:07,324 - INFO - joeynmt.training - Epoch   8, Step:    33500, Batch Loss:     1.294072, Batch Acc: 0.620784, Tokens per Sec:     4737, Lr: 0.000300
2025-05-28 22:05:07,325 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:05:07,325 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:05:47,027 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.57, generation: 39.6950[sec], evaluation: 0.0000[sec]
2025-05-28 22:05:47,136 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/29500.ckpt
2025-05-28 22:05:47,140 - INFO - joeynmt.training - Example #0
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'f@@', 'er@@', 'r@@', 'up@@', 's', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'p@@', 'ec@@', 'ted', 'by', '40', 'percent', '.', '</s>']
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Hypothesis: I 've shown these slides to show that the glacial calots to show that the arct glacial , which for almost three million years had the size of 48 percent of the United States , is referrups of the United States , is repected by 40 percent .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - Example #1
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'under@@', 'l@@', 'ying', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - 	Hypothesis: But this is the underlying gravity of the problem because it doesn 't show the ice of the ice because it doesn 't show the ice of the ice .
2025-05-28 22:05:47,140 - INFO - joeynmt.training - Example #2
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:05:47,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'of', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'cer@@', 'tain', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Hypothesis: The arct of the artical calots is , in a certain sense , the heart of the global climate system .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - Example #3
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'exp@@', 'and@@', 'ing', ',', 'you', 'exp@@', 'and', 'it', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Hypothesis: It expanding , you expand it reverse .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - Example #4
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:05:47,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd', 'on', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:05:47,141 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick card on of the last 25 years .
2025-05-28 22:06:01,467 - INFO - joeynmt.training - Epoch   8, Step:    33600, Batch Loss:     1.281533, Batch Acc: 0.622782, Tokens per Sec:     5016, Lr: 0.000300
2025-05-28 22:06:16,184 - INFO - joeynmt.training - Epoch   8, Step:    33700, Batch Loss:     1.290066, Batch Acc: 0.628903, Tokens per Sec:     4876, Lr: 0.000300
2025-05-28 22:06:31,161 - INFO - joeynmt.training - Epoch   8, Step:    33800, Batch Loss:     1.171881, Batch Acc: 0.624294, Tokens per Sec:     4657, Lr: 0.000300
2025-05-28 22:06:45,808 - INFO - joeynmt.training - Epoch   8, Step:    33900, Batch Loss:     1.231779, Batch Acc: 0.624989, Tokens per Sec:     4854, Lr: 0.000300
2025-05-28 22:07:00,708 - INFO - joeynmt.training - Epoch   8, Step:    34000, Batch Loss:     1.455329, Batch Acc: 0.623566, Tokens per Sec:     4839, Lr: 0.000300
2025-05-28 22:07:00,709 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:07:00,709 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:07:37,234 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.51, acc:   0.58, generation: 36.5182[sec], evaluation: 0.0000[sec]
2025-05-28 22:07:37,237 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:07:37,340 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/30000.ckpt
2025-05-28 22:07:37,344 - INFO - joeynmt.training - Example #0
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'been', 'sho@@', 'wn', 'this', 'last', 'year', ',', 'which', 'is', 'a', 'de@@', 'ep@@', '-@@', 'si@@', 'x@@', '-@@', 'to@@', '-@@', 'de@@', 'mon@@', 'str@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'and', 'it', "'s", 're@@', 'f@@', 'ted', 'about', '40', 'percent', '.', '</s>']
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Hypothesis: I 've been shown this last year , which is a deep-six-to-demonstrate glacial , which for almost three million years had the size of the United States continental , and it 's refted about 40 percent .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - Example #1
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'that', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'ice', '.', '</s>']
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - 	Hypothesis: But this subjects that is the gravity of the problem because it doesn 't show the ice of ice .
2025-05-28 22:07:37,344 - INFO - joeynmt.training - Example #2
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:07:37,344 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'ing', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Hypothesis: The artta glacial calots is , in a sense , the cleaning of the global climate system .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - Example #3
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'int@@', 'er', 'exp@@', 'ands', 'of', 'su@@', 'm@@', 'm@@', 'er', 'and', 'g@@', 'ets', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Hypothesis: It 's winter expands of summer and gets summer .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - Example #4
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:07:37,345 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'car@@', 'r@@', 'ying', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:07:37,345 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast carrying on the last 25 years .
2025-05-28 22:07:52,024 - INFO - joeynmt.training - Epoch   8, Step:    34100, Batch Loss:     1.213739, Batch Acc: 0.626790, Tokens per Sec:     4903, Lr: 0.000300
2025-05-28 22:08:07,257 - INFO - joeynmt.training - Epoch   8, Step:    34200, Batch Loss:     1.289958, Batch Acc: 0.623217, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 22:08:21,848 - INFO - joeynmt.training - Epoch   8, Step:    34300, Batch Loss:     1.291428, Batch Acc: 0.623656, Tokens per Sec:     5011, Lr: 0.000300
2025-05-28 22:08:36,430 - INFO - joeynmt.training - Epoch   8, Step:    34400, Batch Loss:     1.342629, Batch Acc: 0.625917, Tokens per Sec:     4901, Lr: 0.000300
2025-05-28 22:08:51,199 - INFO - joeynmt.training - Epoch   8, Step:    34500, Batch Loss:     1.388278, Batch Acc: 0.623758, Tokens per Sec:     4866, Lr: 0.000300
2025-05-28 22:08:51,201 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:08:51,201 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:09:32,019 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.46, acc:   0.58, generation: 40.8121[sec], evaluation: 0.0000[sec]
2025-05-28 22:09:32,022 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:09:32,139 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/31500.ckpt
2025-05-28 22:09:32,143 - INFO - joeynmt.training - Example #0
2025-05-28 22:09:32,143 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:09:32,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:09:32,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', "'ve", 'sho@@', 'wn', 'this', 'last', 'year', ',', 'sho@@', 'w@@', 'ed', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ds', ',', 'it', "'s", 're@@', 'cor@@', 'ds', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:09:32,143 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:09:32,143 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:09:32,143 - INFO - joeynmt.training - 	Hypothesis: The year I 've shown this last year , showed that the glacial calots to show that the glacial calots , which for almost three million years had the United States continental , it 's records , it 's records of the 48 percent .
2025-05-28 22:09:32,143 - INFO - joeynmt.training - Example #1
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - Example #2
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Hypothesis: The arct glacial calots is , in a sense , the heart of global climate system .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - Example #3
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Hypothesis: You can see it winter and you get sumer .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - Example #4
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:09:32,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:09:32,144 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:09:32,145 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly on the last 25 years .
2025-05-28 22:09:47,340 - INFO - joeynmt.training - Epoch   8, Step:    34600, Batch Loss:     1.339592, Batch Acc: 0.618431, Tokens per Sec:     4546, Lr: 0.000300
2025-05-28 22:10:02,007 - INFO - joeynmt.training - Epoch   8, Step:    34700, Batch Loss:     1.311175, Batch Acc: 0.616748, Tokens per Sec:     4879, Lr: 0.000300
2025-05-28 22:10:17,031 - INFO - joeynmt.training - Epoch   8, Step:    34800, Batch Loss:     1.256812, Batch Acc: 0.624453, Tokens per Sec:     4807, Lr: 0.000300
2025-05-28 22:10:20,613 - INFO - joeynmt.training - Epoch   8: total training loss 5636.69
2025-05-28 22:10:20,614 - INFO - joeynmt.training - EPOCH 9
2025-05-28 22:10:31,724 - INFO - joeynmt.training - Epoch   9, Step:    34900, Batch Loss:     1.260750, Batch Acc: 0.643902, Tokens per Sec:     4708, Lr: 0.000300
2025-05-28 22:10:46,336 - INFO - joeynmt.training - Epoch   9, Step:    35000, Batch Loss:     1.361031, Batch Acc: 0.650209, Tokens per Sec:     4917, Lr: 0.000300
2025-05-28 22:10:46,337 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:10:46,337 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:11:25,213 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.49, acc:   0.58, generation: 38.8687[sec], evaluation: 0.0000[sec]
2025-05-28 22:11:25,336 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/33500.ckpt
2025-05-28 22:11:25,339 - INFO - joeynmt.training - Example #0
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'and', 'it', "'s", 're@@', 'f@@', 'it', 'from', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:11:25,339 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:11:25,339 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:11:25,339 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to show that the glacial calots , which for almost three million years had had the size of the United States continental , and it 's refit from 48 percent .
2025-05-28 22:11:25,339 - INFO - joeynmt.training - Example #1
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:11:25,339 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'sp@@', 'end@@', 'ing', 'ice', '.', '</s>']
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Hypothesis: But this subjected the gravity of the problem because it doesn 't show the ice spending ice .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - Example #2
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'of', 'the', 'ar@@', 't@@', 'ics', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Hypothesis: The artics of the artics is , in a sense , the heart of global climate system .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - Example #3
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:11:25,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'w@@', 'int@@', 'er', 'exp@@', 'ands', 'and', 'w@@', 'int@@', 'er', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - 	Hypothesis: You winter expands and winter reverse .
2025-05-28 22:11:25,340 - INFO - joeynmt.training - Example #4
2025-05-28 22:11:25,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:11:25,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:11:25,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'ro@@', 't', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:11:25,341 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:11:25,341 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:11:25,341 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrot on the last 25 years .
2025-05-28 22:11:40,457 - INFO - joeynmt.training - Epoch   9, Step:    35100, Batch Loss:     1.457893, Batch Acc: 0.642629, Tokens per Sec:     4667, Lr: 0.000300
2025-05-28 22:11:55,384 - INFO - joeynmt.training - Epoch   9, Step:    35200, Batch Loss:     1.265959, Batch Acc: 0.646789, Tokens per Sec:     4672, Lr: 0.000300
2025-05-28 22:12:10,613 - INFO - joeynmt.training - Epoch   9, Step:    35300, Batch Loss:     1.055932, Batch Acc: 0.637735, Tokens per Sec:     4661, Lr: 0.000300
2025-05-28 22:12:25,917 - INFO - joeynmt.training - Epoch   9, Step:    35400, Batch Loss:     1.156587, Batch Acc: 0.644902, Tokens per Sec:     4815, Lr: 0.000300
2025-05-28 22:12:40,703 - INFO - joeynmt.training - Epoch   9, Step:    35500, Batch Loss:     1.272440, Batch Acc: 0.637254, Tokens per Sec:     4850, Lr: 0.000300
2025-05-28 22:12:40,704 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:12:40,704 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:13:19,796 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.50, acc:   0.58, generation: 39.0853[sec], evaluation: 0.0000[sec]
2025-05-28 22:13:19,923 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/33000.ckpt
2025-05-28 22:13:19,926 - INFO - joeynmt.training - Example #0
2025-05-28 22:13:19,926 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:13:19,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:13:19,926 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'been', 'sho@@', 'wing', 'this', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'this', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Hypothesis: I 've been showing this year , I showed this slides to show that the glacial glacial , which for almost three million years had the size of the United States continental , it 's refted 40 percent .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - Example #1
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', ',', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Hypothesis: But this underestimate , this is the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - Example #2
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Hypothesis: The glacial calculation is , in a way , the heart of the global climate system .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - Example #3
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:13:19,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'and', 'w@@', 'int@@', 'er', ',', 'and', 'it', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - 	Hypothesis: It 's expanding and winter , and it reverse .
2025-05-28 22:13:19,927 - INFO - joeynmt.training - Example #4
2025-05-28 22:13:19,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:13:19,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:13:19,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ar@@', 'ter@@', 's', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:13:19,928 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:13:19,928 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:13:19,928 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a quarters on the last 25 years .
2025-05-28 22:13:34,845 - INFO - joeynmt.training - Epoch   9, Step:    35600, Batch Loss:     1.250086, Batch Acc: 0.638722, Tokens per Sec:     4709, Lr: 0.000300
2025-05-28 22:13:50,435 - INFO - joeynmt.training - Epoch   9, Step:    35700, Batch Loss:     1.267098, Batch Acc: 0.642789, Tokens per Sec:     4592, Lr: 0.000300
2025-05-28 22:14:05,900 - INFO - joeynmt.training - Epoch   9, Step:    35800, Batch Loss:     1.218094, Batch Acc: 0.632958, Tokens per Sec:     4485, Lr: 0.000300
2025-05-28 22:14:21,457 - INFO - joeynmt.training - Epoch   9, Step:    35900, Batch Loss:     1.486675, Batch Acc: 0.633437, Tokens per Sec:     4586, Lr: 0.000300
2025-05-28 22:14:36,785 - INFO - joeynmt.training - Epoch   9, Step:    36000, Batch Loss:     1.199562, Batch Acc: 0.634328, Tokens per Sec:     4717, Lr: 0.000300
2025-05-28 22:14:36,786 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:14:36,786 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:15:16,494 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.49, acc:   0.58, generation: 39.7007[sec], evaluation: 0.0000[sec]
2025-05-28 22:15:16,622 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/32500.ckpt
2025-05-28 22:15:16,625 - INFO - joeynmt.training - Example #0
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'been', 'sho@@', 'wn', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'er@@', 'ing', 'about', '40', 'percent', '.', '</s>']
2025-05-28 22:15:16,625 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:15:16,625 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:15:16,625 - INFO - joeynmt.training - 	Hypothesis: I 've been shown this year I showed these slides to demonstrate that the glacial calots , which for almost three million years had the United States continental , it 's refering about 40 percent .
2025-05-28 22:15:16,625 - INFO - joeynmt.training - Example #1
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:15:16,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', ',', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Hypothesis: But this underestimate , this underneath gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - Example #2
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'ing', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Hypothesis: The glacial calots is , in a sense , the cleaning of the global climate system .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - Example #3
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', "'s", 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'm@@', 'ed', '.', '</s>']
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Hypothesis: You can see it 's winter and you get summmed .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - Example #4
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:15:16,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'ro@@', 't', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:15:16,626 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a quickly carrot on the last 25 years .
2025-05-28 22:15:31,312 - INFO - joeynmt.training - Epoch   9, Step:    36100, Batch Loss:     1.430405, Batch Acc: 0.637689, Tokens per Sec:     4981, Lr: 0.000300
2025-05-28 22:15:46,262 - INFO - joeynmt.training - Epoch   9, Step:    36200, Batch Loss:     1.200781, Batch Acc: 0.637887, Tokens per Sec:     4805, Lr: 0.000300
2025-05-28 22:16:01,173 - INFO - joeynmt.training - Epoch   9, Step:    36300, Batch Loss:     1.230516, Batch Acc: 0.633902, Tokens per Sec:     4836, Lr: 0.000300
2025-05-28 22:16:15,802 - INFO - joeynmt.training - Epoch   9, Step:    36400, Batch Loss:     1.366502, Batch Acc: 0.635483, Tokens per Sec:     4912, Lr: 0.000300
2025-05-28 22:16:30,697 - INFO - joeynmt.training - Epoch   9, Step:    36500, Batch Loss:     1.301588, Batch Acc: 0.635778, Tokens per Sec:     4746, Lr: 0.000300
2025-05-28 22:16:30,699 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:16:30,699 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:17:09,484 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.46, acc:   0.58, generation: 38.7785[sec], evaluation: 0.0000[sec]
2025-05-28 22:17:09,627 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/34000.ckpt
2025-05-28 22:17:09,631 - INFO - joeynmt.training - Example #0
2025-05-28 22:17:09,631 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:17:09,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:17:09,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ed', 'about', '40', 'percent', '.', '</s>']
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Hypothesis: I 've shown this year I showed these slides to demonstrate that the glacial calots , that for almost three million years had the size of the 48 States continental , it 's refed about 40 percent .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - Example #1
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Hypothesis: But this subjected the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - Example #2
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'le', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Hypothesis: The article calots is , in a way , the heart of the global climate system .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - Example #3
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'and', 'it', "'s", 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - 	Hypothesis: It 's expanding and it 's reverse .
2025-05-28 22:17:09,632 - INFO - joeynmt.training - Example #4
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:17:09,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'car@@', 'r@@', 'ying', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:17:09,633 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:17:09,633 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:17:09,633 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast carrying on the last 25 years .
2025-05-28 22:17:25,040 - INFO - joeynmt.training - Epoch   9, Step:    36600, Batch Loss:     1.313901, Batch Acc: 0.629863, Tokens per Sec:     4538, Lr: 0.000300
2025-05-28 22:17:40,097 - INFO - joeynmt.training - Epoch   9, Step:    36700, Batch Loss:     1.168251, Batch Acc: 0.634239, Tokens per Sec:     4624, Lr: 0.000300
2025-05-28 22:17:55,343 - INFO - joeynmt.training - Epoch   9, Step:    36800, Batch Loss:     1.353454, Batch Acc: 0.634496, Tokens per Sec:     4624, Lr: 0.000300
2025-05-28 22:18:10,720 - INFO - joeynmt.training - Epoch   9, Step:    36900, Batch Loss:     1.229294, Batch Acc: 0.636909, Tokens per Sec:     4740, Lr: 0.000300
2025-05-28 22:18:25,726 - INFO - joeynmt.training - Epoch   9, Step:    37000, Batch Loss:     1.255610, Batch Acc: 0.635895, Tokens per Sec:     4890, Lr: 0.000300
2025-05-28 22:18:25,727 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:18:25,727 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:19:03,014 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.48, acc:   0.58, generation: 37.2798[sec], evaluation: 0.0000[sec]
2025-05-28 22:19:03,166 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/35500.ckpt
2025-05-28 22:19:03,168 - INFO - joeynmt.training - Example #0
2025-05-28 22:19:03,168 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:19:03,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:19:03,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'he@@', 'at', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'been', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ed', 'about', '40', 'percent', '.', '</s>']
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial heat , which for almost three million years has been the United States continental , it 's refed about 40 percent .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - Example #1
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - Example #2
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'he@@', 'at', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'i@@', 'sh@@', 'ing', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Hypothesis: The glacial heat is , in a sense , the cleanishing heart of the global climate system .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - Example #3
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'w@@', 'int@@', 'er', 'is', 'going', 'to', 'be', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - 	Hypothesis: You can expand winter is going to be reverse .
2025-05-28 22:19:03,169 - INFO - joeynmt.training - Example #4
2025-05-28 22:19:03,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:19:03,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:19:03,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd', 'on', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:19:03,170 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:19:03,170 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:19:03,170 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick card on on the last 25 years .
2025-05-28 22:19:18,598 - INFO - joeynmt.training - Epoch   9, Step:    37100, Batch Loss:     1.104721, Batch Acc: 0.635505, Tokens per Sec:     4694, Lr: 0.000300
2025-05-28 22:19:33,074 - INFO - joeynmt.training - Epoch   9, Step:    37200, Batch Loss:     1.382771, Batch Acc: 0.638155, Tokens per Sec:     4889, Lr: 0.000300
2025-05-28 22:19:47,719 - INFO - joeynmt.training - Epoch   9, Step:    37300, Batch Loss:     1.308588, Batch Acc: 0.636343, Tokens per Sec:     5078, Lr: 0.000300
2025-05-28 22:20:03,403 - INFO - joeynmt.training - Epoch   9, Step:    37400, Batch Loss:     1.317405, Batch Acc: 0.632612, Tokens per Sec:     4715, Lr: 0.000300
2025-05-28 22:20:18,298 - INFO - joeynmt.training - Epoch   9, Step:    37500, Batch Loss:     1.305161, Batch Acc: 0.626500, Tokens per Sec:     4632, Lr: 0.000300
2025-05-28 22:20:18,299 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:20:18,299 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:20:58,039 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.50, acc:   0.58, generation: 39.7332[sec], evaluation: 0.0000[sec]
2025-05-28 22:20:58,042 - INFO - joeynmt.training - Example #0
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', 'about', '40', 'percent', '.', '</s>']
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to show that the glacial glacial calots , which for almost three million years had the size of the United States continental , it 's refted about 40 percent .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - Example #1
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', ',', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'end@@', 'ing', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Hypothesis: But this subjected , this is the gravity of the problem because it doesn 't show the spending of the ice .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - Example #2
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - 	Hypothesis: The artical calots is , in a sense , in a sense , the clean heart global climate system .
2025-05-28 22:20:58,043 - INFO - joeynmt.training - Example #3
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:20:58,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'an@@', 'ding', 'it', 'to', 'in@@', 'ver@@', 'int@@', 'er', 'and', 'you', 'get', 'su@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Hypothesis: You expanding it to inverinter and you get sumer .
2025-05-28 22:20:58,044 - INFO - joeynmt.training - Example #4
2025-05-28 22:20:58,044 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:20:58,044 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:20:58,044 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'ry', 'on', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:20:58,044 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carry on on the last 25 years .
2025-05-28 22:21:12,796 - INFO - joeynmt.training - Epoch   9, Step:    37600, Batch Loss:     1.458583, Batch Acc: 0.632391, Tokens per Sec:     4875, Lr: 0.000300
2025-05-28 22:21:27,431 - INFO - joeynmt.training - Epoch   9, Step:    37700, Batch Loss:     1.204911, Batch Acc: 0.634472, Tokens per Sec:     4794, Lr: 0.000300
2025-05-28 22:21:42,469 - INFO - joeynmt.training - Epoch   9, Step:    37800, Batch Loss:     1.194405, Batch Acc: 0.627761, Tokens per Sec:     4762, Lr: 0.000300
2025-05-28 22:21:57,528 - INFO - joeynmt.training - Epoch   9, Step:    37900, Batch Loss:     1.146003, Batch Acc: 0.629079, Tokens per Sec:     4742, Lr: 0.000300
2025-05-28 22:22:12,655 - INFO - joeynmt.training - Epoch   9, Step:    38000, Batch Loss:     1.220558, Batch Acc: 0.629633, Tokens per Sec:     4850, Lr: 0.000300
2025-05-28 22:22:12,656 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:22:12,656 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:22:53,747 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.45, acc:   0.58, generation: 41.0838[sec], evaluation: 0.0000[sec]
2025-05-28 22:22:53,750 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:22:53,891 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/35000.ckpt
2025-05-28 22:22:53,893 - INFO - joeynmt.training - Example #0
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'and', 're@@', 'f@@', 'ted', 'over', '40', 'percent', '.', '</s>']
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to demonstrate that the glacial calots , which for almost three million years had the United States , for almost three million years had the United States continental , and refted over 40 percent .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - Example #1
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', ',', 'this', 'sub@@', 'j@@', 'ects', ',', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'l@@', 'and@@', 'ing', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Hypothesis: However , this subjects , because it doesn 't show the splanding of the ice .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - Example #2
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - 	Hypothesis: The glacial calots is , in a sense , in a sense , the heart of the global climate system .
2025-05-28 22:22:53,894 - INFO - joeynmt.training - Example #3
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:22:53,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Hypothesis: You can see the winter and you reverse summer .
2025-05-28 22:22:53,895 - INFO - joeynmt.training - Example #4
2025-05-28 22:22:53,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:22:53,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:22:53,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ay', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:22:53,895 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlay on the last 25 years .
2025-05-28 22:23:08,782 - INFO - joeynmt.training - Epoch   9, Step:    38100, Batch Loss:     1.112142, Batch Acc: 0.631198, Tokens per Sec:     4737, Lr: 0.000300
2025-05-28 22:23:23,931 - INFO - joeynmt.training - Epoch   9, Step:    38200, Batch Loss:     1.198659, Batch Acc: 0.631537, Tokens per Sec:     4735, Lr: 0.000300
2025-05-28 22:23:38,850 - INFO - joeynmt.training - Epoch   9, Step:    38300, Batch Loss:     1.265501, Batch Acc: 0.629894, Tokens per Sec:     4779, Lr: 0.000300
2025-05-28 22:23:54,182 - INFO - joeynmt.training - Epoch   9, Step:    38400, Batch Loss:     1.285177, Batch Acc: 0.629496, Tokens per Sec:     4735, Lr: 0.000300
2025-05-28 22:24:09,114 - INFO - joeynmt.training - Epoch   9, Step:    38500, Batch Loss:     1.215344, Batch Acc: 0.632689, Tokens per Sec:     4848, Lr: 0.000300
2025-05-28 22:24:09,115 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:24:09,115 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:24:50,189 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.42, acc:   0.58, generation: 41.0679[sec], evaluation: 0.0000[sec]
2025-05-28 22:24:50,192 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:24:50,317 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/36000.ckpt
2025-05-28 22:24:50,320 - INFO - joeynmt.training - Example #0
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'f@@', 'ted', '40', 'percent', '.', '</s>']
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show that the slides to show that the artical calots , which for almost three million years had the size of the 48 million years had the United States continental , it 's refted 40 percent .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - Example #1
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:24:50,320 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'sp@@', 'r@@', 'ing', 'the', 'ice', '.', '</s>']
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - 	Hypothesis: But this underneath this underneath the problem because it doesn 't show the ice spring the ice .
2025-05-28 22:24:50,320 - INFO - joeynmt.training - Example #2
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'c@@', 'le@@', 'ver', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Hypothesis: The glacial calots is , in a way , the clever of the global climate system .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - Example #3
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'exp@@', 'and', 'w@@', 'int@@', 'er', 're@@', 'ach', '.', '</s>']
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Hypothesis: You can expand winter reach .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - Example #4
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:24:50,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'l@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:24:50,321 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quickly carler on the last 25 years .
2025-05-28 22:25:05,190 - INFO - joeynmt.training - Epoch   9, Step:    38600, Batch Loss:     1.266702, Batch Acc: 0.634162, Tokens per Sec:     4779, Lr: 0.000300
2025-05-28 22:25:20,374 - INFO - joeynmt.training - Epoch   9, Step:    38700, Batch Loss:     1.294103, Batch Acc: 0.626858, Tokens per Sec:     4728, Lr: 0.000300
2025-05-28 22:25:35,955 - INFO - joeynmt.training - Epoch   9, Step:    38800, Batch Loss:     1.409411, Batch Acc: 0.630974, Tokens per Sec:     4660, Lr: 0.000300
2025-05-28 22:25:51,150 - INFO - joeynmt.training - Epoch   9, Step:    38900, Batch Loss:     1.230228, Batch Acc: 0.634854, Tokens per Sec:     4779, Lr: 0.000300
2025-05-28 22:26:06,445 - INFO - joeynmt.training - Epoch   9, Step:    39000, Batch Loss:     1.084793, Batch Acc: 0.625597, Tokens per Sec:     4625, Lr: 0.000300
2025-05-28 22:26:06,446 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:26:06,446 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:26:47,465 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.41, acc:   0.58, generation: 41.0117[sec], evaluation: 0.0000[sec]
2025-05-28 22:26:47,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:26:47,585 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/37000.ckpt
2025-05-28 22:26:47,587 - INFO - joeynmt.training - Example #0
2025-05-28 22:26:47,587 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 'es', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Hypothesis: I showed this year , I showed these slides to demonstrate that the artical calotes , which for almost three million years had the size of 48 years had the size of the 48 percent .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - Example #1
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'tive', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Hypothesis: But this subjective gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - Example #2
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'ul@@', 'ty', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'ar', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Hypothesis: The articulty calots is , in a sense , the clear heart of the global climate system .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - Example #3
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:26:47,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'an@@', 'no@@', 'on', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:26:47,588 - INFO - joeynmt.training - 	Hypothesis: You expannoon winter and you reverse summer .
2025-05-28 22:26:47,589 - INFO - joeynmt.training - Example #4
2025-05-28 22:26:47,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:26:47,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:26:47,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'char@@', 'ac@@', 'ter@@', 's', 'of', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:26:47,589 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:26:47,589 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:26:47,589 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick characters of the last 25 years .
2025-05-28 22:27:02,664 - INFO - joeynmt.training - Epoch   9, Step:    39100, Batch Loss:     1.537606, Batch Acc: 0.633681, Tokens per Sec:     4816, Lr: 0.000300
2025-05-28 22:27:16,330 - INFO - joeynmt.training - Epoch   9: total training loss 5517.25
2025-05-28 22:27:16,330 - INFO - joeynmt.training - EPOCH 10
2025-05-28 22:27:18,074 - INFO - joeynmt.training - Epoch  10, Step:    39200, Batch Loss:     1.124799, Batch Acc: 0.651410, Tokens per Sec:     4475, Lr: 0.000300
2025-05-28 22:27:33,658 - INFO - joeynmt.training - Epoch  10, Step:    39300, Batch Loss:     1.315729, Batch Acc: 0.652182, Tokens per Sec:     4622, Lr: 0.000300
2025-05-28 22:27:48,614 - INFO - joeynmt.training - Epoch  10, Step:    39400, Batch Loss:     1.203729, Batch Acc: 0.654096, Tokens per Sec:     4863, Lr: 0.000300
2025-05-28 22:28:04,280 - INFO - joeynmt.training - Epoch  10, Step:    39500, Batch Loss:     1.156712, Batch Acc: 0.652550, Tokens per Sec:     4531, Lr: 0.000300
2025-05-28 22:28:04,281 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:28:04,281 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:28:52,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.43, acc:   0.58, generation: 48.6758[sec], evaluation: 0.0000[sec]
2025-05-28 22:28:53,108 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/36500.ckpt
2025-05-28 22:28:53,111 - INFO - joeynmt.training - Example #0
2025-05-28 22:28:53,111 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:28:53,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:28:53,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'is', 're@@', 'cor@@', 'ded', 'by', '40', 'percent', '.', '</s>']
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to demonstrate that the glacial glacial , which for almost three million years had the size of the United States , is recorded by 40 percent .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - Example #1
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'under@@', 'l@@', 'ying', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Hypothesis: But this underunderlying gravity of the problem because it doesn 't show the ice of the ice of the ice .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - Example #2
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Hypothesis: The artics calots is , in a way , the heart of the global climate system .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - Example #3
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:28:53,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - 	Hypothesis: You can see winter and you reverse summer .
2025-05-28 22:28:53,112 - INFO - joeynmt.training - Example #4
2025-05-28 22:28:53,113 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:28:53,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:28:53,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:28:53,113 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:28:53,113 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:28:53,113 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carler on the last 25 years .
2025-05-28 22:29:08,513 - INFO - joeynmt.training - Epoch  10, Step:    39600, Batch Loss:     1.207518, Batch Acc: 0.650324, Tokens per Sec:     4572, Lr: 0.000300
2025-05-28 22:29:24,032 - INFO - joeynmt.training - Epoch  10, Step:    39700, Batch Loss:     1.221510, Batch Acc: 0.647636, Tokens per Sec:     4727, Lr: 0.000300
2025-05-28 22:29:39,077 - INFO - joeynmt.training - Epoch  10, Step:    39800, Batch Loss:     1.040565, Batch Acc: 0.654887, Tokens per Sec:     4765, Lr: 0.000300
2025-05-28 22:29:53,975 - INFO - joeynmt.training - Epoch  10, Step:    39900, Batch Loss:     1.260553, Batch Acc: 0.647824, Tokens per Sec:     4839, Lr: 0.000300
2025-05-28 22:30:09,031 - INFO - joeynmt.training - Epoch  10, Step:    40000, Batch Loss:     1.186129, Batch Acc: 0.645977, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 22:30:09,032 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:30:09,032 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:30:49,785 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.58, generation: 40.7458[sec], evaluation: 0.0000[sec]
2025-05-28 22:30:49,914 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/34500.ckpt
2025-05-28 22:30:49,916 - INFO - joeynmt.training - Example #0
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', ',', 'the', 'last', 'last', 'year', ',', 'it', "'s", 're@@', 'f@@', 'er@@', 'ed', '40', 'percent', '.', '</s>']
2025-05-28 22:30:49,916 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:30:49,916 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:30:49,916 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to demonstrate that the glacial glacial , which for almost three million years had the United States , the last last year , it 's refered 40 percent .
2025-05-28 22:30:49,916 - INFO - joeynmt.training - Example #1
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:30:49,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'under@@', 'gr@@', 'av@@', 'ity', ',', 'that', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ac@@', 'ross', '.', '</s>']
2025-05-28 22:30:49,916 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Hypothesis: But this underundergravity , that is the gravity of the problem because it doesn 't show the thacross .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - Example #2
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'A@@', 'c@@', 'tu@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'hear@@', 't', 'system', '.', '</s>']
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Hypothesis: The Actuical calots is , in a sense , the heart of the global climate heart system .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - Example #3
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'ted', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Hypothesis: It 's expanding winter and you reverted summer .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - Example #4
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:30:49,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'ying', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:30:49,917 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlying on the last 25 years .
2025-05-28 22:31:04,684 - INFO - joeynmt.training - Epoch  10, Step:    40100, Batch Loss:     1.058212, Batch Acc: 0.643270, Tokens per Sec:     4930, Lr: 0.000300
2025-05-28 22:31:20,655 - INFO - joeynmt.training - Epoch  10, Step:    40200, Batch Loss:     1.409447, Batch Acc: 0.646990, Tokens per Sec:     4489, Lr: 0.000300
2025-05-28 22:31:36,122 - INFO - joeynmt.training - Epoch  10, Step:    40300, Batch Loss:     1.118694, Batch Acc: 0.642741, Tokens per Sec:     4715, Lr: 0.000300
2025-05-28 22:31:51,510 - INFO - joeynmt.training - Epoch  10, Step:    40400, Batch Loss:     1.218687, Batch Acc: 0.646383, Tokens per Sec:     4684, Lr: 0.000300
2025-05-28 22:32:06,310 - INFO - joeynmt.training - Epoch  10, Step:    40500, Batch Loss:     1.206935, Batch Acc: 0.636139, Tokens per Sec:     4696, Lr: 0.000300
2025-05-28 22:32:06,311 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:32:06,311 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:32:49,332 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.45, acc:   0.58, generation: 43.0140[sec], evaluation: 0.0000[sec]
2025-05-28 22:32:49,335 - INFO - joeynmt.training - Example #0
2025-05-28 22:32:49,335 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:32:49,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:32:49,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'si@@', 'ze', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'tro@@', 'ded', 'for', '40', 'percent', '.', '</s>']
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to demonstrate the artical calots , which for almost three million years has had the size size of the United States continental , it 's retroded for 40 percent .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - Example #1
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'gro@@', 'und', 'gr@@', 'av@@', 'ity', ',', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Hypothesis: But this underground gravity , because it doesn 't show the ice of the ice of the ice .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - Example #2
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'A@@', 'c@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'c@@', 'le@@', 'ar', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - 	Hypothesis: The Acial calots is , in a way , the clear heart of the global climate system .
2025-05-28 22:32:49,336 - INFO - joeynmt.training - Example #3
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:32:49,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Hypothesis: It 's expanding winter and you reverse summer .
2025-05-28 22:32:49,337 - INFO - joeynmt.training - Example #4
2025-05-28 22:32:49,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:32:49,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:32:49,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'l@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:32:49,337 - INFO - joeynmt.training - 	Hypothesis: The next next slide is going to be a quickly carler on the last 25 years .
2025-05-28 22:33:04,297 - INFO - joeynmt.training - Epoch  10, Step:    40600, Batch Loss:     1.141024, Batch Acc: 0.648226, Tokens per Sec:     4787, Lr: 0.000300
2025-05-28 22:33:19,429 - INFO - joeynmt.training - Epoch  10, Step:    40700, Batch Loss:     1.332303, Batch Acc: 0.646948, Tokens per Sec:     4860, Lr: 0.000300
2025-05-28 22:33:34,325 - INFO - joeynmt.training - Epoch  10, Step:    40800, Batch Loss:     1.233369, Batch Acc: 0.638813, Tokens per Sec:     4942, Lr: 0.000300
2025-05-28 22:33:49,007 - INFO - joeynmt.training - Epoch  10, Step:    40900, Batch Loss:     1.186062, Batch Acc: 0.643849, Tokens per Sec:     5036, Lr: 0.000300
2025-05-28 22:34:04,109 - INFO - joeynmt.training - Epoch  10, Step:    41000, Batch Loss:     1.412600, Batch Acc: 0.644944, Tokens per Sec:     4747, Lr: 0.000300
2025-05-28 22:34:04,110 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:34:04,110 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:34:48,403 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.43, acc:   0.58, generation: 44.2862[sec], evaluation: 0.0000[sec]
2025-05-28 22:34:48,545 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/38000.ckpt
2025-05-28 22:34:48,547 - INFO - joeynmt.training - Example #0
2025-05-28 22:34:48,547 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:34:48,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:34:48,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'this', 'last', 'year', ',', 'I', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'es', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'tro@@', 'ver@@', 'si@@', 'ons', 'of', 'the', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:34:48,547 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:34:48,547 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Hypothesis: I 've shown this last year , I 've shown these slides to show that the glacial calotes , which for almost three million years had had the size of 48 of the United States continental , it 's retroversions of the 48 percent .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - Example #1
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'sub@@', 'j@@', 'ects', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Hypothesis: But this subjects this subjects the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - Example #2
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'A@@', 'c@@', 'tu@@', 'ally', ',', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'ar', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Hypothesis: The Actually , the artical calots is , in a sense , the clear heart of the global climate system .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - Example #3
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'exp@@', 'ands', 'of', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - 	Hypothesis: It expands of winter and you reverse .
2025-05-28 22:34:48,548 - INFO - joeynmt.training - Example #4
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:34:48,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'l@@', 'au@@', 'gh@@', 'ing', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:34:48,549 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:34:48,549 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:34:48,549 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly laughing on the last 25 years .
2025-05-28 22:35:03,777 - INFO - joeynmt.training - Epoch  10, Step:    41100, Batch Loss:     1.175311, Batch Acc: 0.643104, Tokens per Sec:     4656, Lr: 0.000300
2025-05-28 22:35:18,631 - INFO - joeynmt.training - Epoch  10, Step:    41200, Batch Loss:     1.486303, Batch Acc: 0.636564, Tokens per Sec:     4851, Lr: 0.000300
2025-05-28 22:35:33,434 - INFO - joeynmt.training - Epoch  10, Step:    41300, Batch Loss:     1.253489, Batch Acc: 0.644160, Tokens per Sec:     4861, Lr: 0.000300
2025-05-28 22:35:48,980 - INFO - joeynmt.training - Epoch  10, Step:    41400, Batch Loss:     1.137487, Batch Acc: 0.639444, Tokens per Sec:     4661, Lr: 0.000300
2025-05-28 22:36:04,059 - INFO - joeynmt.training - Epoch  10, Step:    41500, Batch Loss:     1.122000, Batch Acc: 0.645796, Tokens per Sec:     4686, Lr: 0.000300
2025-05-28 22:36:04,060 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:36:04,060 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:36:42,943 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.58, generation: 38.8761[sec], evaluation: 0.0000[sec]
2025-05-28 22:36:42,946 - INFO - joeynmt.training - Example #0
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'been', 'sho@@', 'wn', 'this', 'last', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', '.', '</s>']
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Hypothesis: I 've been shown this last year , I showed these slides to demonstrate that the glacial calots , which for almost three million years had the size of 48 percent .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - Example #1
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ects', 'this', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ed', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'end@@', 'ing', 'of', 'ice', '.', '</s>']
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - 	Hypothesis: But this subjects this underestablished the problem because it doesn 't show the spending of ice .
2025-05-28 22:36:42,947 - INFO - joeynmt.training - Example #2
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:36:42,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'c@@', 'le@@', 'ver', 'of', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Hypothesis: The artics glacial calots is , in a way , the clever of global climate system .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - Example #3
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ach', '.', '</s>']
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Hypothesis: You can see the winter and you reach .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - Example #4
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:36:42,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:36:42,948 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a faster on the last 25 years .
2025-05-28 22:36:58,335 - INFO - joeynmt.training - Epoch  10, Step:    41600, Batch Loss:     1.175025, Batch Acc: 0.634002, Tokens per Sec:     4807, Lr: 0.000300
2025-05-28 22:37:13,249 - INFO - joeynmt.training - Epoch  10, Step:    41700, Batch Loss:     1.340772, Batch Acc: 0.643167, Tokens per Sec:     4908, Lr: 0.000300
2025-05-28 22:37:28,146 - INFO - joeynmt.training - Epoch  10, Step:    41800, Batch Loss:     1.278322, Batch Acc: 0.639741, Tokens per Sec:     4711, Lr: 0.000300
2025-05-28 22:37:43,581 - INFO - joeynmt.training - Epoch  10, Step:    41900, Batch Loss:     1.366287, Batch Acc: 0.638055, Tokens per Sec:     4782, Lr: 0.000300
2025-05-28 22:37:59,352 - INFO - joeynmt.training - Epoch  10, Step:    42000, Batch Loss:     1.303776, Batch Acc: 0.638242, Tokens per Sec:     4574, Lr: 0.000300
2025-05-28 22:37:59,352 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:37:59,352 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:38:42,634 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.40, acc:   0.58, generation: 43.2732[sec], evaluation: 0.0000[sec]
2025-05-28 22:38:42,637 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:38:42,792 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/40000.ckpt
2025-05-28 22:38:42,794 - INFO - joeynmt.training - Example #0
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', ',', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", '40', 'percent', '.', '</s>']
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Hypothesis: I 've shown these slides to demonstrate that the glacial calots , which for almost three million years , for almost three million years had the United States continental , it 's 40 percent .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - Example #1
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'j@@', 'ec@@', 'ted', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'sp@@', 'ec@@', 'tor', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Hypothesis: But this subjected gravity of the problem because it doesn 't show the spector of the ice .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - Example #2
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ics', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'ar', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - 	Hypothesis: The artics glacial calots is , in a sense , the clear heart of the global climate system .
2025-05-28 22:38:42,795 - INFO - joeynmt.training - Example #3
2025-05-28 22:38:42,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:38:42,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:38:42,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'and', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ach', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Hypothesis: It 's expanding and winter and you reach summer .
2025-05-28 22:38:42,796 - INFO - joeynmt.training - Example #4
2025-05-28 22:38:42,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:38:42,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:38:42,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'ri@@', 'l@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:38:42,796 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carriler on the last 25 years .
2025-05-28 22:38:58,511 - INFO - joeynmt.training - Epoch  10, Step:    42100, Batch Loss:     1.264191, Batch Acc: 0.641278, Tokens per Sec:     4531, Lr: 0.000300
2025-05-28 22:39:14,506 - INFO - joeynmt.training - Epoch  10, Step:    42200, Batch Loss:     1.373068, Batch Acc: 0.635945, Tokens per Sec:     4615, Lr: 0.000300
2025-05-28 22:39:30,583 - INFO - joeynmt.training - Epoch  10, Step:    42300, Batch Loss:     1.200316, Batch Acc: 0.640144, Tokens per Sec:     4425, Lr: 0.000300
2025-05-28 22:39:46,349 - INFO - joeynmt.training - Epoch  10, Step:    42400, Batch Loss:     1.233586, Batch Acc: 0.636499, Tokens per Sec:     4689, Lr: 0.000300
2025-05-28 22:40:02,025 - INFO - joeynmt.training - Epoch  10, Step:    42500, Batch Loss:     1.186076, Batch Acc: 0.642174, Tokens per Sec:     4607, Lr: 0.000300
2025-05-28 22:40:02,026 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:40:02,026 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:40:44,633 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.42, acc:   0.58, generation: 42.6010[sec], evaluation: 0.0000[sec]
2025-05-28 22:40:44,783 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/39500.ckpt
2025-05-28 22:40:44,786 - INFO - joeynmt.training - Example #0
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'sho@@', 'w@@', 'ed', 'this', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'ited', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'it', "'s", 're@@', 'cor@@', 'ded', 'by', '40', 'percent', '.', '</s>']
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Hypothesis: I showed this year I showed these slides to demonstrate that the glacial calots , which for almost three million years had the size of 48 million years had the United States continental , it 's recorded by 40 percent .
2025-05-28 22:40:44,786 - INFO - joeynmt.training - Example #1
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:40:44,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ment', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:40:44,786 - INFO - joeynmt.training - 	Hypothesis: But this underunderestablishment the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - Example #2
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'A@@', 'c@@', 't@@', 'ica', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Hypothesis: The Actica glacial calots is , in a sense , the clean heart of the global climate system .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - Example #3
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'exp@@', 'an@@', 'ding', 'it', 'and', 'w@@', 'int@@', 'er', ',', 'and', 'you', 'get', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Hypothesis: You expanding it and winter , and you get summer .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - Example #4
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:40:44,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'one', 'car@@', 'l@@', 'li@@', 'ant', 'car@@', 'd', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:40:44,787 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-one carlliant card on the last 25 years .
2025-05-28 22:41:00,352 - INFO - joeynmt.training - Epoch  10, Step:    42600, Batch Loss:     1.185334, Batch Acc: 0.634691, Tokens per Sec:     4430, Lr: 0.000300
2025-05-28 22:41:16,362 - INFO - joeynmt.training - Epoch  10, Step:    42700, Batch Loss:     1.155165, Batch Acc: 0.634782, Tokens per Sec:     4390, Lr: 0.000300
2025-05-28 22:41:31,639 - INFO - joeynmt.training - Epoch  10, Step:    42800, Batch Loss:     1.196677, Batch Acc: 0.634107, Tokens per Sec:     4758, Lr: 0.000300
2025-05-28 22:41:47,349 - INFO - joeynmt.training - Epoch  10, Step:    42900, Batch Loss:     1.240894, Batch Acc: 0.636993, Tokens per Sec:     4524, Lr: 0.000300
2025-05-28 22:42:02,912 - INFO - joeynmt.training - Epoch  10, Step:    43000, Batch Loss:     1.244930, Batch Acc: 0.638320, Tokens per Sec:     4543, Lr: 0.000300
2025-05-28 22:42:02,915 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:42:02,915 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:42:46,415 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.37, acc:   0.58, generation: 43.4941[sec], evaluation: 0.0000[sec]
2025-05-28 22:42:46,418 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 22:42:46,560 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/41000.ckpt
2025-05-28 22:42:46,563 - INFO - joeynmt.training - Example #0
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'have', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'ited', 'St@@', 'ates', '.', '</s>']
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to demonstrate that the glacial calots , which for almost three million years had had the size of 48 million years have had the size of 48 percent of the United States .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - Example #1
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', ',', 'this', 'is', 'under@@', 'est@@', 'im@@', 'ate', 'for', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Hypothesis: However , this is underestimate for the problem because it doesn 't show the ice of the ice .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - Example #2
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'way', ',', 'the', 'c@@', 'le@@', 'ar', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - 	Hypothesis: The glacial calots is , in a way , the clear heart of the global climate system .
2025-05-28 22:42:46,564 - INFO - joeynmt.training - Example #3
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:42:46,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'exp@@', 'and@@', 'ing', 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', 'su@@', 'm@@', 'm@@', 'er', '.', '</s>']
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Hypothesis: It 's expanding winter and you reverse summer .
2025-05-28 22:42:46,565 - INFO - joeynmt.training - Example #4
2025-05-28 22:42:46,565 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:42:46,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:42:46,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'l@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:42:46,565 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carler on the last 25 years .
2025-05-28 22:43:02,013 - INFO - joeynmt.training - Epoch  10, Step:    43100, Batch Loss:     1.284826, Batch Acc: 0.632032, Tokens per Sec:     4558, Lr: 0.000300
2025-05-28 22:43:17,909 - INFO - joeynmt.training - Epoch  10, Step:    43200, Batch Loss:     1.323075, Batch Acc: 0.636964, Tokens per Sec:     4524, Lr: 0.000300
2025-05-28 22:43:33,586 - INFO - joeynmt.training - Epoch  10, Step:    43300, Batch Loss:     1.196873, Batch Acc: 0.636945, Tokens per Sec:     4566, Lr: 0.000300
2025-05-28 22:43:49,336 - INFO - joeynmt.training - Epoch  10, Step:    43400, Batch Loss:     1.337451, Batch Acc: 0.638235, Tokens per Sec:     4469, Lr: 0.000300
2025-05-28 22:44:05,301 - INFO - joeynmt.training - Epoch  10, Step:    43500, Batch Loss:     1.307021, Batch Acc: 0.635599, Tokens per Sec:     4469, Lr: 0.000300
2025-05-28 22:44:05,302 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:44:05,302 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:44:51,118 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.40, acc:   0.58, generation: 45.8087[sec], evaluation: 0.0000[sec]
2025-05-28 22:44:51,263 - INFO - joeynmt.helpers - delete models/transformer_bpe_2000/38500.ckpt
2025-05-28 22:44:51,266 - INFO - joeynmt.training - Example #0
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '40', '%', '.']
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "'ve", 'sho@@', 'wn', 'this', 'last', 'year', ',', 'I', 'sho@@', 'w@@', 'ed', 'this', 'di@@', 'o@@', 'x@@', 'i@@', 'al', 'cal@@', 'ot@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ice', 'of', 'the', 'A@@', 'c@@', 'ti@@', 'c', ',', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', ',', 'you', "'re", 'clo@@', 'se', 'to', '40', 'percent', '.', '</s>']
2025-05-28 22:44:51,266 - INFO - joeynmt.training - 	Source:     L' anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2025-05-28 22:44:51,266 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-28 22:44:51,266 - INFO - joeynmt.training - 	Hypothesis: I 've shown this last year , I showed this dioxial calots to demonstrate that the ice of the Actic , that for almost three million years had the size of 48 States continental , you 're close to 40 percent .
2025-05-28 22:44:51,266 - INFO - joeynmt.training - Example #1
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['T@@', 'utt@@', 'av@@', 'ia', 'questo', 'so@@', 'tto@@', 'val@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'cio', '.']
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'ser@@', 'i@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-28 22:44:51,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'w@@', 'ever', ',', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn 't show the thickness of the ice .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Hypothesis: However , this is the gravity of the problem because it doesn 't show the ice of the ice .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - Example #2
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'C@@', 'O@@', 's', ',', 'the', 'ar@@', 't@@', 'ical', 'cal@@', 'ot@@', 's', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'c@@', 'le@@', 'an@@', 'ing', 'of', 'the', 'glob@@', 'al', 'cli@@', 'm@@', 'ate', 'system', '.', '</s>']
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Hypothesis: CCOs , the artical calots is , in a sense , the cleaning of the global climate system .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - Example #3
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'pan@@', 'de', "d'", 'in@@', 'ver@@', 'no', 'e', 'si', 'r@@', 'iti@@', 'ra', "d'", 'est@@', 'ate', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'exp@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', "'s", 'w@@', 'int@@', 'er', 'and', 'you', 're@@', 'ver@@', 'se', '.', '</s>']
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Source:     Si espande d' inverno e si ritira d' estate .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Hypothesis: It 's winter and you reverse .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - Example #4
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 'rel@@', 'l@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-28 22:44:51,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd', 'on', 'the', 'last', '2@@', '5', 'years', '.', '</s>']
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what 's happened over the last 25 years .
2025-05-28 22:44:51,267 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick card on the last 25 years .
2025-05-28 22:44:56,743 - INFO - joeynmt.training - Epoch  10: total training loss 5384.08
2025-05-28 22:44:56,744 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-28 22:44:56,744 - INFO - joeynmt.training - Best validation result (greedy) at step    43000:   4.37 ppl.
2025-05-28 22:44:56,756 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 22:44:56,791 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 22:44:56,844 - INFO - joeynmt.helpers - Load model from /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_bpe_2000/43000.ckpt.
2025-05-28 22:44:56,846 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2222),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2222),
	loss_function=None)
2025-05-28 22:44:56,852 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-28 22:44:56,853 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:44:56,853 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:45:53,333 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 56.4737[sec], evaluation: 0.0000[sec]
2025-05-28 22:45:53,336 - INFO - joeynmt.prediction - Translations saved to: /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_bpe_2000/00043000.hyps.dev.
2025-05-28 22:45:53,336 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-28 22:45:53,337 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 22:45:53,337 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 22:47:13,122 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 79.7752[sec], evaluation: 0.0000[sec]
2025-05-28 22:47:13,129 - INFO - joeynmt.prediction - Translations saved to: /Users/jingma/Desktop/machine_translation/mt-exercise-4/models/transformer_bpe_2000/00043000.hyps.test.
